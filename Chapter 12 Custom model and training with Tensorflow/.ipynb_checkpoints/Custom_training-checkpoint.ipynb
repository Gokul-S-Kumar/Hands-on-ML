{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building custom training loops"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "from ipynb.fs.full.Useful_funcs import data_pipeline, pre_model, create_huber # Custom funcs for data processing, modelling, compiling and training\n",
    "import tensorflow as tf\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.activations import selu, relu, elu\n",
    "from tensorflow.keras.initializers import lecun_normal, he_normal\n",
    "from tensorflow.keras.optimizers import Nadam\n",
    "from tensorflow.keras.losses import mean_squared_error, mse\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from tensorflow.keras.metrics import Mean, MeanAbsoluteError"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing = fetch_california_housing()\n",
    "x_train, x_train_scaled, x_valid, x_valid_scaled, x_test, x_test_scaled, y_train, y_valid, y_test = data_pipeline(housing)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "pre_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- We will first build a simple model. Need not compile it as we will be handling the training manually."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(30, activation = elu, kernel_initializer = he_normal, kernel_regularizer = l2(0.05)))\n",
    "model.add(Dense(1, kernel_regularizer = l2(0.05)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Define a func that will randomly sample a batch of data from the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_batch(x, y, batch_size = 32):\n",
    "    ids = np.random.randint(len(x), size = batch_size) # Collecting random indexes\n",
    "    return x[ids], y[ids]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Define a function that display the training details like the no. of batches, total no. of batches, the mean loss since start of the epoch and other metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_status_bar(iteration, total, loss, metrics = None):\n",
    "    metrics = ' - '.join(['{} : {:.4f}'.format(m.name, m.result()) for m in [loss] + (metrics or [])])\n",
    "    end = '' if iteration < total else '\\n'\n",
    "    print('\\r{}/{} - '.format(iteration, total) + metrics, end = end)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- {:.4f} will format with 4 digits after the decimal point.\n",
    "- Using \\r and end = '' ensures that the status bar always gets printed on the same line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50/50 - loss : 0.0900 - mean_square : 858.5000\n"
     ]
    }
   ],
   "source": [
    "mean_loss = keras.metrics.Mean(name = 'loss')\n",
    "mean_square = keras.metrics.Mean(name = 'mean_square')\n",
    "for i in range(1, 50 + 1):\n",
    "    loss = 1 / i\n",
    "    mean_loss(loss)\n",
    "    mean_square(i ** 2)\n",
    "    print_status_bar(i, 50, mean_loss, [mean_square])\n",
    "    time.sleep(0.05)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Lets define a fancier status update with a progress bar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def progress_bar(iteration, total, size = 30): # Size of bar, not batch size.\n",
    "    running = iteration < total\n",
    "    c = '>' if running else '='\n",
    "    p = (size - 1) * iteration // total\n",
    "    fmt = '{}/{} [{}]' # Format for the status bar\n",
    "    params = [iteration, total, '=' * p + c + '.' * (size - p - 1)]\n",
    "    return fmt.format(*params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'3500/10000 [=>....]'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "progress_bar(3500, 10000, size = 6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_status_bar(iteration, total, loss, metrics = None):\n",
    "    metrics = ' - '.join(['{}: {:.4f}'.format(m.name, m.result()) for m in [loss] + (metrics or [])])\n",
    "    end = '' if iteration < total else '\\n'\n",
    "    print('\\r{} - {} - '.format(progress_bar(iteration, total), metrics), end = end)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50/50 [==============================] - loss: 0.0900 - mean_square: 858.5000 - \n"
     ]
    }
   ],
   "source": [
    "mean_loss = keras.metrics.Mean(name = 'loss')\n",
    "mean_square = keras.metrics.Mean(name = 'mean_square')\n",
    "for i in range(1, 50 + 1):\n",
    "    loss = 1 / i\n",
    "    mean_loss(loss)\n",
    "    mean_square(i ** 2)\n",
    "    print_status_bar(i, 50, mean_loss, [mean_square])\n",
    "    time.sleep(0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "pre_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Lets now define the hyperparameters to be used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = 5\n",
    "batch_size = 32\n",
    "n_steps = len(x_train) // batch_size\n",
    "optimizer = Nadam(lr = 0.01)\n",
    "loss_fn = mean_squared_error\n",
    "mean_loss = Mean()\n",
    "metrics = [MeanAbsoluteError()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "11610/11610 [==============================] - mean: 1.3473 - mean_absolute_error: 0.9187 - \n",
      "Epoch 2/5\n",
      "11610/11610 [==============================] - mean: 1.3207 - mean_absolute_error: 0.9107 - \n",
      "Epoch 3/5\n",
      "11610/11610 [==============================] - mean: 1.3411 - mean_absolute_error: 0.9142 - \n",
      "Epoch 4/5\n",
      "11610/11610 [==============================] - mean: 1.3407 - mean_absolute_error: 0.9195 - \n",
      "Epoch 5/5\n",
      "11610/11610 [==============================] - mean: 1.3734 - mean_absolute_error: 0.9265 - \n"
     ]
    }
   ],
   "source": [
    "for epoch in range(1, n_epochs + 1):\n",
    "    print('Epoch {}/{}'.format(epoch, n_epochs)) # Printing epoch status\n",
    "    for step in range(1, n_steps + 1):\n",
    "        x_batch, y_batch = random_batch(x_train_scaled, y_train) # Randomly picking a batch from training data\n",
    "        with tf.GradientTape() as tape: # Taping forward prop\n",
    "            y_pred = model(x_batch) # Predicting the labels\n",
    "            main_loss = tf.reduce_mean(loss_fn(y_batch, y_pred)) # Calculating MSE \n",
    "            loss = tf.add_n([main_loss] + model.losses) # Adding MSE with other model losses\n",
    "        gradients = tape.gradient(loss, model.trainable_variables) # Computing the gradients\n",
    "        optimizer.apply_gradients(zip(gradients, model.trainable_variables)) # Using the optimizer for Gradient Descent\n",
    "        for variable in model.variables:\n",
    "            if variable.constraint is not None:\n",
    "                variable.assign(variable.constraint(variable)) # Applying constraints to the variables if any \n",
    "        mean_loss(loss) # Calculating the mean loss\n",
    "        for metric in metrics:\n",
    "            metric(y_batch, y_pred) # Calculating the metrics\n",
    "        print_status_bar(step * batch_size, len(y_train), mean_loss, metrics) # Printing the status bar for steps\n",
    "    print_status_bar(len(y_train), len(y_train), mean_loss, metrics) # Printing the status bar for the epochs\n",
    "    for metric in [mean_loss] + metrics:\n",
    "        metric.reset_states() # Resetting the metrics "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- We created two nested loops, one for the epochs and the other one for the steps within each epoch.\n",
    "- Then we sampled a random batch from the training data. These batches are not mutually exclusive as the some instances may be sampled repeatedly and some may not be sampled at all.\n",
    "- Inside the tf.GradientTape() loop we make the prediction for one batch using the model() as func and we compute the loss. The total loss is equal to the sum of the MSE loss computed and the regularization loss per layer in this case. Since MSE returns one loss value per instance we will calculate the mean across the entire batch.\n",
    "- We ask the tape to compute the gradients of the loss wrt each variable. Then we apply them to the optimizer to perform a Gradient Descent step.\n",
    "- We calculate the mean loss and the metrics over the current epoch and we display the status bar.\n",
    "- In the end after each epoch we display the final status bar and reset the metric values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
