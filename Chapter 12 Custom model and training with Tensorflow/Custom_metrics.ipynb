{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Developing custom metrics to use in NN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from ipynb.fs.full.Useful_funcs import data_pipeline, compile_train, pre_model, create_huber # Custom funcs for data processing, modelling, compiling and training\n",
    "import tensorflow as tf\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.activations import selu\n",
    "from tensorflow.keras.initializers import lecun_normal\n",
    "from tensorflow.keras.optimizers import Nadam\n",
    "from tensorflow.keras.losses import mse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing = fetch_california_housing()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_train_scaled, x_valid, x_valid_scaled, x_test, x_test_scaled, y_train, y_valid, y_test = data_pipeline(housing)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The huber loss func defined earlier can be used as a custom metric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_shape = x_train.shape[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "pre_model()\n",
    "model = Sequential()\n",
    "model.add(Dense(30, activation = selu, kernel_initializer = lecun_normal(), input_shape = input_shape))\n",
    "model.add(Dense(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense (Dense)                (None, 30)                270       \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 31        \n",
      "=================================================================\n",
      "Total params: 301\n",
      "Trainable params: 301\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "WARNING:tensorflow:AutoGraph could not transform <function create_huber.<locals>.huber_loss at 0x7fe577ac4280> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: unexpected EOF while parsing (<unknown>, line 1)\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function create_huber.<locals>.huber_loss at 0x7fe577ac4280> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: unexpected EOF while parsing (<unknown>, line 1)\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 2.0982 - huber_loss: 0.9192 - val_loss: 9.0831 - val_huber_loss: 0.4418\n",
      "Epoch 2/5\n",
      "363/363 [==============================] - 0s 940us/step - loss: 0.6052 - huber_loss: 0.2733 - val_loss: 5.8943 - val_huber_loss: 0.3463\n",
      "Epoch 3/5\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.5113 - huber_loss: 0.2389 - val_loss: 3.4119 - val_huber_loss: 0.3016\n",
      "Epoch 4/5\n",
      "363/363 [==============================] - 0s 952us/step - loss: 0.4814 - huber_loss: 0.2306 - val_loss: 1.8444 - val_huber_loss: 0.2703\n",
      "Epoch 5/5\n",
      "363/363 [==============================] - 0s 850us/step - loss: 0.4646 - huber_loss: 0.2259 - val_loss: 1.0392 - val_huber_loss: 0.2481\n"
     ]
    }
   ],
   "source": [
    "history = compile_train(model, (x_train_scaled, y_train), (x_valid_scaled, y_valid), mse, Nadam(), create_huber(2.), 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- If we use the same func as loss and metric, we can find that both gives different results.\n",
    "- This is mostly due to floating point precision error: even though the mathematicla equations of both are same, the operations are not run in the same order, which can lead to small differences.\n",
    "- The loss since the start of an epoch is the mean of all batch losses seen so far. Each batch loss is the sum of weighted instances divided by the batch size.\n",
    "- The metric since the start of the epoch is the sum of the weighted instance losses seen so far divided by the sum of weights. It is the weighted mean of all the instance losses.\n",
    "- Loss = metric * mean of sample weights (plus some floating point precision error factor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense (Dense)                (None, 30)                270       \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 31        \n",
      "=================================================================\n",
      "Total params: 301\n",
      "Trainable params: 301\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss = create_huber(2.), optimizer = Nadam(), metrics = [create_huber(2.)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_weights = np.random.rand(len(y_train)) # Creating a sample of weights to be used in the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "WARNING:tensorflow:AutoGraph could not transform <function create_huber.<locals>.huber_loss at 0x7faebc476670> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: unexpected EOF while parsing (<unknown>, line 1)\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function create_huber.<locals>.huber_loss at 0x7faebc476670> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: unexpected EOF while parsing (<unknown>, line 1)\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING:tensorflow:AutoGraph could not transform <function create_huber.<locals>.huber_loss at 0x7faebc476790> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: unexpected EOF while parsing (<unknown>, line 1)\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function create_huber.<locals>.huber_loss at 0x7faebc476790> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: unexpected EOF while parsing (<unknown>, line 1)\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "363/363 [==============================] - 0s 925us/step - loss: 0.4455 - huber_loss: 0.8931\n",
      "Epoch 2/5\n",
      "363/363 [==============================] - 0s 939us/step - loss: 0.1305 - huber_loss: 0.2662\n",
      "Epoch 3/5\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.1176 - huber_loss: 0.2381\n",
      "Epoch 4/5\n",
      "363/363 [==============================] - 0s 876us/step - loss: 0.1140 - huber_loss: 0.2321\n",
      "Epoch 5/5\n",
      "363/363 [==============================] - 0s 832us/step - loss: 0.1116 - huber_loss: 0.2272\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(x_train_scaled, y_train, epochs = 5, sample_weight = sample_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.44554394483566284, 0.44320641348494266)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "history.history['loss'][0], history.history['huber_loss'][0] * sample_weights.mean() # Loss = metric * mean of sample weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Streaming metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=float32, numpy=0.8>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "precision = keras.metrics.Precision() # Creating a precision metric\n",
    "precision([0, 1, 1, 1, 0, 1, 0, 1], [1, 1, 0, 1, 0, 1, 0, 1]) # Giving it the labels and predicted values of the first batch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=float32, numpy=0.5>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "precision([0, 1, 0, 0, 1, 0, 1, 1], [1, 0, 1, 1, 0, 0, 0, 0]) # Giving the metric the labels and the predicted values of the second batch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=float32, numpy=0.5>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "precision.result()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Variable 'true_positives:0' shape=(1,) dtype=float32, numpy=array([4.], dtype=float32)>,\n",
       " <tf.Variable 'false_positives:0' shape=(1,) dtype=float32, numpy=array([4.], dtype=float32)>]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "precision.variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "precision.reset_states() # Resetting the metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Huber_metric(keras.metrics.Metric):\n",
    "    def __init__(self, threshold = 1., **kwargs):\n",
    "        super().__init__(**kwargs) # Handles kwargs of base class\n",
    "        self.threshold = threshold\n",
    "        self.total = self.add_weight('total', initializer = 'zeros') # variable for storing the total metric value for all the batches.\n",
    "        self.count = self.add_weight('count', initializer = 'zeros') # variable for storing the no. of instances\n",
    "    def huber_fn(self, y_true, y_pred): # Func for cacculating huber loss\n",
    "        error = y_true - y_pred\n",
    "        small_loss = tf.abs(error) < self.threshold\n",
    "        squared_loss = tf.square(error) / 2\n",
    "        linear_loss = self.threshold * tf.abs(error) - (tf.square(self.threshold) / 2)\n",
    "        return tf.where(small_loss, squared_loss, linear_loss)\n",
    "    def update_state(self, y_true, y_pred, sample_weight = None):\n",
    "        metric = self.huber_fn(y_true, y_pred) # Huber loss is calculated\n",
    "        self.total.assign_add(tf.reduce_sum(metric)) # The sum of the metrics of all the instances is added to the variable\n",
    "        self.count.assign_add(tf.cast(tf.size(y_true), tf.float32)) # The total no. of instances is added to the variable\n",
    "    def result(self):\n",
    "        return self.total / self.count # The Huber metric over all the instances.\n",
    "        # When we use the metrics func, first the update_state method is called then the result method and the output is returned.\n",
    "    def get_config(self): # Method to save the threshold along with the model\n",
    "        base_config = super().get_config() # Config params from the base class\n",
    "        return {**base_config, 'threshold' : self.threshold}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = Huber_metric(2.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=float32, numpy=14.0>"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m(tf.constant([[2.]]), tf.constant([[10.]]))\n",
    "# total = 2 * |10 - 2| - 2^2 / 2 = 14\n",
    "# count = 1\n",
    "# metric = total / count = 14"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=float32, numpy=7.0>"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m(tf.constant([[0.], [5.]]), tf.constant([[1.], [9.25]]))\n",
    "m.result()\n",
    "# total = total + (|1 - 0|² / 2) + (2 * |9.25 - 5| - 2² / 2) = 14 + 7 = 21\n",
    "# count = count + 2 = 3\n",
    "# result = total / count = 21 / 3 = 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Variable 'total:0' shape=() dtype=float32, numpy=21.0>,\n",
       " <tf.Variable 'count:0' shape=() dtype=float32, numpy=3.0>]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m.variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Variable 'total:0' shape=() dtype=float32, numpy=0.0>,\n",
       " <tf.Variable 'count:0' shape=() dtype=float32, numpy=0.0>]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m.reset_states()\n",
    "m.variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense (Dense)                (None, 30)                270       \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 31        \n",
      "=================================================================\n",
      "Total params: 301\n",
      "Trainable params: 301\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "pre_model()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss = create_huber(2.), optimizer = Nadam(), metrics = [Huber_metric(2.)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "WARNING:tensorflow:AutoGraph could not transform <function create_huber.<locals>.huber_loss at 0x7fae5f00c820> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: unexpected EOF while parsing (<unknown>, line 1)\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function create_huber.<locals>.huber_loss at 0x7fae5f00c820> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: unexpected EOF while parsing (<unknown>, line 1)\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.2237 - huber_metric: 0.2237 - val_loss: 0.2958 - val_huber_metric: 0.2958\n",
      "Epoch 2/5\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.2196 - huber_metric: 0.2196 - val_loss: 0.2754 - val_huber_metric: 0.2754\n",
      "Epoch 3/5\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.2164 - huber_metric: 0.2164 - val_loss: 0.2500 - val_huber_metric: 0.2500\n",
      "Epoch 4/5\n",
      "363/363 [==============================] - 0s 966us/step - loss: 0.2128 - huber_metric: 0.2128 - val_loss: 0.2212 - val_huber_metric: 0.2212\n",
      "Epoch 5/5\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.2099 - huber_metric: 0.2099 - val_loss: 0.1958 - val_huber_metric: 0.1958\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(x_train_scaled, y_train, epochs = 5, validation_data = (x_valid_scaled, y_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/gokul/.local/lib/python3.8/site-packages/tensorflow/python/training/tracking/tracking.py:111: Model.state_updates (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This property should not be used in TensorFlow 2.0, as updates are applied automatically.\n",
      "WARNING:tensorflow:From /home/gokul/.local/lib/python3.8/site-packages/tensorflow/python/training/tracking/tracking.py:111: Layer.updates (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This property should not be used in TensorFlow 2.0, as updates are applied automatically.\n",
      "INFO:tensorflow:Assets written to: Custom_huber_metric_model/assets\n"
     ]
    }
   ],
   "source": [
    "model.save('Custom_huber_metric_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['loss', 'huber_metric']"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.metrics_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.0"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.metrics[-1].threshold"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- We can create a more simple custom metric using the keras.metrics.mean class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Huber_metric_mean(keras.metrics.Mean):\n",
    "    def __init__(self, threshold = 1., name = 'Huber_metric_mean', dtype = None):\n",
    "        self.threshold = threshold\n",
    "        self.huber_fn = create_huber(threshold) # Creating the huber metric func\n",
    "        super().__init__(name = name, dtype = dtype)\n",
    "    def update_state(self, y_true, y_pred, sample_weight = None):\n",
    "        metric = self.huber_fn(y_true, y_pred) # Calculating huber metric for each instance\n",
    "        super(Huber_metric_mean, self).update_state(metric, sample_weight) # Recursively updating the metric value and the sample_weights\n",
    "    def get_config(self):\n",
    "        base_config = super().get_config() # Getting the config params from the base class\n",
    "        return {**base_config, 'threshold' : self.threshold} # Saving threshold along with the rest of the params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense (Dense)                (None, 30)                270       \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 31        \n",
      "=================================================================\n",
      "Total params: 301\n",
      "Trainable params: 301\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "pre_model()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss = create_huber(2.), optimizer = Nadam(), metrics = [Huber_metric_mean(2.)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "WARNING:tensorflow:AutoGraph could not transform <function create_huber.<locals>.huber_loss at 0x7fae5fc51b80> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: unexpected EOF while parsing (<unknown>, line 1)\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function create_huber.<locals>.huber_loss at 0x7fae5fc51b80> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: unexpected EOF while parsing (<unknown>, line 1)\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING:tensorflow:AutoGraph could not transform <function create_huber.<locals>.huber_loss at 0x7fae5fc51820> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: unexpected EOF while parsing (<unknown>, line 1)\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function create_huber.<locals>.huber_loss at 0x7fae5fc51820> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: unexpected EOF while parsing (<unknown>, line 1)\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.0948 - Huber_metric_mean: 0.1929 - val_loss: 0.1825 - val_Huber_metric_mean: 0.1825\n",
      "Epoch 2/5\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.0944 - Huber_metric_mean: 0.1917 - val_loss: 0.2078 - val_Huber_metric_mean: 0.2078\n",
      "Epoch 3/5\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.0945 - Huber_metric_mean: 0.1919 - val_loss: 0.1806 - val_Huber_metric_mean: 0.1806\n",
      "Epoch 4/5\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.0935 - Huber_metric_mean: 0.1905 - val_loss: 0.2016 - val_Huber_metric_mean: 0.2016\n",
      "Epoch 5/5\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.0932 - Huber_metric_mean: 0.1893 - val_loss: 0.1795 - val_Huber_metric_mean: 0.1795\n"
     ]
    }
   ],
   "source": [
    "sample_weight = np.random.rand(len(y_train))\n",
    "history = model.fit(x_train_scaled, y_train, epochs = 5, validation_data = (x_valid_scaled, y_valid), sample_weight = sample_weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.09483620524406433, 0.09570989377475683)"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "history.history['loss'][0], history.history['Huber_metric_mean'][0] * sample_weight.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: Custom_huber_metric_model_v2/assets\n"
     ]
    }
   ],
   "source": [
    "model.save('Custom_huber_metric_model_v2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.0"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.metrics[-1].threshold"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
