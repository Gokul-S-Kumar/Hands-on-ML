{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from pathlib import Path\n",
    "import os\n",
    "import numpy as np\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "DOWNLOAD_ROOT = 'http://ai.stanford.edu/~amaas/data/sentiment/'\n",
    "FILENAME = 'aclImdb_v1.tar.gz'\n",
    "filepath = keras.utils.get_file(FILENAME, DOWNLOAD_ROOT + FILENAME, extract = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = Path(filepath).parent / 'aclImdb'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(path.parts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "aclImdb/\n",
      "    README\n",
      "    imdb.vocab\n",
      "    imdbEr.txt\n",
      "    test/\n",
      "        labeledBow.feat\n",
      "        urls_neg.txt\n",
      "        urls_pos.txt\n",
      "        neg/\n",
      "            0_2.txt\n",
      "            10000_4.txt\n",
      "            10001_1.txt\n",
      "            ...\n",
      "        pos/\n",
      "            0_10.txt\n",
      "            10000_7.txt\n",
      "            10001_9.txt\n",
      "            ...\n",
      "    train/\n",
      "        labeledBow.feat\n",
      "        unsupBow.feat\n",
      "        urls_neg.txt\n",
      "        ...\n",
      "        neg/\n",
      "            0_3.txt\n",
      "            10000_4.txt\n",
      "            10001_4.txt\n",
      "            ...\n",
      "        pos/\n",
      "            0_9.txt\n",
      "            10000_8.txt\n",
      "            10001_10.txt\n",
      "            ...\n",
      "        unsup/\n",
      "            0_0.txt\n",
      "            10000_0.txt\n",
      "            10001_0.txt\n",
      "            ...\n"
     ]
    }
   ],
   "source": [
    "# Displaying the filepaths\n",
    "for name, subdirs, files in os.walk(path):\n",
    "    indent = len(Path(name).parts) - len(path.parts)\n",
    "    print('    ' * indent + Path(name).parts[-1] + os.sep)\n",
    "    for index, filename in enumerate(sorted(files)):\n",
    "        if index == 3:\n",
    "            print('    ' * (indent + 1) + '...')\n",
    "            break\n",
    "        print('    ' * (indent + 1) + filename)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Func to store the filepaths as strings\n",
    "def review_paths(dirpath):\n",
    "    return [str(path) for path in dirpath.glob('*.txt')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_pos = review_paths(path / 'train' / 'pos')\n",
    "train_neg = review_paths(path / 'train' / 'neg')\n",
    "test_valid_pos = review_paths(path / 'test' / 'pos')\n",
    "test_valid_neg = review_paths(path / 'test' / 'neg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(12500, 12500, 12500, 12500)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_pos), len(train_neg), len(test_valid_pos), len(test_valid_neg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Splitting the test set into test and validation sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.shuffle(test_valid_pos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_pos = test_valid_pos[:5000]\n",
    "valid_pos = test_valid_pos[5000:]\n",
    "test_neg = test_valid_neg[:5000]\n",
    "valid_neg = test_valid_neg[5000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5000, 5000, 7500, 7500)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test_pos), len(test_neg), len(valid_pos), len(valid_neg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use tf.data for creating a dataset for each set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Since the dataset fits in memory, we can use a simple python func to create a dataset using the from_tenso_slices() method.\n",
    "def imdb_dataset(filepaths_pos, filepaths_neg):\n",
    "    reviews = []\n",
    "    labels = []\n",
    "    for filepaths, label in ((filepaths_neg, 0), (filepaths_pos, 1)):\n",
    "        for filepath in filepaths:\n",
    "            with open(filepath) as review_file:\n",
    "                reviews.append(review_file.read())\n",
    "            labels.append(label)\n",
    "    return tf.data.Dataset.from_tensor_slices((tf.constant(reviews), tf.constant(labels))) # Creating the dataset from the tensors "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(b'It\\'s been a while since seeing this the first time, so I watched it again with the second movie in the series. While I realize there is a 3rd movie out that I haven\\'t seen yet, I\\'ll review under the original title...<br /><br />Just from the standpoint of production value, screen writing, and movie making, this movie fails on many levels, though it succeeds on a few as well. What can you expect from a low-budget, \"B\" movie? Not much, and it works from the standpoint of production. However, the writing is certainly disjointed, with little in the way of character development...exactly what I\\'d expect when there is an agenda to a film. I didn\\'t have a problem with the acting...the cast is solid; however, the screenplay in both movies gives the actors little opportunity to really stretch themselves. Because the film is \"Christian,\" this is predictable, as you can\\'t very well portray violent chaos of the \"end times\" without also breaking some of the ethics which are normally associated with Christianity. In other words, the mistake comes in making this into a G-rated film when the content, even in the most conservative of Bible interpretations, would be R-rated by any measure. So, if the purpose of the movie is to scare people into Christian faith, then the movie should be somewhat scary, right? However, you can\\'t comment on a film adaptation from a book without commenting on the book, or in this case, series of books. There are certainly plenty of Christian materials worthy enough to be made into movies...but not the \"Left Behind\" series...and these movies ultimately fail because, while being best-sellers, they are poorly written novels based on bad theology.<br /><br />As a Southern Baptist minister, I confess that the books were a guilty pleasure for me, though I have yet to finish the last two books of the series. I have described them as decent fiction, and if the books would take the point of view that this is one \"possibility\" or interpretation of the subject of biblical eschatology (study of the \"end times), then I could live with that. However, this series is divisive in Christian circles because it promotes the \"literalist\" interpretation of all Scripture above a more proper hermeneutic. Inevitably, this leads to the \"pre-trib, pre-millenial\" dispensation point of view, which confines an all-powerful God far too by humanity\\'s world. In other words, as I\\'ve always said, God shouldn\\'t need our helicopters and bombs to do his ultimate work. But because many people, particularly unstudied Christians, can\\'t think beyond their own world-views, we are left with a pro-conservative, fundamentalist stance with regard to Bible interpretation, and attempts to push it through as the \"only\" interpretation.<br /><br />Thus, the books carry with them an agenda, not so much to get the \"lost\" to understand their need for Christ, but to state that the fundamentalist point of view is the only valid way to understand the Bible. I recall very clearly reading (several years ago) in the second novel a scene where the characters reference a person who was \"left behind\" BECAUSE of his non-adherence to this point of view; as if \"real\" christians worthy to be \"raptured\" couldn\\'t possibly hold to another eschatology. This is disturbing for several reasons, the least of which is because a \"rapture\" is only briefly mentioned in Scripture and it\\'s connection to real, end-time prophecy is tenuous at best.<br /><br />But the real issue with these books is comes in the way they divide the Christian community and how they portray \"true\" Christian behavior. Ultimately, I feel they harden more people to an otherwise legitimate faith/religion instead of win people towards it. It turns all Christians into caricatures, equally disdained and laughed at by the world despite the fact that there is theological room for a wide diversity of believes within Christian thought and practice. As a Christian body, on the whole, we\\'ve done enough of that kind of damage to society over 2000 years of history...and we certainly don\\'t need to promote it by film to thousands, maybe millions of others.<br /><br />Thus, the \"Left Behind\" movies fail because the \"Left Behind\" books aren\\'t worthy to be interpreted into movies.', shape=(), dtype=string)\n",
      "tf.Tensor(0, shape=(), dtype=int32)\n",
      "\n",
      "tf.Tensor(b'Laughed a lot - because it is so incredibly bad - sorry folks, but definitely one of the worst movies I have ever seen... I know it is low budget, but anyway: the actors behave like playing in a soap, the dialogues are absolutely crappy and the last time I have seen such odd pictures was at a trash nite at some youth video festival ten years ago. I really appreciate that people gather together and shoot cheap movies, but at least a certain amount of quality should be accomplished. But at least one good thing: the first three minutes of the movie were quiet interesting and looked okay - and the score was really worth listening to. The DVD cover promised a lot, but that is by far the best this film has to offer...', shape=(), dtype=string)\n",
      "tf.Tensor(0, shape=(), dtype=int32)\n",
      "\n",
      "tf.Tensor(b'This movie is horrible! It rivals \"Ishtar\" in the number of embarrassingly bad moments. I would have rated it lower than a 3, save for a couple of funny lines; but, overall, this film was crap! It looked like they made it over a weekend at some bankrupt resort somewhere. Joe Roth should join Elaine May on the directing sidelines forever!', shape=(), dtype=string)\n",
      "tf.Tensor(0, shape=(), dtype=int32)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for x, y in imdb_dataset(train_pos, train_neg).take(3):\n",
    "    print(x)\n",
    "    print(y)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23.5 s ± 0 ns per loop (mean ± std. dev. of 1 run, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%timeit -r1 for x, y in imdb_dataset(train_pos, train_neg).repeat(10): pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- It took 16 secs to load the dataset and go through it 10 times."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- If the dataset didnt fit in memory we would have to load it using tf.data or convert it into TFRecord file. Luckily each review fits on just one line and is seperated by  `<br />` ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def imdb_dataset(filepaths_pos, filepaths_neg, n_parallel_threads = 5):\n",
    "    dataset_neg = tf.data.TextLineDataset(filepaths_neg, num_parallel_reads = n_parallel_threads)\n",
    "    dataset_neg = dataset_neg.map(lambda review : (review, 0))\n",
    "    dataset_pos = tf.data.TextLineDataset(filepaths_pos, num_parallel_reads = n_parallel_threads)\n",
    "    dataset_pos = dataset_pos.map(lambda review : (review, 1))\n",
    "    return tf.data.Dataset.concatenate(dataset_pos, dataset_neg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1min 4s ± 0 ns per loop (mean ± std. dev. of 1 run, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%timeit -r1 for x, y in imdb_dataset(train_pos, train_neg).repeat(10) : pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- It now takes 30 secs to do the same.This is much slower because the dataset is not cached in RAM and it must be loaded each time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25.2 s ± 0 ns per loop (mean ± std. dev. of 1 run, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%timeit -r1 for x, y in imdb_dataset(train_pos, train_neg).cache().repeat(10) : pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The time taken has reduced as we cached the dataset in the RAM without loading it each time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "train_set = imdb_dataset(train_pos, train_neg).shuffle(25000).batch(batch_size).prefetch(1)\n",
    "valid_set = imdb_dataset(valid_pos, valid_neg).batch(batch_size).prefetch(1)\n",
    "test_set = imdb_dataset(test_pos, test_neg).batch(batch_size).prefetch(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- We will first define a func to prerpocess the data. It will crop them to 300 characters, converting them to lower cases then replacing the `<br />` and all other non-letter characters to spaces, splitting the reviews into words and finally padding or cropping each review so it ends up with exactly n_words tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(x_batch, n_words = 50):\n",
    "    shape = tf.shape(x_batch) * tf.constant([1, 0]) + tf.constant([0, n_words])\n",
    "    z = tf.strings.substr(x_batch, 0, 300) # Converting the main review to strings containing 300 characters.\n",
    "    z = tf.strings.lower(z) # converting the text to lower case\n",
    "    z = tf.strings.regex_replace(z, b'<br\\\\s*/?>', b' ') # Converting the non-letter characters to spaces.\n",
    "    z = tf.strings.regex_replace(z, b'[^a-z]', b' ')\n",
    "    z = tf.strings.split(z)\n",
    "    return z.to_tensor(shape = shape, default_value = b'<pad>')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_example = tf.constant([\"It's a great, great movie! I loved it.\", \"It was terrible, run away!!!\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2,), dtype=string, numpy=\n",
       "array([b\"It's a great, great movie! I loved it.\",\n",
       "       b'It was terrible, run away!!!'], dtype=object)>"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2, 50), dtype=string, numpy=\n",
       "array([[b'it', b's', b'a', b'great', b'great', b'movie', b'i', b'loved',\n",
       "        b'it', b'<pad>', b'<pad>', b'<pad>', b'<pad>', b'<pad>',\n",
       "        b'<pad>', b'<pad>', b'<pad>', b'<pad>', b'<pad>', b'<pad>',\n",
       "        b'<pad>', b'<pad>', b'<pad>', b'<pad>', b'<pad>', b'<pad>',\n",
       "        b'<pad>', b'<pad>', b'<pad>', b'<pad>', b'<pad>', b'<pad>',\n",
       "        b'<pad>', b'<pad>', b'<pad>', b'<pad>', b'<pad>', b'<pad>',\n",
       "        b'<pad>', b'<pad>', b'<pad>', b'<pad>', b'<pad>', b'<pad>',\n",
       "        b'<pad>', b'<pad>', b'<pad>', b'<pad>', b'<pad>', b'<pad>'],\n",
       "       [b'it', b'was', b'terrible', b'run', b'away', b'<pad>', b'<pad>',\n",
       "        b'<pad>', b'<pad>', b'<pad>', b'<pad>', b'<pad>', b'<pad>',\n",
       "        b'<pad>', b'<pad>', b'<pad>', b'<pad>', b'<pad>', b'<pad>',\n",
       "        b'<pad>', b'<pad>', b'<pad>', b'<pad>', b'<pad>', b'<pad>',\n",
       "        b'<pad>', b'<pad>', b'<pad>', b'<pad>', b'<pad>', b'<pad>',\n",
       "        b'<pad>', b'<pad>', b'<pad>', b'<pad>', b'<pad>', b'<pad>',\n",
       "        b'<pad>', b'<pad>', b'<pad>', b'<pad>', b'<pad>', b'<pad>',\n",
       "        b'<pad>', b'<pad>', b'<pad>', b'<pad>', b'<pad>', b'<pad>',\n",
       "        b'<pad>']], dtype=object)>"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocess(x_example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utility func that will take a data sample and will output the list of the top max_size most frequent words.\n",
    "def get_vocabulary(data_sample, max_size = 1000):\n",
    "    preprocessed_reviews = preprocess(data_sample).numpy() # Preprocessing the input using the func defined above.\n",
    "    counter = Counter() # Initializing the counter\n",
    "    for words in preprocessed_reviews:\n",
    "        for word in words:\n",
    "            if word != b'<pad>':\n",
    "                counter[word] += 1\n",
    "    return [b'<pad>'] + [word for word, count in counter.most_common(max_size)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[b'<pad>',\n",
       " b'it',\n",
       " b'great',\n",
       " b's',\n",
       " b'a',\n",
       " b'movie',\n",
       " b'i',\n",
       " b'loved',\n",
       " b'was',\n",
       " b'terrible',\n",
       " b'run',\n",
       " b'away']"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_vocabulary(x_example)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building the custom layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom layer class to convert the text to vectors by the help of lookup tables.\n",
    "class Text_vectorization(keras.layers.Layer):\n",
    "    def __init__(self, max_vocabulary_size = 1000, n_oov_buckets = 100, dtype = tf.string, **kwargs): # Init method\n",
    "        super().__init__(dtype = dtype, **kwargs)\n",
    "        self.max_vocabulary_size = max_vocabulary_size # Maximum size of the vocabulary\n",
    "        self.n_oov_buckets = n_oov_buckets # Size of out-of-vocabulary buckets\n",
    "    def adapt(self, data_sample):\n",
    "        self.vocab = get_vocabulary(data_sample, self.max_vocabulary_size) # Creating the vocabulary\n",
    "        words = tf.constant(self.vocab) # Converting the vocabulary to a tensor\n",
    "        word_ids = tf.range(len(self.vocab), dtype = tf.int64) # Initializing the indices for the lookup table\n",
    "        vocab_init = tf.lookup.KeyValueTensorInitializer(words, word_ids) # Creating teh Key Value Tensor\n",
    "        self.table = tf.lookup.StaticVocabularyTable(vocab_init, self.n_oov_buckets) # Creating the lookup table\n",
    "    def call(self, inputs):\n",
    "        preprocessed_inputs = preprocess(inputs) # Preprocessing the inputs\n",
    "        return self.table.lookup(preprocessed_inputs) # Returning the vectorized texts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_vectorization = Text_vectorization()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_vectorization.adapt(x_example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2, 50), dtype=int64, numpy=\n",
       "array([[ 1,  3,  4,  2,  2,  5,  6,  7,  1,  0,  0,  0,  0,  0,  0,  0,\n",
       "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "         0,  0],\n",
       "       [ 1,  8,  9, 10, 11,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "         0,  0]])>"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_vectorization(x_example)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- We can see that each review was cleaned up and tokenized, then each word was encoded as its index in the vocabulary. All the 0s correspond to the `<pad>` token.\n",
    "- Now create another instance of the Text_vectorization() layer and adapt it to the entire IMDB dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_vocabulary_size = 1000\n",
    "n_oov_buckets = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_review_batches = train_set.map(lambda review, label : review) # Extracting only the reviews out the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_reviews = np.concatenate(list(sample_review_batches.as_numpy_iterator()), axis = 0) # Creating a numpy array of the reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_vectorization = Text_vectorization(max_vocabulary_size, n_oov_buckets, input_shape = [])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_vectorization.adapt(sample_reviews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2, 50), dtype=int64, numpy=\n",
       "array([[  9,  14,   2,  64,  64,  12,   5, 257,   9,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
       "       [  9,  13, 269, 530, 335,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0]])>"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_vectorization(x_example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[b'<pad>', b'the', b'a', b'of', b'and', b'i', b'to', b'is', b'this', b'it']"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_vectorization.vocab[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The above are the top 10 used words in the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Now we have to encode these word ids before feeding to the model. One method is to  use bag of words, for each review and for each word in the vocabulary we count the no. of occurences of that word in that review."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2, 4), dtype=float32, numpy=\n",
       "array([[2., 2., 0., 1.],\n",
       "       [3., 0., 2., 0.]], dtype=float32)>"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "simple_example = tf.constant([[1, 3, 1, 0, 0], [2, 2, 0, 0, 0]])\n",
    "tf.reduce_sum(tf.one_hot(simple_example, 4), axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The first review has the word zero 2 times, one 2 times, two 0 times, 3 one time. So we have denoted the count respectively. The same applies for the seconf part."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2, 5, 4), dtype=float32, numpy=\n",
       "array([[[0., 1., 0., 0.],\n",
       "        [0., 0., 0., 1.],\n",
       "        [0., 1., 0., 0.],\n",
       "        [1., 0., 0., 0.],\n",
       "        [1., 0., 0., 0.]],\n",
       "\n",
       "       [[0., 0., 1., 0.],\n",
       "        [0., 0., 1., 0.],\n",
       "        [1., 0., 0., 0.],\n",
       "        [1., 0., 0., 0.],\n",
       "        [1., 0., 0., 0.]]], dtype=float32)>"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.one_hot(simple_example, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Bag_of_words(keras.layers.Layer):\n",
    "    def __init__(self, n_tokens, dtype = tf.int32, **kwargs):\n",
    "        super().__init__(dtype = dtype, **kwargs)\n",
    "        self.n_tokens = n_tokens\n",
    "    def call(self, inputs):\n",
    "        one_hot = tf.one_hot(inputs, self.n_tokens) # One-hot encoding the review\n",
    "        return tf.reduce_sum(one_hot, axis = 1)[:, 1:] # Omitting the <pad> as it occurs the most"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2, 3), dtype=float32, numpy=\n",
       "array([[2., 0., 1.],\n",
       "       [0., 2., 0.]], dtype=float32)>"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bag_of_words = Bag_of_words(n_tokens = 4)\n",
    "bag_of_words(simple_example)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- It works fine. Now create another instance with the size required for the train set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_tokens = max_vocabulary_size + n_oov_buckets + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "bag_of_words = Bag_of_words(n_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Now we can build and train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.Sequential()\n",
    "model.add(text_vectorization)\n",
    "model.add(bag_of_words)\n",
    "model.add(keras.layers.Dense(100, activation = 'relu'))\n",
    "model.add(keras.layers.Dense(1, activation = 'sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss = keras.losses.binary_crossentropy, optimizer = 'nadam', metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "782/782 [==============================] - 5s 6ms/step - loss: 0.5425 - accuracy: 0.7173 - val_loss: 0.5121 - val_accuracy: 0.7395\n",
      "Epoch 2/5\n",
      "782/782 [==============================] - 4s 5ms/step - loss: 0.4695 - accuracy: 0.7708 - val_loss: 0.5058 - val_accuracy: 0.7423\n",
      "Epoch 3/5\n",
      "782/782 [==============================] - 4s 5ms/step - loss: 0.4176 - accuracy: 0.8078 - val_loss: 0.5184 - val_accuracy: 0.7357\n",
      "Epoch 4/5\n",
      "782/782 [==============================] - 4s 5ms/step - loss: 0.3431 - accuracy: 0.8551 - val_loss: 0.5540 - val_accuracy: 0.7329\n",
      "Epoch 5/5\n",
      "782/782 [==============================] - 4s 5ms/step - loss: 0.2564 - accuracy: 0.9065 - val_loss: 0.5770 - val_accuracy: 0.7284\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fe815748e50>"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(train_set, epochs = 5, validation_data = valid_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[b'I was honestly surprised by Alone in the Dark. It was so bad, I could hardly believe what I was seeing. There are no characters, just a few stereotypes wandering around and getting killed. The extent of the character development was giving each character a name and an occupation, and that\\'s about it. There was no real plot, and none of the characters seemed to have any motivation. In fact, many action scenes just began on their own, coming from nowhere with a pounding techno track. While I was watching this movie I kept asking \"Where is this happening? What\\'s going on?\" The acting was high school drama quality, with stiff wooden delivery, as though the actors were reading from cue cards without comprehending their lines. Their trouble delivering lines was made even more obvious by horrible sound design. ADR sounded like it was recorded in an open room. The actors were constantly taking obvious care to hit their marks, looking almost robotic in their movements. So, these listless automatons are whisked through a series of implausible and confusing scenarios, often without even the benefit of transition scenes. They were here, now they\\'re there. This was happening, now that\\'s happening. Random scenes with little rhyme or reason. I had a lot of fun watching it. Definitely not worth nine bucks though.'\n",
      " b\"A high school principal (Keenan Wynn) with a losing basketball team unwittingly hires a coach who turns out not only to be a gorgeous blond woman (Cathy Lee Crosby) but a catalyst for their new winning ways. Are you really surprised? Along the way a romance grows between the coach and the team's star player Jack (Michael Biehn). The police are never notified.<br /><br />Packaged along with other Crown International Pictures as a grindhouse movie really does this film no service. This can easily be edited into a television movie of the week. Cathy Lee Crosby looks great as coach Randy Rawlings especially in her skimpy outfits but I expected more than mere titillation from an R-rated film. A side plot involving a dorky center who is hypnotized by his teammates into thinking he is former NBA player Sydney Wicks is the actual reason for the team's new success rather than Cathy Lee's coaching. Too much tease and not enough sleaze makes this a major disappointment.\"\n",
      " b\"Unless you are an Evangelical Christian then make like an Egyptian and avoid like the biblical plague.<br /><br />Awful - why oh why does IMDb list the most favourable reviews at the top of the list - it was due to one of these that I have just wasted the end of what started out as good evening on this claptrap.<br /><br />The plot premise started out strong enough - I was drawn into the film and was interested right up to the point where the Bible sermons took over. What a waste.<br /><br />This film has so incensed me that I have registered with IMDb for the first time just to complain about it - I hope at least that by doing so I save someone else's evening.<br /><br />Hay - what a Christian act on my part ;-)\"\n",
      " b\"This movie was absolutely terrible. I can't believe I paid to see it in the theatre. I wouldn't watch it on free cable t.v. I'm surprised that Joe Magtena even made it. Do not waste your time with this movie.\"\n",
      " b'The master of cheap erotic horror, Rolfe Kanefsky, finally makes a movie that doesn\\'t go straight to the Playboy Channel. \"The Hazing\" borrows heavily from everything that came before it from Nightmare on Elm Street to Evil Dead, but still manages to do it with enough humor to make it watchable... just barely. The characters are cardboard, the dialogue is wooden, the story is paper-thin and the actors couldn\\'t act their way out of a grocery bag. Put that all together and you have a pulpy ball of mulch for a movie. Sometimes, when I\\'m bored, I like to eat paper. Watching this movie is a lot like that. Chew on it for 90 minutes and you\\'re left with a weird taste in your mouth and no nutritional value.'\n",
      " b\"A woman (Sylvia Kristel) seduces a 15 year old boy (Eric Brown). They have sex...but it's all tied into some stupid plot or something.<br /><br />Easily one of the most disturbing sex comedies ever. Does anyone realize this movie is making light of child molestation? I suppose it's OK cause it's a teenage boy--if we had one with a man seducing a teenage girl there would (rightfully) be outrage. Sorry, but having it done to a boy doesn't excuse it. It's still sick. I realize Brown was of age (he was actually 18 when this was made) but he LOOKS 15. I just find it disturbing that some people find this OK.<br /><br />Plot aside the acting sucks (Kristel is beautiful--but can't act; Brown is easily one of the worst child actors I've ever seen) and the constant nudity gets boring and isn't even remotely erotic.<br /><br />I saw this drivel at a theatre back in 1981. I was 19 and with my 14 year old cousin (who could easily pass for 18). HE wanted to see it--I didn't but I decideD what the heck? We got in and I actually bought tickets for three teenage boys who were obviously underage. My cousin thought is was boring and the three other kids left halfway through! Let me make this clear--three TEENAGE BOYS left a movie with tons of female nudity! That should give you an idea of how bad this is. I'm surprised this was ever released. A 1 all the way.\"\n",
      " b\"Mahatma has been depicted as a man who neglected his own son in this movie. Don't get me wrong I am not condemning the movie; it is such a wonderful movie and walked out of the cinema with a lump in my throat. We need to understand Mahatma's spiritual standing, he is a true spiritual leader. Only a fully enlightened man could possibly detach himself from his loved ones. A man with such caliber leads his family and followers by example. According to the movie, he spoke to his son and try to make him understand where he is coming from, but poor Harilall with so little intellect could not understand his father. When things went wrong with Harilall, Mahatma could remain calm and accepted that his son is a big tragedy. <br /><br />Had it been any other parents, they would have compromised their values to assist the son to get on his feet. Mahatma didn't do that, he is true leader who leads by example.\"\n",
      " b'Robert Taylor as the mad buffalo hunter Charlie Gilson is the main character in this film. At the beginning I was thinking that Charlie would end up redeeming himself like John Wayne in The Searchers or James Stewart in The Naked Spur. But as the film goes along Gilson keeps doing more atrocities until you realize there is no hope for him. Stewart Granger is Sandy McKenzie, who wants to stop hunting because he realizes that the buffaloes will soon be gone and he becomes disgusted by the act of killing. Gilson is a natural killer who makes no distinction between animals or human beings. Debra Paget as the Indian girl is a surprising character considering the self imposed censorship of that time. She lies with Gilson in total resignation even though she hates him. The last scene of a frozen Gilson, is unforgettable.'\n",
      " b'this movie is practically impossible to describe. the alternate title \"Don\\'t Look Up\" is a lot more descriptive. Like most Japanese cinema, the story is not as linear as American. The story revolves around a director who is filming a story about a ww2 deserter. The set is haunted(?) by an actress who died(?) during the filming of a tv show back in the 60s. the director is the ONLY one who saw this show. if you have seen Ringu (the director Hideo Nakata is the same) and liked it, you\\'ll like ghost actress. i loved ghost actress a lot more than ringu. a truly scary and disturbing movie. a 10!'\n",
      " b'This is exactly the type of film that frustrates me the most. Great cast, great director, great story potential, then they ruin it all with a screenplay that goes nowhere...and says nothing while going there! There is no depth here whatsoever. No depth of characters, no depth of plot, no depth of surprise, suspense, or common sense. We know what\\'s happening, we are told how they plan to fix the problem, they fix the problem, throw a surprise at us near the end that fails to generate any suspense, then they end the film abruptly. Wasted opportunity.<br /><br />On the plus side, Glenn Ford leads a cast of UK (and one French) actors who are all fantastic, doing an incredibly impressive job with the one-dimensional writing they were given. One of the absolute favorites is Herbert Walton as \"Old Charlie\", who provides some wonderful bits of humor and warmth to a dark and serious film. I also thought the film had a great look to it...all shadows and fog...very film noir in feel.<br /><br />Even though the actors do the best they can and the directing is enjoyable, it still just isn\\'t enough for me to recommend spending the time to view the film. There are far better Glenn Ford movies out there: The Big Heat, Gilda, Affair in Trinidad, etc.'\n",
      " b'This delectable fusion of New Age babble and luridly bad film-making may not \"open\" you up, to borrow one of the film\\'s favorite verbs, but it might leave your jaw slack and your belly sore from laughter or retching. Based on the best-selling book by James Redfield, first (self) published in 1993, this cornucopia of kitsch tracks the spiritual awakening of an American history teacher (Matthew Settle) who, on traveling to deepest, darkest, phoniest Peru and sniffing either the air or something else more illegal. Namely what he discovers is a schlock Shangri La populated by smiling zombies who may be nuts or just heavily medicated, perhaps because they\\'re often accompanied by a panpipe flourish and an occasional shout out from a celestial choir. Although there\\'s a lot of talk about \"energy,\" that quality is decidedly missing from the motley cast whose numbers include Thomas Kretschmann, Annabeth Gish, Hector Elizondo and Jurgen Prochnow, all of whom are now firmly ensconced in the camp pantheon. For those who care, the plot involves the military, terrorists and the Roman Catholic Church; Armand Mastroianni provided the inept direction while Mr. Redfield, Barnet Bain and Dan Gordon wrote the hoot of a script. In short, easily the worst film seen in 40+ years of viewing movies.'\n",
      " b\"I thought I had seen this movie, twice in fact. Then I read all the other reviews, and they didn't quite match up. A man and three young students, two girls and a boy, go to this town to study alleged bigfoot sightings. I still feel pretty confident that this is the movie I saw, despite the discrepancies in the reviews. Therefore I'm putting my review back: If you like the occasional 'B' movie, as I do, then Return to Boggy Creek is the movie for you! Whether it's setting the sleep timer, and nodding off to your favorite movie-bomb, or just hanging out with friends. Boggy Creek, the mute button, and you've got a fun night of improv. Look out! Is the legend true? I think we just might find out, along with a not-so-stellar cast. Will there be any equipment malfunctions at particularly key moments in the film? Does our blonde, manly, young hero have any chest hair? Will the exceptionally high-tech Technicolor last the entire film? You'll have to watch to find out for yourself.\"\n",
      " b'This ranks way up there on my top list of worst movies I\\'ve seen so far on Starz on Demand. They seem to pick up every straight to DVD crap-fest they can find and put it on here.<br /><br />Why? Who knows! Apparently anyone with a digital camera and a shoestring budget can come up with a horror movie and get it put on TV. To be honest, this looked terrible from the moment I saw the trailer--but I did give it a real chance.<br /><br />I always try to have an open mind about low-budget movies. Some of the best movies I\\'ve ever seen were films that worked around their low budget or in other cases only required that low budget to be great.<br /><br />This is not one of those movies.<br /><br />You know the plot by now, I\\'m sure, if you\\'re reading this. Either you heard about it on Starz on Demand or for whatever reason you ended up on this page out of boredom. It\\'s about a pathetic and whiny girl we get to know for all of 3 minutes in an incredibly bad \"heavy metal\" music video. Whoever put it together must have thought it looked really interesting, but it really, really doesn\\'t. Anyway, she kills herself. Then she possesses someone. Then some killing starts. It\\'s really unmemorable and as completely average and boring as possible. When the first gunshot goes off in her apartment it quite seriously sounds like a piece of popcorn popping. Was that the best sound effect they could come up with? I could find a better sound effect to use for free, (with no copyright,) on the internet... right. now.<br /><br />Don\\'t let the other reviews claiming this is a 10 star movie fool you. They are obviously either distributors of the film or maybe even the director trying to con you into thinking this piece of junk is worth buying.<br /><br />Laughable.'\n",
      " b\"Saw this again recently on Comedy Central. I'd love to see Jean Schertler(Memama) and Emmy Collins(Hippie in supermarket) cast as mother and son in a film, it would probably be the weirdest flicker ever made! Hats off to Waters for making a consistently funny film.\"\n",
      " b'I entered my first comment on this film almost five years ago. Then, the ideas presented in the movie still seemed mostly fictional, if indeed they could ever transpire at all. Not any longer. Now, the politics, society, and media in The Running Man seem very close to home indeed.<br /><br />Consider the following factors, which were mostly absent in 1987 (the year The Running Man came out) that are present today:<br /><br />Concern with, as Richard Dawson\\'s character Damon Killian puts it, \"traditional morality.\" CHECK<br /><br />Entertainment in the form of extreme reality, including pain, fear, and discomfort on the part of contestants. CHECK<br /><br />Cameras everywhere. CHECK<br /><br />Restricted travel for citizens at the whim of the government, controlled by a centralized computer system complete with barcoded passports (\"travel passes\" in the movie) and sanctioned under the guise of national security. CHECK<br /><br />An increased intermingling, bordering on incestuous, of government and media. CHECK<br /><br />Computer-generated graphics that are advanced enough to manipulate real film footage (such as the \"digital matting\" of Ben Richards\\' image onto the stunt double). CHECK<br /><br />Jailing of conscientious objectors or detractors of the current administration. CHECK<br /><br />Flagging economy further widening the gulf between the wealthy and not-so-wealthy; increasing numbers of fringe groups reacting to the tightening noose of big government; civil unrest brewing just under or at the surface of nearly every sizable public event regardless of its origin or intent. CHECK, CHECK, CHECK<br /><br />Then again, maybe it\\'s just a movie based on a Stephen King novella. But just to be safe, I\\'m moving to Switzerland.'\n",
      " b\"I want to state first that I am a Christian (and that I do work in the film and TV industry) so I understand what it is like to work on a feature length film so props to the filmmakers in that regard. I'm all for positive, uplifting messages if they are true to the nature of life (that this is a fallen world and that things don't always work out ... even for followers of Christ). I'm glad that others are having such overwhelmingly positive reactions to the overt Christian message; for me it was just that the execution is where the film fell on its face. A movie lives and dies on its story and here you have one dimensional stereotypes, exposition aplenty, and spontaneous changes in character behavior that are inexplicably to say the least. I believe that a film does not have to club you over the head with its message to get the point across. I'm sure the Kendrick bros. will improve with time and that their storytelling methods will as well. Maybe they could direct someone else's screenplay as their next project.<br /><br />* Sports films are not exactly my first love but a good one (Hoosiers, Field of Dreams, etc) can inspire in a multitude of ways. If you would be interested in a PG-rated film that inspires, give Steven Soderbergh's 'King of the Hill' a look. All truth is God's truth ...\"\n",
      " b'<br /><br />I watched this movie just a little while ago and I found that this movie was terrible! It moved very slowly and was hardly entertaining!<br /><br />Sorry for all those that liked it.... this is only my opinion!'\n",
      " b'As a psychiatrist specialized in trauma, I find this film a beautiful shown example of a severe psychic trauma, even a trauma. It not only explains the enormous difficulties those people have to cope wither, but that even love is sometimes not enough. But she tries!'\n",
      " b\"It's such a shame that because of it's title this film will be avoided by people who hate football. Bend it Like Beckham is much more than a cheesy sports flick. The story line is touching and intelligent without being soppy, the jokes were laugh out loud funny, and the characters are well acted. Parminder Nagra and Keira Knightley are brilliant as teenagers Jess and Jules, putting in great performances both on and off the pitch. Anupam Kher is wonderful as Jess' worried father, and Jonathan Rhys-Meyers, who was so amazingly evil in 'Ride with the Devil,' comes across so well as the nice guy for once, making full use of his gorgeous Irish accent! Even if you don't like football, go see this film. If anything it'll make you smile.\"\n",
      " b'Another American Pie movie has been shoved down our throats and this one is the worst one of them all. It doesn\\'t deserve the name American Pie. They should have stopped at \"The Wedding\".<br /><br />This movie feels like just a stupid porn movie which they slapped the title American Pie on. When i was watching this i felt like i was watching a different series. It doesn\\'t fell like American Pie at all. It has different humor and it is much more rude and has many more sex scenes then the other American Pie movies.<br /><br />I don\\'t recommend it ever. Actually i don\\'t recommend any of the \"American Pie Presents\" movies. Just stick with the nice original trilogy.<br /><br />2/10'\n",
      " b'As B movies go, it was well above average (I warn the reader now that I may reveal certain key elements of the plot or other parts of the movie, although I am trying to minimise any such tendency). As sequels usually go, it was utterly fantastic(despite a \"cookie cutter\" approach to trying to copy certain elements from the original movie verbatum. Despite this sometimes tedious tendency, it seemed to work in this particular film, so long as the viewer could divorce his attention from comparisons to the original \"Scanners\").<br /><br />The movie was similar in ways to the \"Superman\" series, in terms of the main character\\'s description of his early childhood and relationship with his parents (who seemed modelled along the same lines as the Kents in the \"Superman\" stories) and the theme of a morally pure hero possessed of extraordinary powers from an early age, etc. The depiction of profound feelings of alienation of prodigious or otherwise non-conforming children, adolescents and/or adults was a theme which reminded me of films such as \"Real Genius\", and (to a more superficial degree) \"Doctor Mordrid\" and struck a particularly strong chord.<br /><br />The film had a positive message, and was fun to watch. I found some of the insights and accuracy (in terms of depiction of certain aspects of paranormal experiences) fascinating, and even profoundly touching at times. These moments occasionally appeared from among all of the great formula-driven schlock and gratuitous sex(uality, in this case, as the sexual elements were tastefully done) and violence that makes B movies (or Shakespearian plays, for that matter!) so much fun to watch!<br /><br />This is a must watch for all comic book, Sci-fi, \"remote viewing\" enthusiasts, and horror fans! With the right exposure in the right circles, the film could develop quite a cult following, along with the original \"Scanners\".'\n",
      " b'let me say that i love Adam Sandler, watching reign over me i was paying close attention to his acting<br /><br />when he raises his voice, i cant help but think of happy gilmore yelling at a golf ball, then i snap back as Adam Sandler sucks me in<br /><br />Reign over Me is a great film, a film that comes off slow at first with you expecting emotion in every scene<br /><br />Don Cheadle always does a great job and is no exception here with some truly great lines and is worthy of an Oscar in any movie he does<br /><br />adam sandler was amazing in so many ways not only was this his most dramatic/best acted film of his career. but i can recall laughing out loud at many parts of this film<br /><br />The supporting cast was great also with Saffron Burroughs and Jada Pinkett Smith<br /><br />I would highly recommend this movie its got tremendous acting beautiful shots of NYC great comedy great drama And a new found respect for Adam sandler if you ever doubted him or a reassurance at how great Don Cheadle is'\n",
      " b'Though \"The Sopranos\" is yet another gift from the megahit \"The Godfather\" and sequels, which dramatized and to a certain extent glamorized the mafia, \"The Sopranos\" takes another tack. No suited up, classy mobsters here with homes in Lake Tahoe and stakes in Vegas casinos - these guys are goombahs, with a front of waste management, who deal with things that fall off the back of trucks, topless bars, protection money - in short, what the neighborhood mobs were all about.<br /><br />Colorful characters dominate this series, which doesn\\'t hold back on the sex and graphic violence. Tony Soprano (James Gandolfini) is a mob head with a wife and two children, living in New Jersey, who suffers from panic attacks as he tries to balance his biological family with his mafia one. To get to the bottom of his attacks, he sees a psychiatrist, Jennifer Melfi (Lorraine Bracco), who is afraid of him and yet attracted to him at the same time. Tony\\'s henchman - Paulie, his nephew Christopher, his Uncle Junior (the titular head of the mob), his good friend Pussy - are all fully fleshed-out characters.<br /><br />As we learn going through the series, there are enemies not only from without, but from within, and one of those enemies includes Tony\\'s sickly but horrible mother (Nancy Marchand), who convinces Junior that Tony is a danger to him. Tony\\'s sister Janice, meanwhile, is searching for money in her mother\\'s house with a stethoscope and a Geiger counter. Tony has mistress problems, and a wife (Edie Falco) who puts up with a lot because she loves him, all the while keeping ties to her Catholic religion. \"The church frowns on divorce,\" she tells one woman contemplating a split. \"Let the Pope live with him,\" is the response. As far as Tony\\'s mistress problems, his psychiatrist points out that Tony is attracted to demanding women for whom nothing is ever enough, and asks him if it sounds familiar. Yeah, it sounds like his mother.<br /><br />I\\'m of Italian descent, and yes, I\\'m sick of Italians being shown in a negative light and everyone assuming all Italians are mobsters. Yet you can\\'t help liking this show, which is a constant reminder of our culture. (Thanksgiving, it\\'s pointed out, isn\\'t turkey and sweet potato pie - it\\'s the antipasto, the manicotti, the meatballs and escarole, and then the bird!) Not to mention, the right-on pronunciation of words like melenzana (mullinyan), escarole (scarole), manicotti (manigot) etc. The only un-Italian thing about Tony is that he doesn\\'t have a finished basement, something unheard of in the rest of my family (except my parents never had one either).<br /><br />The standouts in this show are Gandolfini, as a ruthless gangster on antidepressants, Falco, who is brilliant as his wife, and Bracco as the tortured Jennifer. But everyone is excellent. If you can take the violence and the language, this is a great show, an unrelenting portrait of New Jersey mob life.'\n",
      " b\"If, unlike some of the commenters here, you are not staging a class war and don't mind seeing the lives of other people who are fairly successful, extroverted, bohemian (gasp) and not being terribly English at a party and getting into all sorts of trouble as a result this is not a bad film, closer to Euro cinema rather than an imitation of the usual slick American crap... I believe the minimal sound design and cheap camera is a conscious decision rather than bad film making, I'd defend this, the film isn't any worse as a result, and it puts the spotlight on the cast, some of whom are really good (Kate Hardie- think that's her name, as the sarcastic drunk is spot-on) the one exception being David Baddiel, who should never be allowed to appear in serious stuff!! It's light, and we don't go for this kind of anatomising-of-relationship crap in this country, but if you don't have any real friends to go to a party with than you could do worse than to sit in and watch this.\"\n",
      " b'Following the brilliant \"Goy\\xc3\\xb4kiba\" (aka. \"Hanzo The Razor - Sword Of Justice\", 1972) and its excellent (and even sleazier) sequel \"Goy\\xc3\\xb4kiba: Kamisori Hanz\\xc3\\xb4 jigoku zeme\" (aka. \"Razor 2: The Snare\", 1973), this \"Goy\\xc3\\xb4kiba: Oni no Hanz\\xc3\\xb4 yawahada koban\" aka. \"Razor 3: Who\\'s Got The Gold\" is the third, and sadly final installment to the awesome saga about the incorruptible Samurai-constable Hanzo \\'The Razor\\' Ittami (brilliantly played by the great Shintar\\xc3\\xb4 Katsu), who fights corruption with his fighting expertise as well as his enormous sexual powers. As a big fan of 70s exploitation cinema made in Nippon, \"Sword Of Justice\" became an instant favorite of mine, and I was therefore more than eager to find the sequels, and full of anticipation when I finally stumbled over them recently. While this third \"Hanzo\" film is just not quite as brilliant as its predecessors it is definitely another great piece of cult-cinema that no lover of Japanese exploitation cinema can afford to miss. \"Who\\'s Got The Gold\" is a bit tamer than the two foregoing Hanzo films, but it is just as brilliantly comical and crudely humorous, and immediately starts out fabulously odd: The film begins, when Hanzo\\'s two assistants see a female ghost when fishing. Having always wanted to sleep with a ghost, Hanzo insists that his assistants lead him to the site of the occurrence... If that is not a promising beginning for an awesome film experience, I don\\'t know what is. Shintaro Katsu, one of my personal favorite actors, is once again brilliant in the role of Hanzo, a role that seems to have been written specifically for him. Katsu IS Hanzo, the obstinate and fearless constable, who hates corruption and deliberately insults his superiors, and whose unique interrogation techniques include raping female suspects. The interrogated women than immediately fall for him, due to his sexual powers and enormous penis, which he trains in a rather grotesque routine ritual. I will not give away more about the plot in \"Who\\'s Got The Gold\", but I can assure that it is as cool as it sounds. The supporting performances are also very good, and, as in the predecessors, there are plenty of hilariously eccentric characters. This is sadly the last film in the awesomely sleazy \\'Hanzo\\' series. If they had made 20 sequels more, I would have happily watched them all! The entire Hanzo series is brilliant, and while this third part is a bit inferior compared to its predecessors, it is definitely a must-see for all lovers of cult-cinema! Oh how I wish they had made more sequels!'\n",
      " b'Mr. VanHook took a good idea and kicked like a football. Unfortunately, it didn\\'t make the goal. The historical subject of giants is a good one, but pour in the goon milk and you end up with a giant wheel of cheese. I say, take this reel wheel and roll it off a cliff. I couldn\\'t even watch the entire film. That says a lot because I rarely walk away from any movie. I always like to give them a chance for last-minute redemption. It\\'s impossible to redeem something this bad. Well, at least the acting was good....NOT! <br /><br />The only thing \"falling\" in this film is the rating. 1/10 and sinking into the negative numbers!'\n",
      " b\"I love the movie. It brought me back to the best time of my life. <br /><br />We need that time again, now more than ever. For me it was a time of freedom, learning, and finding myself. I will always miss it. There will never be another time like the 60's, unfortunately.\"\n",
      " b\"Gadar is an example of one of Bollywood worst overrated movies ever. Directed by Anil Sharma, who prefers making period related movie gives a rubbish movie. The songs were boring and ain't the kind of song you want to listen to in your car, full volume. Sunny Deol is famous for making daft movies, where he beats up a 100 bad guys on his own. He even kicks a metal jail door (Indian) and kicks a moving car far away (Teesri Aankh). I can give another 50 examples of disgraceful action by Sunny Deol. But I'm sure most people know this already. Sunny gives a pathetic performance once again repeating the same type of role. A guy claiming to be fighting for his countries piece, by using violence. Amisha Patel is hands down dead sexy with an amazing body that i would love to bone. But even she couldn't save the film from being a disaster. Instead of wearing sexy clothes like she usually does, in this movie she doesn't. Maybe cos she was playing a Muslim, but she doesn't act like one in the movie. Overall, this is a poor show all the way, I'm sure it will appeal to some people, who love seeing the Bollywood actor beat up 100 guys. Give me a break.\"\n",
      " b\"I've seen this movie and I must say I'm very impressed. There are not much movies I like, but I do like this one. You should see this movie by yourself and comment it,because this is one of my most favorite movie. I fancy to see this again. Action fused with a fantastic story. Very impressing. I like Modesty's character. Actually she's very mystic and mysterious (I DO like that^^). The bad boy is pretty too. Well, actually this whole movie is rare in 'movieworld'. I considered about the vote of this movie, I thought this is should be a very popular movie. I guess wrong. It was ME who was very impressed about this movie, and I hope I'm not the only one who takes only the cost to watch this one. See and vote.\"\n",
      " b\"A team of archaeologists uncover a real treasure \\xc2\\x96 the Crown of the Queen of Sheeba. From Egypt, the crown is to be transferred via steamship to San Francisco. But it won't be an easy journey. There are plenty of would-be thieves who would love to get their hands on the priceless jewels contained in the crown. Fortunately for all involved, Mr. Moto is on hand to guard the crown on its journey. However, that doesn't mean someone won't try to get their hands on the treasure.<br /><br />After the disappointment of Mr. Moto's Gamble, I went into Mr. Moto Takes a Vacation hoping for the best, but, admittedly, fearing the worst. But within the first 10 seconds of the film, I knew I would find it more enjoyable. I'm a sucker for a 1930s style mystery that features anything to do with archaeological digs in Egypt. And seeing Moto disguised as a German archaeologist (Imagine that, Peter Lorre playing a German?), the beginning scenes really drew me in. While the movie may have quickly shifted to the less exotic San Francisco, it remained just as enjoyable. Dark, sinister characters lurking in the rainy night; gunshots fired from open windows that narrowly miss the hero's head; sophisticated and supposed foolproof alarm systems just begging for someone to test them; and master criminals believed to be dead \\xc2\\x96 these are the kind of elements found in a lot of the really good 1930s mysteries that I love. And Mr. Moto Takes a Vacation's got 'em all. A couple other bonuses for me included the always enjoyable Lionel Atwill in a nice little role, comic relief from G.P. Huntley that's actually funny, and a return to form for Mr. Moto. I've already mentioned his disguise in the movie's opening scenes, well the athletic Moto comes out near the film's finale. Moto is a like a Whirling Dervish of activity as he goes after his prey. All this and I haven't even mentioned the wonderful performance turned in by Lorre. Any way you look at it, Mr. Moto Takes a Vacation is a winner.<br /><br />As much as I hate that the Mr. Moto series had to end after this installment, it's understandable when you think about it. WWII was just around the corner. And after Pearl Harbor, a movie with a Japanese hero wouldn't have gone over very well. At least the Mr. Moto series ended on a very positive note.\"\n",
      " b'*WARNING* Contains MANY SPOILERS!<br /><br />Let me start by saying I have a huge respect for Gillian Anderson\\'s incredible talent as a varied and versatile actress - which is why I cannot comprehend her reasons for agreeing to make this film once she saw the script (or lack thereof.) <br /><br />The premise of the film was, in my opinion, a great idea and there were some genuinely thought-provoking themes in there but it ended up like a collapsed souffl\\xc3\\xa9. It exemplifies why I hate 99% of British cinema. It feels too long, it\\'s tedious, for the most part, and not a lot happens after the first twenty minutes. Just when you think there\\'s a chance of it picking up some speed it disappoints like Paula Radcliffe running a marathon. With little imaginative directing and a minimalist plot, there isn\\'t much to keep the audience from nodding off into their popcorn. As for the script I can only surmise that the writer was trying to save a few trees, with the average scene reading something along the lines of \"Alice: F*** OFF! (Adam stares. Adam runs off into woods)(Alice follows) Alice: ADAM! ADAM!\" I suspect that, word for word, the actors probably got paid more than Kate Moss did for her Virgin Mobile adverts. What few lines there were didn\\'t have a lot of variation with a frequent use of the f-word that would make Bridget Jones\\'s friend, Shazza, proud. There is little establishment of the main characters before the main sordid event which leaves the audience lacking much sympathy for the characters beyond an automatic \\'Oh that\\'s terrible\\' reaction.<br /><br />Alice isn\\'t the kind of woman who courts sympathy either. She\\'s got a great job, an expensive London apartment with roof space to die for yet she comes across on screen as conceited, bitter and dissatisfied before her life takes a turn for the worst. After the attack a few layers are peeled back which sort-of explain why she is this way to start with; she grew up with a tough-as-old-boots soldier who thought that teaching her how to shoot his gun was the ultimate expression of love so, instead of following in his footsteps, she ran away to the big city in search of something to make her feel like her life is worth living. Instead she found a group of stereotypical middle-class Toffs who look down on anyone not rich enough to drive a Lexus and the luxuries that come with an integrated security/entertainment system (i.e. becoming Mrs Robinson to a wanna-be Cockney wide-boy electrician) Someone pass me a tissue. The one saving grace of this character is that she is played by Gillian Anderson. In the hands of a lesser actress she would\\'ve been intolerably one-dimensional but Ms Anderson actually manages to inject a few fleeting moments of humanity into this otherwise lifeless human being, most notably when she\\'s sincerely apologising for her road rage in a vain attempt to stop her attackers from continuing their assault.<br /><br />I can\\'t say that Adam fared much better either. Danny Dyer played him well as a fish-out-of-water Jack the Lad but a good performance couldn\\'t save him from both the lack of a script and the total absence of any character background. <br /><br />This film relied mostly on shock value but the timing was off and it felt far too engineered from beginning to end. As for the shock, the most shocking thing about this film is the unashamed demonstration of how painfully thin Ms Anderson has become; it was almost as unsettling to see as the brutal attack scenes. On a side note, only in a British film would a gang of violent sex attackers take the time to offer each other contraception before continuing to cheer their mates on - talk about stiff-upper-lip taken to the extreme! If this is the kind of film that the National Lottery is donating money to make then I\\'m not surprised that fewer and fewer people are choosing to spend their pound each week. <br /><br />Saying that I hated this film is giving it too much credit, I didn\\'t care enough about any of the characters to warrant that strong an emotion. I want that one-and-a-bit hours of my life back, please!'\n",
      " b'This movie is not in anyway funny, it tries to be funny with it\\'s lame humor, which is so dry and boring that the movie is just 2 hours of torture. Throughout the whole movie i was thinking one thing, \"when is this gonna end\". One thing you have to hand to them, is that they do have a very few mildly funny moments, which is also why i gave it a whole 2 stars. It is unoriginal and uses up almost every old blonde joke in the book, even the ones that wasn\\'t funny the first time. It basically is a movie to belittle blondes and to record the whole repetoir of blonde jokes.<br /><br />To sum it all up, this movie is blonde humor gone bad, it is not worth paying any amount of money to watch, it is just that bad.'], shape=(32,), dtype=string)\n"
     ]
    }
   ],
   "source": [
    "for x, y in train_set.take(1):\n",
    "    print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
