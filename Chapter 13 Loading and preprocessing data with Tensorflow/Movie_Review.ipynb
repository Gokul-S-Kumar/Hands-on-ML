{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from pathlib import Path\n",
    "import os\n",
    "import numpy as np\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "DOWNLOAD_ROOT = 'http://ai.stanford.edu/~amaas/data/sentiment/'\n",
    "FILENAME = 'aclImdb_v1.tar.gz'\n",
    "filepath = keras.utils.get_file(FILENAME, DOWNLOAD_ROOT + FILENAME, extract = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = Path(filepath).parent / 'aclImdb'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(path.parts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "aclImdb/\n",
      "    README\n",
      "    imdb.vocab\n",
      "    imdbEr.txt\n",
      "    test/\n",
      "        labeledBow.feat\n",
      "        urls_neg.txt\n",
      "        urls_pos.txt\n",
      "        neg/\n",
      "            0_2.txt\n",
      "            10000_4.txt\n",
      "            10001_1.txt\n",
      "            ...\n",
      "        pos/\n",
      "            0_10.txt\n",
      "            10000_7.txt\n",
      "            10001_9.txt\n",
      "            ...\n",
      "    train/\n",
      "        labeledBow.feat\n",
      "        unsupBow.feat\n",
      "        urls_neg.txt\n",
      "        ...\n",
      "        neg/\n",
      "            0_3.txt\n",
      "            10000_4.txt\n",
      "            10001_4.txt\n",
      "            ...\n",
      "        pos/\n",
      "            0_9.txt\n",
      "            10000_8.txt\n",
      "            10001_10.txt\n",
      "            ...\n",
      "        unsup/\n",
      "            0_0.txt\n",
      "            10000_0.txt\n",
      "            10001_0.txt\n",
      "            ...\n"
     ]
    }
   ],
   "source": [
    "# Displaying the filepaths\n",
    "for name, subdirs, files in os.walk(path):\n",
    "    indent = len(Path(name).parts) - len(path.parts)\n",
    "    print('    ' * indent + Path(name).parts[-1] + os.sep)\n",
    "    for index, filename in enumerate(sorted(files)):\n",
    "        if index == 3:\n",
    "            print('    ' * (indent + 1) + '...')\n",
    "            break\n",
    "        print('    ' * (indent + 1) + filename)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Func to store the filepaths as strings\n",
    "def review_paths(dirpath):\n",
    "    return [str(path) for path in dirpath.glob('*.txt')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_pos = review_paths(path / 'train' / 'pos')\n",
    "train_neg = review_paths(path / 'train' / 'neg')\n",
    "test_valid_pos = review_paths(path / 'test' / 'pos')\n",
    "test_valid_neg = review_paths(path / 'test' / 'neg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(12500, 12500, 12500, 12500)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_pos), len(train_neg), len(test_valid_pos), len(test_valid_neg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Splitting the test set into test and validation sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.shuffle(test_valid_pos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_pos = test_valid_pos[:5000]\n",
    "valid_pos = test_valid_pos[5000:]\n",
    "test_neg = test_valid_neg[:5000]\n",
    "valid_neg = test_valid_neg[5000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5000, 5000, 7500, 7500)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test_pos), len(test_neg), len(valid_pos), len(valid_neg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use tf.data for creating a dataset for each set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Since the dataset fits in memory, we can use a simple python func to create a dataset using the from_tenso_slices() method.\n",
    "def imdb_dataset(filepaths_pos, filepaths_neg):\n",
    "    reviews = []\n",
    "    labels = []\n",
    "    for filepaths, label in ((filepaths_neg, 0), (filepaths_pos, 1)):\n",
    "        for filepath in filepaths:\n",
    "            with open(filepath) as review_file:\n",
    "                reviews.append(review_file.read())\n",
    "            labels.append(label)\n",
    "    return tf.data.Dataset.from_tensor_slices((tf.constant(reviews), tf.constant(labels))) # Creating the dataset from the tensors "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(b'It\\'s been a while since seeing this the first time, so I watched it again with the second movie in the series. While I realize there is a 3rd movie out that I haven\\'t seen yet, I\\'ll review under the original title...<br /><br />Just from the standpoint of production value, screen writing, and movie making, this movie fails on many levels, though it succeeds on a few as well. What can you expect from a low-budget, \"B\" movie? Not much, and it works from the standpoint of production. However, the writing is certainly disjointed, with little in the way of character development...exactly what I\\'d expect when there is an agenda to a film. I didn\\'t have a problem with the acting...the cast is solid; however, the screenplay in both movies gives the actors little opportunity to really stretch themselves. Because the film is \"Christian,\" this is predictable, as you can\\'t very well portray violent chaos of the \"end times\" without also breaking some of the ethics which are normally associated with Christianity. In other words, the mistake comes in making this into a G-rated film when the content, even in the most conservative of Bible interpretations, would be R-rated by any measure. So, if the purpose of the movie is to scare people into Christian faith, then the movie should be somewhat scary, right? However, you can\\'t comment on a film adaptation from a book without commenting on the book, or in this case, series of books. There are certainly plenty of Christian materials worthy enough to be made into movies...but not the \"Left Behind\" series...and these movies ultimately fail because, while being best-sellers, they are poorly written novels based on bad theology.<br /><br />As a Southern Baptist minister, I confess that the books were a guilty pleasure for me, though I have yet to finish the last two books of the series. I have described them as decent fiction, and if the books would take the point of view that this is one \"possibility\" or interpretation of the subject of biblical eschatology (study of the \"end times), then I could live with that. However, this series is divisive in Christian circles because it promotes the \"literalist\" interpretation of all Scripture above a more proper hermeneutic. Inevitably, this leads to the \"pre-trib, pre-millenial\" dispensation point of view, which confines an all-powerful God far too by humanity\\'s world. In other words, as I\\'ve always said, God shouldn\\'t need our helicopters and bombs to do his ultimate work. But because many people, particularly unstudied Christians, can\\'t think beyond their own world-views, we are left with a pro-conservative, fundamentalist stance with regard to Bible interpretation, and attempts to push it through as the \"only\" interpretation.<br /><br />Thus, the books carry with them an agenda, not so much to get the \"lost\" to understand their need for Christ, but to state that the fundamentalist point of view is the only valid way to understand the Bible. I recall very clearly reading (several years ago) in the second novel a scene where the characters reference a person who was \"left behind\" BECAUSE of his non-adherence to this point of view; as if \"real\" christians worthy to be \"raptured\" couldn\\'t possibly hold to another eschatology. This is disturbing for several reasons, the least of which is because a \"rapture\" is only briefly mentioned in Scripture and it\\'s connection to real, end-time prophecy is tenuous at best.<br /><br />But the real issue with these books is comes in the way they divide the Christian community and how they portray \"true\" Christian behavior. Ultimately, I feel they harden more people to an otherwise legitimate faith/religion instead of win people towards it. It turns all Christians into caricatures, equally disdained and laughed at by the world despite the fact that there is theological room for a wide diversity of believes within Christian thought and practice. As a Christian body, on the whole, we\\'ve done enough of that kind of damage to society over 2000 years of history...and we certainly don\\'t need to promote it by film to thousands, maybe millions of others.<br /><br />Thus, the \"Left Behind\" movies fail because the \"Left Behind\" books aren\\'t worthy to be interpreted into movies.', shape=(), dtype=string)\n",
      "tf.Tensor(0, shape=(), dtype=int32)\n",
      "\n",
      "tf.Tensor(b'Laughed a lot - because it is so incredibly bad - sorry folks, but definitely one of the worst movies I have ever seen... I know it is low budget, but anyway: the actors behave like playing in a soap, the dialogues are absolutely crappy and the last time I have seen such odd pictures was at a trash nite at some youth video festival ten years ago. I really appreciate that people gather together and shoot cheap movies, but at least a certain amount of quality should be accomplished. But at least one good thing: the first three minutes of the movie were quiet interesting and looked okay - and the score was really worth listening to. The DVD cover promised a lot, but that is by far the best this film has to offer...', shape=(), dtype=string)\n",
      "tf.Tensor(0, shape=(), dtype=int32)\n",
      "\n",
      "tf.Tensor(b'This movie is horrible! It rivals \"Ishtar\" in the number of embarrassingly bad moments. I would have rated it lower than a 3, save for a couple of funny lines; but, overall, this film was crap! It looked like they made it over a weekend at some bankrupt resort somewhere. Joe Roth should join Elaine May on the directing sidelines forever!', shape=(), dtype=string)\n",
      "tf.Tensor(0, shape=(), dtype=int32)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for x, y in imdb_dataset(train_pos, train_neg).take(3):\n",
    "    print(x)\n",
    "    print(y)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15.8 s ± 0 ns per loop (mean ± std. dev. of 1 run, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%timeit -r1 for x, y in imdb_dataset(train_pos, train_neg).repeat(10): pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- It took 16 secs to load the dataset and go through it 10 times."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- If the dataset didnt fit in memory we would have to load it using tf.data or convert it into TFRecord file. Luckily each review fits on just one line and is seperated by  `<br />` ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def imdb_dataset(filepaths_pos, filepaths_neg, n_parallel_threads = 5):\n",
    "    dataset_neg = tf.data.TextLineDataset(filepaths_neg, num_parallel_reads = n_parallel_threads)\n",
    "    dataset_neg = dataset_neg.map(lambda review : (review, 0))\n",
    "    dataset_pos = tf.data.TextLineDataset(filepaths_pos, num_parallel_reads = n_parallel_threads)\n",
    "    dataset_pos = dataset_pos.map(lambda review : (review, 0))\n",
    "    return tf.data.Dataset.concatenate(dataset_pos, dataset_neg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30.6 s ± 0 ns per loop (mean ± std. dev. of 1 run, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%timeit -r1 for x, y in imdb_dataset(train_pos, train_neg).repeat(10) : pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- It now takes 31 secs to do the same.This is much slower because the dataset is not cached in RAM and it must be loaded each time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18.8 s ± 0 ns per loop (mean ± std. dev. of 1 run, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%timeit -r1 for x, y in imdb_dataset(train_pos, train_neg).cache().repeat(10) : pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The time taken has reduced as we cached the dataset in the RAM without loading it each time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "train_set = imdb_dataset(train_pos, train_neg).shuffle(25000).batch(batch_size).prefetch(1)\n",
    "valid_set = imdb_dataset(valid_pos, valid_neg).batch(batch_size).prefetch(1)\n",
    "test_set = imdb_dataset(test_pos, test_neg).batch(batch_size).prefetch(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- We will first define a func to prerpocess the data. It will crop them to 300 characters, converting them to lower cases then replacing the `<br />` and all other non-letter characters to spaces, splitting the reviews into words and finally padding or cropping each review so it ends up with exactly n_words tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(x_batch, n_words = 50):\n",
    "    shape = tf.shape(x_batch) * tf.constant([1, 0]) + tf.constant([0, n_words])\n",
    "    z = tf.strings.substr(x_batch, 0, 300) # Converting the main review to strings containing 300 characters.\n",
    "    z = tf.strings.lower(z) # converting the text to lower case\n",
    "    z = tf.strings.regex_replace(z, b'<br\\\\s*/?>', b' ') # Converting the non-letter characters to spaces.\n",
    "    z = tf.strings.regex_replace(z, b'[^a-z]', b' ')\n",
    "    z = tf.strings.split(z)\n",
    "    return z.to_tensor(shape = shape, default_value = b'<pad>')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_example = tf.constant([\"It's a great, great movie! I loved it.\", \"It was terrible, run away!!!\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[b'it', b's', b'a', b'great', b'great', b'movie', b'i', b'loved',\n",
       "        b'it', b'<pad>', b'<pad>', b'<pad>', b'<pad>', b'<pad>',\n",
       "        b'<pad>', b'<pad>', b'<pad>', b'<pad>', b'<pad>', b'<pad>',\n",
       "        b'<pad>', b'<pad>', b'<pad>', b'<pad>', b'<pad>', b'<pad>',\n",
       "        b'<pad>', b'<pad>', b'<pad>', b'<pad>', b'<pad>', b'<pad>',\n",
       "        b'<pad>', b'<pad>', b'<pad>', b'<pad>', b'<pad>', b'<pad>',\n",
       "        b'<pad>', b'<pad>', b'<pad>', b'<pad>', b'<pad>', b'<pad>',\n",
       "        b'<pad>', b'<pad>', b'<pad>', b'<pad>', b'<pad>', b'<pad>'],\n",
       "       [b'it', b'was', b'terrible', b'run', b'away', b'<pad>', b'<pad>',\n",
       "        b'<pad>', b'<pad>', b'<pad>', b'<pad>', b'<pad>', b'<pad>',\n",
       "        b'<pad>', b'<pad>', b'<pad>', b'<pad>', b'<pad>', b'<pad>',\n",
       "        b'<pad>', b'<pad>', b'<pad>', b'<pad>', b'<pad>', b'<pad>',\n",
       "        b'<pad>', b'<pad>', b'<pad>', b'<pad>', b'<pad>', b'<pad>',\n",
       "        b'<pad>', b'<pad>', b'<pad>', b'<pad>', b'<pad>', b'<pad>',\n",
       "        b'<pad>', b'<pad>', b'<pad>', b'<pad>', b'<pad>', b'<pad>',\n",
       "        b'<pad>', b'<pad>', b'<pad>', b'<pad>', b'<pad>', b'<pad>',\n",
       "        b'<pad>']], dtype=object)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocess(x_example).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utility func that will take a data sample and will output the list of the top max_size most frequent words.\n",
    "def get_vocabulary(data_sample, max_size = 1000):\n",
    "    preprocessed_reviews = preprocess(data_sample).numpy() # Preprocessing the input using the func defined above.\n",
    "    counter = Counter() # Initializing the counter\n",
    "    for words in preprocessed_reviews:\n",
    "        for word in words:\n",
    "            if word != b'<pad>':\n",
    "                counter[word] += 1\n",
    "    return [b'<pad>'] + [word for word, count in counter.most_common(max_size)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[b'<pad>',\n",
       " b'it',\n",
       " b'great',\n",
       " b's',\n",
       " b'a',\n",
       " b'movie',\n",
       " b'i',\n",
       " b'loved',\n",
       " b'was',\n",
       " b'terrible',\n",
       " b'run',\n",
       " b'away']"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_vocabulary(x_example)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building the custom layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom layer class to convert the text to vectors by the help of lookup tables.\n",
    "class Text_vectorization(keras.layers.Layer):\n",
    "    def __init__(self, max_vocabulary_size = 1000, n_oov_buckets = 100, dtype = tf.string, **kwargs): # Init method\n",
    "        super().__init__(dtype = dtype, **kwargs)\n",
    "        self.max_vocabulary_size = max_vocabulary_size # Maximum size of the vocabulary\n",
    "        self.n_oov_buckets = n_oov_buckets # Size of out-of-vocabulary buckets\n",
    "    def adapt(self, data_sample):\n",
    "        self.vocab = get_vocabulary(data_sample, self.max_vocabulary_size) # Creating the vocabulary\n",
    "        words = tf.constant(self.vocab) # Converting the vocabulary to a tensor\n",
    "        word_ids = tf.range(len(self.vocab), dtype = tf.int64) # Initializing the indices for the lookup table\n",
    "        vocab_init = tf.lookup.KeyValueTensorInitializer(words, word_ids) # Creating teh Key Value Tensor\n",
    "        self.table = tf.lookup.StaticVocabularyTable(vocab_init, self.n_oov_buckets) # Creating the lookup table\n",
    "    def call(self, inputs):\n",
    "        preprocessed_inputs = preprocess(inputs) # Preprocessing the inputs\n",
    "        return self.table.lookup(preprocessed_inputs) # Returning the vectorized texts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_vectorization = Text_vectorization()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_vectorization.adapt(x_example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2, 50), dtype=int64, numpy=\n",
       "array([[ 1,  3,  4,  2,  2,  5,  6,  7,  1,  0,  0,  0,  0,  0,  0,  0,\n",
       "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "         0,  0],\n",
       "       [ 1,  8,  9, 10, 11,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "         0,  0]])>"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_vectorization(x_example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
