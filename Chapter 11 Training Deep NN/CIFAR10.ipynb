{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building a NN and training it using the CIFAR10 dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import os\n",
    "from sklearn.model_selection import RandomizedSearchCV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "(x_train_full, y_train_full), (x_test, y_test) = keras.datasets.cifar10.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50000, 32, 32, 3)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train_full.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 32, 32, 3)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_valid, x_train = x_train_full[:5000], x_train_full[5000:]\n",
    "y_valid, y_train = y_train_full[:5000], y_train_full[5000:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "keras.backend.clear_session()\n",
    "tf.random.set_seed(42)\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.Sequential()\n",
    "model.add(keras.layers.Flatten(input_shape = [32, 32, 3]))\n",
    "for _ in range(20):\n",
    "    model.add(keras.layers.Dense(100, activation = keras.activations.relu, kernel_initializer = keras.initializers.he_normal()))\n",
    "model.add(keras.layers.Dense(10, activation = keras.activations.softmax))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "flatten (Flatten)            (None, 3072)              0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 100)               307300    \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "dense_12 (Dense)             (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "dense_13 (Dense)             (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "dense_14 (Dense)             (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "dense_15 (Dense)             (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "dense_16 (Dense)             (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "dense_17 (Dense)             (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "dense_18 (Dense)             (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "dense_19 (Dense)             (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "dense_20 (Dense)             (None, 10)                1010      \n",
      "=================================================================\n",
      "Total params: 500,210\n",
      "Trainable params: 500,210\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Now we need to find an optimal learning rate. To do this we will use a Nadam optimizer and train the model for 10 epochs with different learning rates.\n",
    "- We will be setting up a tensorboard environment using a tensorboard callback and check the performance of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.00025"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_logdir = os.path.join(os.curdir, 'CIFAR10_logs', 'optimal_lr_model', 'lr = {}'.format(learning_rate))\n",
    "tb_callback = keras.callbacks.TensorBoard(run_logdir)\n",
    "early_stopping_cb = keras.callbacks.EarlyStopping(patience = 20)\n",
    "model_checkpoint_cb = keras.callbacks.ModelCheckpoint('CIFAR10_model.h5', save_best_only = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss = keras.losses.sparse_categorical_crossentropy, optimizer = keras.optimizers.Nadam(learning_rate = learning_rate), metrics = 'accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "   1/1407 [..............................] - ETA: 0s - loss: 165.7273 - accuracy: 0.0625WARNING:tensorflow:From /home/gokul/.local/lib/python3.8/site-packages/tensorflow/python/ops/summary_ops_v2.py:1277: stop (from tensorflow.python.eager.profiler) is deprecated and will be removed after 2020-07-01.\n",
      "Instructions for updating:\n",
      "use `tf.profiler.experimental.stop` instead.\n",
      "   2/1407 [..............................] - ETA: 1:02 - loss: 129.1794 - accuracy: 0.0625WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0241s vs `on_train_batch_end` time: 0.0637s). Check your callbacks.\n",
      "1407/1407 [==============================] - 8s 6ms/step - loss: 2.7819 - accuracy: 0.1722 - val_loss: 2.0820 - val_accuracy: 0.2038\n",
      "Epoch 2/10\n",
      "1407/1407 [==============================] - 7s 5ms/step - loss: 1.9937 - accuracy: 0.2514 - val_loss: 2.1159 - val_accuracy: 0.2318\n",
      "Epoch 3/10\n",
      "1407/1407 [==============================] - 9s 6ms/step - loss: 1.9164 - accuracy: 0.2812 - val_loss: 1.9130 - val_accuracy: 0.2824\n",
      "Epoch 4/10\n",
      "1407/1407 [==============================] - 10s 7ms/step - loss: 1.8650 - accuracy: 0.3061 - val_loss: 1.8742 - val_accuracy: 0.3116\n",
      "Epoch 5/10\n",
      "1407/1407 [==============================] - 9s 7ms/step - loss: 1.8361 - accuracy: 0.3218 - val_loss: 1.9204 - val_accuracy: 0.2920\n",
      "Epoch 6/10\n",
      "1407/1407 [==============================] - 8s 6ms/step - loss: 1.7965 - accuracy: 0.3409 - val_loss: 1.8150 - val_accuracy: 0.3338\n",
      "Epoch 7/10\n",
      "1407/1407 [==============================] - 8s 6ms/step - loss: 1.7458 - accuracy: 0.3650 - val_loss: 1.7359 - val_accuracy: 0.3738\n",
      "Epoch 8/10\n",
      "1407/1407 [==============================] - 8s 6ms/step - loss: 1.7039 - accuracy: 0.3815 - val_loss: 1.6928 - val_accuracy: 0.3882\n",
      "Epoch 9/10\n",
      "1407/1407 [==============================] - 8s 6ms/step - loss: 1.6794 - accuracy: 0.3924 - val_loss: 1.6576 - val_accuracy: 0.4050\n",
      "Epoch 10/10\n",
      "1407/1407 [==============================] - 7s 5ms/step - loss: 1.6503 - accuracy: 0.4036 - val_loss: 1.6520 - val_accuracy: 0.4004\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fd0205f71f0>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x_train, y_train, epochs = 10, validation_data = (x_valid, y_valid), callbacks = [tb_callback])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- From multiple trial and error values for learning rate, it was found that the model performed the best at a learning rate of 0.00025."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "keras.backend.clear_session()\n",
    "tf.random.set_seed(42)\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.Sequential()\n",
    "model.add(keras.layers.Flatten(input_shape = [32, 32, 3]))\n",
    "for _ in range(20):\n",
    "    model.add(keras.layers.Dense(100, activation = keras.activations.relu, kernel_initializer = keras.initializers.he_normal()))\n",
    "model.add(keras.layers.Dense(10, activation = keras.activations.softmax))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "flatten (Flatten)            (None, 3072)              0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 100)               307300    \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "dense_12 (Dense)             (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "dense_13 (Dense)             (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "dense_14 (Dense)             (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "dense_15 (Dense)             (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "dense_16 (Dense)             (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "dense_17 (Dense)             (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "dense_18 (Dense)             (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "dense_19 (Dense)             (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "dense_20 (Dense)             (None, 10)                1010      \n",
      "=================================================================\n",
      "Total params: 500,210\n",
      "Trainable params: 500,210\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimal_lr = 0.00025"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss = keras.losses.sparse_categorical_crossentropy, optimizer = keras.optimizers.Nadam(learning_rate = optimal_lr), metrics = 'accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "early_stopping_cb = keras.callbacks.EarlyStopping(patience=20)\n",
    "model_checkpoint_cb = keras.callbacks.ModelCheckpoint('CIFAR10_model', save_best_only=True, save_weights_only = True)\n",
    "run_logdir = os.path.join(os.curdir, 'CIFAR10_logs', 'optimal_lr_model')\n",
    "tensorboard_cb = keras.callbacks.TensorBoard(run_logdir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "   2/1407 [..............................] - ETA: 3:01 - loss: 129.1794 - accuracy: 0.0625WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0084s vs `on_train_batch_end` time: 0.2490s). Check your callbacks.\n",
      "1407/1407 [==============================] - 9s 7ms/step - loss: 2.7819 - accuracy: 0.1722 - val_loss: 2.0820 - val_accuracy: 0.2038\n",
      "Epoch 2/100\n",
      "1407/1407 [==============================] - 8s 6ms/step - loss: 1.9937 - accuracy: 0.2514 - val_loss: 2.1159 - val_accuracy: 0.2318\n",
      "Epoch 3/100\n",
      "1407/1407 [==============================] - 8s 6ms/step - loss: 1.9164 - accuracy: 0.2812 - val_loss: 1.9130 - val_accuracy: 0.2824\n",
      "Epoch 4/100\n",
      "1407/1407 [==============================] - 8s 5ms/step - loss: 1.8650 - accuracy: 0.3061 - val_loss: 1.8742 - val_accuracy: 0.3116\n",
      "Epoch 5/100\n",
      "1407/1407 [==============================] - 8s 6ms/step - loss: 1.8361 - accuracy: 0.3218 - val_loss: 1.9204 - val_accuracy: 0.2920\n",
      "Epoch 6/100\n",
      "1407/1407 [==============================] - 9s 6ms/step - loss: 1.7965 - accuracy: 0.3409 - val_loss: 1.8150 - val_accuracy: 0.3338\n",
      "Epoch 7/100\n",
      "1407/1407 [==============================] - 8s 6ms/step - loss: 1.7458 - accuracy: 0.3650 - val_loss: 1.7359 - val_accuracy: 0.3738\n",
      "Epoch 8/100\n",
      "1407/1407 [==============================] - 8s 6ms/step - loss: 1.7039 - accuracy: 0.3815 - val_loss: 1.6928 - val_accuracy: 0.3882\n",
      "Epoch 9/100\n",
      "1407/1407 [==============================] - 8s 6ms/step - loss: 1.6794 - accuracy: 0.3924 - val_loss: 1.6576 - val_accuracy: 0.4050\n",
      "Epoch 10/100\n",
      "1407/1407 [==============================] - 8s 6ms/step - loss: 1.6503 - accuracy: 0.4036 - val_loss: 1.6520 - val_accuracy: 0.4004\n",
      "Epoch 11/100\n",
      "1407/1407 [==============================] - 8s 6ms/step - loss: 1.6255 - accuracy: 0.4115 - val_loss: 1.6933 - val_accuracy: 0.3872\n",
      "Epoch 12/100\n",
      "1407/1407 [==============================] - 8s 6ms/step - loss: 1.6121 - accuracy: 0.4191 - val_loss: 1.6392 - val_accuracy: 0.4160\n",
      "Epoch 13/100\n",
      "1407/1407 [==============================] - 8s 6ms/step - loss: 1.5878 - accuracy: 0.4299 - val_loss: 1.6281 - val_accuracy: 0.4120\n",
      "Epoch 14/100\n",
      "1407/1407 [==============================] - 8s 6ms/step - loss: 1.5719 - accuracy: 0.4362 - val_loss: 1.5983 - val_accuracy: 0.4252\n",
      "Epoch 15/100\n",
      "1407/1407 [==============================] - 8s 6ms/step - loss: 1.5594 - accuracy: 0.4389 - val_loss: 1.5984 - val_accuracy: 0.4212\n",
      "Epoch 16/100\n",
      "1407/1407 [==============================] - 9s 6ms/step - loss: 1.5420 - accuracy: 0.4448 - val_loss: 1.6114 - val_accuracy: 0.4276\n",
      "Epoch 17/100\n",
      "1407/1407 [==============================] - 8s 6ms/step - loss: 1.5275 - accuracy: 0.4523 - val_loss: 1.6050 - val_accuracy: 0.4242\n",
      "Epoch 18/100\n",
      "1407/1407 [==============================] - 8s 6ms/step - loss: 1.5154 - accuracy: 0.4550 - val_loss: 1.5782 - val_accuracy: 0.4350\n",
      "Epoch 19/100\n",
      "1407/1407 [==============================] - 8s 6ms/step - loss: 1.5056 - accuracy: 0.4588 - val_loss: 1.6181 - val_accuracy: 0.4234\n",
      "Epoch 20/100\n",
      "1407/1407 [==============================] - 9s 6ms/step - loss: 1.4939 - accuracy: 0.4629 - val_loss: 1.5991 - val_accuracy: 0.4358\n",
      "Epoch 21/100\n",
      "1407/1407 [==============================] - 8s 6ms/step - loss: 1.4822 - accuracy: 0.4701 - val_loss: 1.5511 - val_accuracy: 0.4512\n",
      "Epoch 22/100\n",
      "1407/1407 [==============================] - 8s 6ms/step - loss: 1.4749 - accuracy: 0.4714 - val_loss: 1.5564 - val_accuracy: 0.4450\n",
      "Epoch 23/100\n",
      "1407/1407 [==============================] - 7s 5ms/step - loss: 1.4591 - accuracy: 0.4788 - val_loss: 1.5605 - val_accuracy: 0.4428\n",
      "Epoch 24/100\n",
      "1407/1407 [==============================] - 8s 5ms/step - loss: 1.4525 - accuracy: 0.4805 - val_loss: 1.5362 - val_accuracy: 0.4546\n",
      "Epoch 25/100\n",
      "1407/1407 [==============================] - 8s 6ms/step - loss: 1.4420 - accuracy: 0.4823 - val_loss: 1.5810 - val_accuracy: 0.4422\n",
      "Epoch 26/100\n",
      "1407/1407 [==============================] - 8s 5ms/step - loss: 1.4386 - accuracy: 0.4849 - val_loss: 1.5680 - val_accuracy: 0.4388\n",
      "Epoch 27/100\n",
      "1407/1407 [==============================] - 8s 6ms/step - loss: 1.4274 - accuracy: 0.4871 - val_loss: 1.5378 - val_accuracy: 0.4570\n",
      "Epoch 28/100\n",
      "1407/1407 [==============================] - 8s 6ms/step - loss: 1.4211 - accuracy: 0.4888 - val_loss: 1.5360 - val_accuracy: 0.4530\n",
      "Epoch 29/100\n",
      "1407/1407 [==============================] - 8s 5ms/step - loss: 1.4097 - accuracy: 0.4954 - val_loss: 1.5543 - val_accuracy: 0.4536\n",
      "Epoch 30/100\n",
      "1407/1407 [==============================] - 8s 6ms/step - loss: 1.4060 - accuracy: 0.4972 - val_loss: 1.5355 - val_accuracy: 0.4644\n",
      "Epoch 31/100\n",
      "1407/1407 [==============================] - 7s 5ms/step - loss: 1.3957 - accuracy: 0.4988 - val_loss: 1.5489 - val_accuracy: 0.4530\n",
      "Epoch 32/100\n",
      "1407/1407 [==============================] - 8s 6ms/step - loss: 1.3849 - accuracy: 0.5055 - val_loss: 1.5574 - val_accuracy: 0.4596\n",
      "Epoch 33/100\n",
      "1407/1407 [==============================] - 8s 6ms/step - loss: 1.3828 - accuracy: 0.5036 - val_loss: 1.5846 - val_accuracy: 0.4484\n",
      "Epoch 34/100\n",
      "1407/1407 [==============================] - 8s 5ms/step - loss: 1.3707 - accuracy: 0.5095 - val_loss: 1.5558 - val_accuracy: 0.4634\n",
      "Epoch 35/100\n",
      "1407/1407 [==============================] - 8s 6ms/step - loss: 1.3609 - accuracy: 0.5108 - val_loss: 1.5805 - val_accuracy: 0.4476\n",
      "Epoch 36/100\n",
      "1407/1407 [==============================] - 8s 5ms/step - loss: 1.3571 - accuracy: 0.5157 - val_loss: 1.5809 - val_accuracy: 0.4542\n",
      "Epoch 37/100\n",
      "1407/1407 [==============================] - 8s 6ms/step - loss: 1.3471 - accuracy: 0.5163 - val_loss: 1.5970 - val_accuracy: 0.4514\n",
      "Epoch 38/100\n",
      "1407/1407 [==============================] - 8s 6ms/step - loss: 1.3367 - accuracy: 0.5228 - val_loss: 1.5556 - val_accuracy: 0.4662\n",
      "Epoch 39/100\n",
      "1407/1407 [==============================] - 8s 6ms/step - loss: 1.3314 - accuracy: 0.5247 - val_loss: 1.5809 - val_accuracy: 0.4452\n",
      "Epoch 40/100\n",
      "1407/1407 [==============================] - 7s 5ms/step - loss: 1.3198 - accuracy: 0.5281 - val_loss: 1.5258 - val_accuracy: 0.4720\n",
      "Epoch 41/100\n",
      "1407/1407 [==============================] - 9s 6ms/step - loss: 1.3164 - accuracy: 0.5303 - val_loss: 1.5828 - val_accuracy: 0.4620\n",
      "Epoch 42/100\n",
      "1407/1407 [==============================] - 9s 6ms/step - loss: 1.3079 - accuracy: 0.5340 - val_loss: 1.5581 - val_accuracy: 0.4528\n",
      "Epoch 43/100\n",
      "1407/1407 [==============================] - 8s 6ms/step - loss: 1.3023 - accuracy: 0.5316 - val_loss: 1.5584 - val_accuracy: 0.4596\n",
      "Epoch 44/100\n",
      "1407/1407 [==============================] - 8s 6ms/step - loss: 1.2967 - accuracy: 0.5346 - val_loss: 1.5677 - val_accuracy: 0.4580\n",
      "Epoch 45/100\n",
      "1407/1407 [==============================] - 8s 6ms/step - loss: 1.2894 - accuracy: 0.5360 - val_loss: 1.5443 - val_accuracy: 0.4726\n",
      "Epoch 46/100\n",
      "1407/1407 [==============================] - 8s 6ms/step - loss: 1.2842 - accuracy: 0.5389 - val_loss: 1.5458 - val_accuracy: 0.4612\n",
      "Epoch 47/100\n",
      "1407/1407 [==============================] - 8s 6ms/step - loss: 1.2735 - accuracy: 0.5455 - val_loss: 1.5767 - val_accuracy: 0.4582\n",
      "Epoch 48/100\n",
      "1407/1407 [==============================] - 8s 6ms/step - loss: 1.2651 - accuracy: 0.5465 - val_loss: 1.5355 - val_accuracy: 0.4732\n",
      "Epoch 49/100\n",
      "1407/1407 [==============================] - 9s 6ms/step - loss: 1.2556 - accuracy: 0.5521 - val_loss: 1.5536 - val_accuracy: 0.4640\n",
      "Epoch 50/100\n",
      "1407/1407 [==============================] - 9s 6ms/step - loss: 1.2556 - accuracy: 0.5488 - val_loss: 1.5992 - val_accuracy: 0.4658\n",
      "Epoch 51/100\n",
      "1407/1407 [==============================] - 9s 7ms/step - loss: 1.2410 - accuracy: 0.5552 - val_loss: 1.6099 - val_accuracy: 0.4656\n",
      "Epoch 52/100\n",
      "1407/1407 [==============================] - 9s 7ms/step - loss: 1.2386 - accuracy: 0.5579 - val_loss: 1.5442 - val_accuracy: 0.4644\n",
      "Epoch 53/100\n",
      "1407/1407 [==============================] - 9s 6ms/step - loss: 1.2362 - accuracy: 0.5585 - val_loss: 1.5767 - val_accuracy: 0.4710\n",
      "Epoch 54/100\n",
      "1407/1407 [==============================] - 9s 6ms/step - loss: 1.2257 - accuracy: 0.5603 - val_loss: 1.6144 - val_accuracy: 0.4570\n",
      "Epoch 55/100\n",
      "1407/1407 [==============================] - 8s 6ms/step - loss: 1.2226 - accuracy: 0.5596 - val_loss: 1.5958 - val_accuracy: 0.4748\n",
      "Epoch 56/100\n",
      "1407/1407 [==============================] - 10s 7ms/step - loss: 1.2129 - accuracy: 0.5658 - val_loss: 1.5730 - val_accuracy: 0.4708\n",
      "Epoch 57/100\n",
      "1407/1407 [==============================] - 9s 7ms/step - loss: 1.2076 - accuracy: 0.5651 - val_loss: 1.5680 - val_accuracy: 0.4716\n",
      "Epoch 58/100\n",
      "1407/1407 [==============================] - 9s 6ms/step - loss: 1.2068 - accuracy: 0.5685 - val_loss: 1.5710 - val_accuracy: 0.4674\n",
      "Epoch 59/100\n",
      "1407/1407 [==============================] - 9s 7ms/step - loss: 1.1984 - accuracy: 0.5688 - val_loss: 1.6206 - val_accuracy: 0.4676\n",
      "Epoch 60/100\n",
      "1407/1407 [==============================] - 9s 6ms/step - loss: 1.1861 - accuracy: 0.5761 - val_loss: 1.5790 - val_accuracy: 0.4686\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(x_train, y_train, epochs = 100, validation_data = (x_valid, y_valid), callbacks = [early_stopping_cb, model_checkpoint_cb, tensorboard_cb])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "157/157 [==============================] - 0s 2ms/step - loss: 1.5790 - accuracy: 0.4686\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[1.5790321826934814, 0.46860000491142273]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(x_valid, y_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 1s 2ms/step - loss: 1.5602 - accuracy: 0.4689\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[1.5602147579193115, 0.46889999508857727]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.training.tracking.util.CheckpointLoadStatus at 0x7fd0206aae50>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_weights('CIFAR10_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "157/157 [==============================] - 0s 2ms/step - loss: 1.5258 - accuracy: 0.4720\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[1.5257840156555176, 0.47200000286102295]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(x_valid, y_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 1s 2ms/step - loss: 1.5075 - accuracy: 0.4742\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[1.5074900388717651, 0.4742000102996826]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(x_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- We can see that the model with the least validation_loss gives us am accuracy of 47% on the validation and test data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding batch normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- We will be adding the batvh normalization layers in the net and check its performance variations.\n",
    "- Again as we changed the model architecture, we need to find the optimal learning rate by trial and error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "keras.backend.clear_session()\n",
    "tf.random.set_seed(42)\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.Sequential()\n",
    "model.add(keras.layers.Flatten(input_shape = [32, 32, 3]))\n",
    "model.add(keras.layers.BatchNormalization())\n",
    "for _ in range(20):\n",
    "    model.add(keras.layers.Dense(100, kernel_initializer = keras.initializers.he_normal()))\n",
    "    model.add(keras.layers.BatchNormalization())\n",
    "    model.add(keras.layers.Activation('elu'))\n",
    "model.add(keras.layers.Dense(10, activation = keras.activations.softmax))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "flatten (Flatten)            (None, 3072)              0         \n",
      "_________________________________________________________________\n",
      "batch_normalization (BatchNo (None, 3072)              12288     \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 100)               307300    \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 100)               400       \n",
      "_________________________________________________________________\n",
      "activation (Activation)      (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 100)               400       \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 100)               400       \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "batch_normalization_4 (Batch (None, 100)               400       \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "batch_normalization_5 (Batch (None, 100)               400       \n",
      "_________________________________________________________________\n",
      "activation_4 (Activation)    (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "batch_normalization_6 (Batch (None, 100)               400       \n",
      "_________________________________________________________________\n",
      "activation_5 (Activation)    (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "batch_normalization_7 (Batch (None, 100)               400       \n",
      "_________________________________________________________________\n",
      "activation_6 (Activation)    (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "batch_normalization_8 (Batch (None, 100)               400       \n",
      "_________________________________________________________________\n",
      "activation_7 (Activation)    (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "batch_normalization_9 (Batch (None, 100)               400       \n",
      "_________________________________________________________________\n",
      "activation_8 (Activation)    (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "batch_normalization_10 (Batc (None, 100)               400       \n",
      "_________________________________________________________________\n",
      "activation_9 (Activation)    (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "batch_normalization_11 (Batc (None, 100)               400       \n",
      "_________________________________________________________________\n",
      "activation_10 (Activation)   (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "batch_normalization_12 (Batc (None, 100)               400       \n",
      "_________________________________________________________________\n",
      "activation_11 (Activation)   (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_12 (Dense)             (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "batch_normalization_13 (Batc (None, 100)               400       \n",
      "_________________________________________________________________\n",
      "activation_12 (Activation)   (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_13 (Dense)             (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "batch_normalization_14 (Batc (None, 100)               400       \n",
      "_________________________________________________________________\n",
      "activation_13 (Activation)   (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_14 (Dense)             (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "batch_normalization_15 (Batc (None, 100)               400       \n",
      "_________________________________________________________________\n",
      "activation_14 (Activation)   (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_15 (Dense)             (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "batch_normalization_16 (Batc (None, 100)               400       \n",
      "_________________________________________________________________\n",
      "activation_15 (Activation)   (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_16 (Dense)             (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "batch_normalization_17 (Batc (None, 100)               400       \n",
      "_________________________________________________________________\n",
      "activation_16 (Activation)   (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_17 (Dense)             (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "batch_normalization_18 (Batc (None, 100)               400       \n",
      "_________________________________________________________________\n",
      "activation_17 (Activation)   (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_18 (Dense)             (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "batch_normalization_19 (Batc (None, 100)               400       \n",
      "_________________________________________________________________\n",
      "activation_18 (Activation)   (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_19 (Dense)             (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "batch_normalization_20 (Batc (None, 100)               400       \n",
      "_________________________________________________________________\n",
      "activation_19 (Activation)   (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_20 (Dense)             (None, 10)                1010      \n",
      "=================================================================\n",
      "Total params: 520,498\n",
      "Trainable params: 510,354\n",
      "Non-trainable params: 10,144\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.0015"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss = keras.losses.sparse_categorical_crossentropy, optimizer = keras.optimizers.Nadam(lr = learning_rate), metrics = 'accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_logdir = os.path.join(os.curdir, 'CIFAR10_logs', 'BN', 'lr = {}'.format(learning_rate))\n",
    "tensorboard_cb = keras.callbacks.TensorBoard(run_logdir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "   2/1407 [..............................] - ETA: 7:03 - loss: 2.9106 - accuracy: 0.0781WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0179s vs `on_train_batch_end` time: 0.5836s). Check your callbacks.\n",
      "1407/1407 [==============================] - 18s 13ms/step - loss: 1.8514 - accuracy: 0.3351 - val_loss: 1.7488 - val_accuracy: 0.3762\n",
      "Epoch 2/20\n",
      "1407/1407 [==============================] - 17s 12ms/step - loss: 1.7044 - accuracy: 0.3926 - val_loss: 1.6794 - val_accuracy: 0.3950\n",
      "Epoch 3/20\n",
      "1407/1407 [==============================] - 18s 13ms/step - loss: 1.6400 - accuracy: 0.4171 - val_loss: 1.6474 - val_accuracy: 0.4116\n",
      "Epoch 4/20\n",
      "1407/1407 [==============================] - 16s 11ms/step - loss: 1.5878 - accuracy: 0.4350 - val_loss: 1.6109 - val_accuracy: 0.4320\n",
      "Epoch 5/20\n",
      "1407/1407 [==============================] - 15s 11ms/step - loss: 1.5416 - accuracy: 0.4529 - val_loss: 1.5223 - val_accuracy: 0.4540\n",
      "Epoch 6/20\n",
      "1407/1407 [==============================] - 15s 11ms/step - loss: 1.5039 - accuracy: 0.4670 - val_loss: 1.4697 - val_accuracy: 0.4698\n",
      "Epoch 7/20\n",
      "1407/1407 [==============================] - 15s 10ms/step - loss: 1.4666 - accuracy: 0.4794 - val_loss: 1.4893 - val_accuracy: 0.4736\n",
      "Epoch 8/20\n",
      "1407/1407 [==============================] - 15s 10ms/step - loss: 1.4346 - accuracy: 0.4920 - val_loss: 1.4309 - val_accuracy: 0.4852\n",
      "Epoch 9/20\n",
      "1407/1407 [==============================] - 15s 10ms/step - loss: 1.4133 - accuracy: 0.5032 - val_loss: 1.4155 - val_accuracy: 0.4990\n",
      "Epoch 10/20\n",
      "1407/1407 [==============================] - 15s 11ms/step - loss: 1.3865 - accuracy: 0.5094 - val_loss: 1.3645 - val_accuracy: 0.5188\n",
      "Epoch 11/20\n",
      "1407/1407 [==============================] - 17s 12ms/step - loss: 1.3616 - accuracy: 0.5222 - val_loss: 1.3474 - val_accuracy: 0.5280\n",
      "Epoch 12/20\n",
      "1407/1407 [==============================] - 16s 11ms/step - loss: 1.3380 - accuracy: 0.5302 - val_loss: 1.3894 - val_accuracy: 0.5094\n",
      "Epoch 13/20\n",
      "1407/1407 [==============================] - 16s 11ms/step - loss: 1.3187 - accuracy: 0.5348 - val_loss: 1.4025 - val_accuracy: 0.5072\n",
      "Epoch 14/20\n",
      "1407/1407 [==============================] - 16s 11ms/step - loss: 1.2980 - accuracy: 0.5441 - val_loss: 1.3652 - val_accuracy: 0.5234\n",
      "Epoch 15/20\n",
      "1407/1407 [==============================] - 16s 11ms/step - loss: 1.2749 - accuracy: 0.5521 - val_loss: 1.3748 - val_accuracy: 0.5222\n",
      "Epoch 16/20\n",
      "1407/1407 [==============================] - 16s 11ms/step - loss: 1.2628 - accuracy: 0.5581 - val_loss: 1.3527 - val_accuracy: 0.5256\n",
      "Epoch 17/20\n",
      "1407/1407 [==============================] - 16s 11ms/step - loss: 1.2399 - accuracy: 0.5637 - val_loss: 1.3515 - val_accuracy: 0.5306\n",
      "Epoch 18/20\n",
      "1407/1407 [==============================] - 16s 11ms/step - loss: 1.2238 - accuracy: 0.5709 - val_loss: 1.3480 - val_accuracy: 0.5214\n",
      "Epoch 19/20\n",
      "1407/1407 [==============================] - 16s 11ms/step - loss: 1.2071 - accuracy: 0.5767 - val_loss: 1.3954 - val_accuracy: 0.5088\n",
      "Epoch 20/20\n",
      "1407/1407 [==============================] - 16s 12ms/step - loss: 1.1952 - accuracy: 0.5837 - val_loss: 1.3511 - val_accuracy: 0.5364\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(x_train, y_train, epochs = 20, validation_data = (x_valid, y_valid), callbacks = [tensorboard_cb])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- After using different learning rates, it was found that the model having lr = 0.001 gives the best performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [],
   "source": [
    "keras.backend.clear_session()\n",
    "tf.random.set_seed(42)\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.Sequential()\n",
    "model.add(keras.layers.Flatten(input_shape = [32, 32, 3]))\n",
    "model.add(keras.layers.BatchNormalization())\n",
    "for _ in range(20):\n",
    "    model.add(keras.layers.Dense(100, kernel_initializer = keras.initializers.he_normal()))\n",
    "    model.add(keras.layers.BatchNormalization())\n",
    "    model.add(keras.layers.Activation('elu'))\n",
    "model.add(keras.layers.Dense(10, activation = keras.activations.softmax))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "flatten (Flatten)            (None, 3072)              0         \n",
      "_________________________________________________________________\n",
      "batch_normalization (BatchNo (None, 3072)              12288     \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 100)               307300    \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 100)               400       \n",
      "_________________________________________________________________\n",
      "activation (Activation)      (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 100)               400       \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 100)               400       \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "batch_normalization_4 (Batch (None, 100)               400       \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "batch_normalization_5 (Batch (None, 100)               400       \n",
      "_________________________________________________________________\n",
      "activation_4 (Activation)    (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "batch_normalization_6 (Batch (None, 100)               400       \n",
      "_________________________________________________________________\n",
      "activation_5 (Activation)    (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "batch_normalization_7 (Batch (None, 100)               400       \n",
      "_________________________________________________________________\n",
      "activation_6 (Activation)    (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "batch_normalization_8 (Batch (None, 100)               400       \n",
      "_________________________________________________________________\n",
      "activation_7 (Activation)    (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "batch_normalization_9 (Batch (None, 100)               400       \n",
      "_________________________________________________________________\n",
      "activation_8 (Activation)    (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "batch_normalization_10 (Batc (None, 100)               400       \n",
      "_________________________________________________________________\n",
      "activation_9 (Activation)    (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "batch_normalization_11 (Batc (None, 100)               400       \n",
      "_________________________________________________________________\n",
      "activation_10 (Activation)   (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "batch_normalization_12 (Batc (None, 100)               400       \n",
      "_________________________________________________________________\n",
      "activation_11 (Activation)   (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_12 (Dense)             (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "batch_normalization_13 (Batc (None, 100)               400       \n",
      "_________________________________________________________________\n",
      "activation_12 (Activation)   (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_13 (Dense)             (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "batch_normalization_14 (Batc (None, 100)               400       \n",
      "_________________________________________________________________\n",
      "activation_13 (Activation)   (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_14 (Dense)             (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "batch_normalization_15 (Batc (None, 100)               400       \n",
      "_________________________________________________________________\n",
      "activation_14 (Activation)   (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_15 (Dense)             (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "batch_normalization_16 (Batc (None, 100)               400       \n",
      "_________________________________________________________________\n",
      "activation_15 (Activation)   (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_16 (Dense)             (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "batch_normalization_17 (Batc (None, 100)               400       \n",
      "_________________________________________________________________\n",
      "activation_16 (Activation)   (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_17 (Dense)             (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "batch_normalization_18 (Batc (None, 100)               400       \n",
      "_________________________________________________________________\n",
      "activation_17 (Activation)   (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_18 (Dense)             (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "batch_normalization_19 (Batc (None, 100)               400       \n",
      "_________________________________________________________________\n",
      "activation_18 (Activation)   (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_19 (Dense)             (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "batch_normalization_20 (Batc (None, 100)               400       \n",
      "_________________________________________________________________\n",
      "activation_19 (Activation)   (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_20 (Dense)             (None, 10)                1010      \n",
      "=================================================================\n",
      "Total params: 520,498\n",
      "Trainable params: 510,354\n",
      "Non-trainable params: 10,144\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimal_lr = 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss = keras.losses.sparse_categorical_crossentropy, optimizer = keras.optimizers.Nadam(learning_rate = optimal_lr), metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [],
   "source": [
    "early_stopping_cb = keras.callbacks.EarlyStopping(patience = 20)\n",
    "model_checkpoint_cb = keras.callbacks.ModelCheckpoint('CIFAR10_BN_model', save_best_only = True, save_weights_only = True)\n",
    "run_logdir = os.path.join(os.curdir, 'CIFAR10_logs', 'BN', 'optimal_lr_model', 'lr = {}'.format(optimal_lr))\n",
    "tensorboard_cb = keras.callbacks.TensorBoard(run_logdir)\n",
    "one_cycle_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "   2/1407 [..............................] - ETA: 7:29 - loss: 2.9545 - accuracy: 0.0938WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0180s vs `on_train_batch_end` time: 0.6210s). Check your callbacks.\n",
      "1407/1407 [==============================] - 17s 12ms/step - loss: 1.8374 - accuracy: 0.3410 - val_loss: 1.6944 - val_accuracy: 0.4008\n",
      "Epoch 2/100\n",
      "1407/1407 [==============================] - 15s 11ms/step - loss: 1.6859 - accuracy: 0.3998 - val_loss: 1.6820 - val_accuracy: 0.3850\n",
      "Epoch 3/100\n",
      "1407/1407 [==============================] - 15s 11ms/step - loss: 1.6192 - accuracy: 0.4231 - val_loss: 1.6028 - val_accuracy: 0.4250\n",
      "Epoch 4/100\n",
      "1407/1407 [==============================] - 16s 11ms/step - loss: 1.5717 - accuracy: 0.4369 - val_loss: 1.5617 - val_accuracy: 0.4480\n",
      "Epoch 5/100\n",
      "1407/1407 [==============================] - 15s 11ms/step - loss: 1.5286 - accuracy: 0.4608 - val_loss: 1.4750 - val_accuracy: 0.4708\n",
      "Epoch 6/100\n",
      "1407/1407 [==============================] - 15s 11ms/step - loss: 1.4899 - accuracy: 0.4730 - val_loss: 1.4929 - val_accuracy: 0.4742\n",
      "Epoch 7/100\n",
      "1407/1407 [==============================] - 16s 11ms/step - loss: 1.4539 - accuracy: 0.4841 - val_loss: 1.4742 - val_accuracy: 0.4694\n",
      "Epoch 8/100\n",
      "1407/1407 [==============================] - 16s 11ms/step - loss: 1.4262 - accuracy: 0.4974 - val_loss: 1.4236 - val_accuracy: 0.4880\n",
      "Epoch 9/100\n",
      "1407/1407 [==============================] - 15s 11ms/step - loss: 1.3987 - accuracy: 0.5075 - val_loss: 1.4360 - val_accuracy: 0.4966\n",
      "Epoch 10/100\n",
      "1407/1407 [==============================] - 16s 12ms/step - loss: 1.3739 - accuracy: 0.5157 - val_loss: 1.3685 - val_accuracy: 0.5222\n",
      "Epoch 11/100\n",
      "1407/1407 [==============================] - 16s 11ms/step - loss: 1.3558 - accuracy: 0.5216 - val_loss: 1.3487 - val_accuracy: 0.5174\n",
      "Epoch 12/100\n",
      "1407/1407 [==============================] - 16s 11ms/step - loss: 1.3275 - accuracy: 0.5332 - val_loss: 1.3995 - val_accuracy: 0.5044\n",
      "Epoch 13/100\n",
      "1407/1407 [==============================] - 16s 11ms/step - loss: 1.3099 - accuracy: 0.5385 - val_loss: 1.3863 - val_accuracy: 0.5120\n",
      "Epoch 14/100\n",
      "1407/1407 [==============================] - 16s 11ms/step - loss: 1.2898 - accuracy: 0.5475 - val_loss: 1.3638 - val_accuracy: 0.5280\n",
      "Epoch 15/100\n",
      "1407/1407 [==============================] - 16s 11ms/step - loss: 1.2677 - accuracy: 0.5532 - val_loss: 1.3788 - val_accuracy: 0.5132\n",
      "Epoch 16/100\n",
      "1407/1407 [==============================] - 16s 11ms/step - loss: 1.2558 - accuracy: 0.5580 - val_loss: 1.3443 - val_accuracy: 0.5376\n",
      "Epoch 17/100\n",
      "1407/1407 [==============================] - 16s 11ms/step - loss: 1.2358 - accuracy: 0.5645 - val_loss: 1.3359 - val_accuracy: 0.5348\n",
      "Epoch 18/100\n",
      "1407/1407 [==============================] - 16s 11ms/step - loss: 1.2192 - accuracy: 0.5705 - val_loss: 1.3512 - val_accuracy: 0.5396\n",
      "Epoch 19/100\n",
      "1407/1407 [==============================] - 16s 11ms/step - loss: 1.2059 - accuracy: 0.5767 - val_loss: 1.3877 - val_accuracy: 0.5216\n",
      "Epoch 20/100\n",
      "1407/1407 [==============================] - 16s 12ms/step - loss: 1.1962 - accuracy: 0.5808 - val_loss: 1.3433 - val_accuracy: 0.5428\n",
      "Epoch 21/100\n",
      "1407/1407 [==============================] - 16s 11ms/step - loss: 1.1752 - accuracy: 0.5890 - val_loss: 1.3990 - val_accuracy: 0.5252\n",
      "Epoch 22/100\n",
      "1407/1407 [==============================] - 16s 11ms/step - loss: 1.1626 - accuracy: 0.5927 - val_loss: 1.3276 - val_accuracy: 0.5452\n",
      "Epoch 23/100\n",
      "1407/1407 [==============================] - 17s 12ms/step - loss: 1.1493 - accuracy: 0.5974 - val_loss: 1.3185 - val_accuracy: 0.5530\n",
      "Epoch 24/100\n",
      "1407/1407 [==============================] - 17s 12ms/step - loss: 1.1351 - accuracy: 0.6028 - val_loss: 1.3257 - val_accuracy: 0.5398\n",
      "Epoch 25/100\n",
      "1407/1407 [==============================] - 16s 12ms/step - loss: 1.1271 - accuracy: 0.6064 - val_loss: 1.3468 - val_accuracy: 0.5466\n",
      "Epoch 26/100\n",
      "1407/1407 [==============================] - 17s 12ms/step - loss: 1.1080 - accuracy: 0.6114 - val_loss: 1.3523 - val_accuracy: 0.5468\n",
      "Epoch 27/100\n",
      "1407/1407 [==============================] - 17s 12ms/step - loss: 1.0962 - accuracy: 0.6155 - val_loss: 1.3475 - val_accuracy: 0.5366\n",
      "Epoch 28/100\n",
      "1407/1407 [==============================] - 16s 11ms/step - loss: 1.0872 - accuracy: 0.6216 - val_loss: 1.3529 - val_accuracy: 0.5334\n",
      "Epoch 29/100\n",
      "1407/1407 [==============================] - 16s 11ms/step - loss: 1.0813 - accuracy: 0.6214 - val_loss: 1.3307 - val_accuracy: 0.5452\n",
      "Epoch 30/100\n",
      "1407/1407 [==============================] - 16s 11ms/step - loss: 1.0621 - accuracy: 0.6288 - val_loss: 1.3573 - val_accuracy: 0.5394\n",
      "Epoch 31/100\n",
      "1407/1407 [==============================] - 16s 11ms/step - loss: 1.0546 - accuracy: 0.6296 - val_loss: 1.3592 - val_accuracy: 0.5492\n",
      "Epoch 32/100\n",
      "1407/1407 [==============================] - 16s 12ms/step - loss: 1.0455 - accuracy: 0.6343 - val_loss: 1.3805 - val_accuracy: 0.5376\n",
      "Epoch 33/100\n",
      "1407/1407 [==============================] - 17s 12ms/step - loss: 1.0292 - accuracy: 0.6395 - val_loss: 1.3270 - val_accuracy: 0.5594\n",
      "Epoch 34/100\n",
      "1407/1407 [==============================] - 16s 11ms/step - loss: 1.0210 - accuracy: 0.6437 - val_loss: 1.3622 - val_accuracy: 0.5488\n",
      "Epoch 35/100\n",
      "1407/1407 [==============================] - 16s 11ms/step - loss: 1.0111 - accuracy: 0.6480 - val_loss: 1.3527 - val_accuracy: 0.5430\n",
      "Epoch 36/100\n",
      "1407/1407 [==============================] - 16s 11ms/step - loss: 1.0039 - accuracy: 0.6487 - val_loss: 1.3727 - val_accuracy: 0.5484\n",
      "Epoch 37/100\n",
      "1407/1407 [==============================] - 16s 11ms/step - loss: 0.9844 - accuracy: 0.6548 - val_loss: 1.3452 - val_accuracy: 0.5492\n",
      "Epoch 38/100\n",
      "1407/1407 [==============================] - 16s 11ms/step - loss: 0.9847 - accuracy: 0.6560 - val_loss: 1.3862 - val_accuracy: 0.5428\n",
      "Epoch 39/100\n",
      "1407/1407 [==============================] - 16s 12ms/step - loss: 0.9688 - accuracy: 0.6601 - val_loss: 1.3631 - val_accuracy: 0.5486\n",
      "Epoch 40/100\n",
      "1407/1407 [==============================] - 16s 11ms/step - loss: 0.9593 - accuracy: 0.6663 - val_loss: 1.3795 - val_accuracy: 0.5422\n",
      "Epoch 41/100\n",
      "1407/1407 [==============================] - 17s 12ms/step - loss: 0.9536 - accuracy: 0.6669 - val_loss: 1.3716 - val_accuracy: 0.5468\n",
      "Epoch 42/100\n",
      "1407/1407 [==============================] - 16s 11ms/step - loss: 0.9407 - accuracy: 0.6702 - val_loss: 1.3822 - val_accuracy: 0.5448\n",
      "Epoch 43/100\n",
      "1407/1407 [==============================] - 16s 12ms/step - loss: 0.9391 - accuracy: 0.6716 - val_loss: 1.3898 - val_accuracy: 0.5472\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(x_train, y_train, epochs = 100, validation_data = (x_valid, y_valid), callbacks = [early_stopping_cb, model_checkpoint_cb, tensorboard_cb])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "157/157 [==============================] - 0s 2ms/step - loss: 1.3898 - accuracy: 0.5472\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[1.389768123626709, 0.5472000241279602]"
      ]
     },
     "execution_count": 247,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(x_valid, y_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 1s 2ms/step - loss: 1.3966 - accuracy: 0.5371\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[1.3966196775436401, 0.5371000170707703]"
      ]
     },
     "execution_count": 248,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.training.tracking.util.CheckpointLoadStatus at 0x7fcf9c37adc0>"
      ]
     },
     "execution_count": 249,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_weights('CIFAR10_BN_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "157/157 [==============================] - 0s 2ms/step - loss: 1.3185 - accuracy: 0.5530\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[1.3185102939605713, 0.5529999732971191]"
      ]
     },
     "execution_count": 250,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(x_valid, y_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 1s 2ms/step - loss: 1.3248 - accuracy: 0.5398\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[1.3248482942581177, 0.5397999882698059]"
      ]
     },
     "execution_count": 251,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(x_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- We can see that the model reached the lowest val_loss at 23 epochs when we added BN to the net.\n",
    "- Adding BN enabled us to use much larger lr which reduced the total training time.\n",
    "- The metrics of the model are also better with the validation and test accuracies being 55% and 53% respectively.\n",
    "- The only drawback is that the time taken to complete each epoch increased from 10s to 16s, but on overall the total training time is decreased."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Replacing BN with SELU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- We need to standardize the inputs in order to use the SELU activation func."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_mean = x_train.mean(axis = 0, keepdims = True)\n",
    "x_std = x_train.std(axis = 0, keepdims = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_scaled = (x_train - x_mean) / x_std\n",
    "x_valid_scaled = (x_valid - x_mean) / x_std\n",
    "x_test_scaled = (x_test - x_mean) / x_std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 342,
   "metadata": {},
   "outputs": [],
   "source": [
    "keras.backend.clear_session()\n",
    "tf.random.set_seed(42)\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 343,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.Sequential()\n",
    "model.add(keras.layers.Flatten(input_shape = [32, 32, 3]))\n",
    "for _ in range(20):\n",
    "    model.add(keras.layers.Dense(100, activation = keras.activations.selu, kernel_initializer = keras.initializers.lecun_normal()))\n",
    "model.add(keras.layers.Dense(10, activation = keras.activations.softmax))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 344,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "flatten (Flatten)            (None, 3072)              0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 100)               307300    \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "dense_12 (Dense)             (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "dense_13 (Dense)             (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "dense_14 (Dense)             (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "dense_15 (Dense)             (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "dense_16 (Dense)             (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "dense_17 (Dense)             (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "dense_18 (Dense)             (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "dense_19 (Dense)             (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "dense_20 (Dense)             (None, 10)                1010      \n",
      "=================================================================\n",
      "Total params: 500,210\n",
      "Trainable params: 500,210\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 345,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.0008"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 346,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss = keras.losses.sparse_categorical_crossentropy, optimizer = keras.optimizers.Nadam(learning_rate), metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 347,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_logdir = os.path.join(os.curdir, 'CIFAR10_logs', 'SELU', 'lr = {}'.format(learning_rate))\n",
    "tensorboard_cb = keras.callbacks.TensorBoard(run_logdir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 348,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "   2/1407 [..............................] - ETA: 2:58 - loss: 3.0571 - accuracy: 0.0938WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0076s vs `on_train_batch_end` time: 0.2454s). Check your callbacks.\n",
      "1407/1407 [==============================] - 9s 7ms/step - loss: 1.9313 - accuracy: 0.3097 - val_loss: 1.8588 - val_accuracy: 0.3136\n",
      "Epoch 2/20\n",
      "1407/1407 [==============================] - 9s 6ms/step - loss: 1.7200 - accuracy: 0.3890 - val_loss: 1.7384 - val_accuracy: 0.3778\n",
      "Epoch 3/20\n",
      "1407/1407 [==============================] - 9s 6ms/step - loss: 1.6249 - accuracy: 0.4275 - val_loss: 1.7290 - val_accuracy: 0.3988\n",
      "Epoch 4/20\n",
      "1407/1407 [==============================] - 8s 6ms/step - loss: 1.5634 - accuracy: 0.4478 - val_loss: 1.6357 - val_accuracy: 0.4336\n",
      "Epoch 5/20\n",
      "1407/1407 [==============================] - 9s 6ms/step - loss: 1.5074 - accuracy: 0.4691 - val_loss: 1.5887 - val_accuracy: 0.4392\n",
      "Epoch 6/20\n",
      "1407/1407 [==============================] - 9s 6ms/step - loss: 1.4673 - accuracy: 0.4866 - val_loss: 1.5583 - val_accuracy: 0.4644\n",
      "Epoch 7/20\n",
      "1407/1407 [==============================] - 8s 6ms/step - loss: 1.4314 - accuracy: 0.5005 - val_loss: 1.5709 - val_accuracy: 0.4644\n",
      "Epoch 8/20\n",
      "1407/1407 [==============================] - 9s 7ms/step - loss: 1.3879 - accuracy: 0.5141 - val_loss: 1.4943 - val_accuracy: 0.4806\n",
      "Epoch 9/20\n",
      "1407/1407 [==============================] - 9s 6ms/step - loss: 1.3576 - accuracy: 0.5269 - val_loss: 1.5016 - val_accuracy: 0.4694\n",
      "Epoch 10/20\n",
      "1407/1407 [==============================] - 9s 7ms/step - loss: 1.3339 - accuracy: 0.5359 - val_loss: 1.4981 - val_accuracy: 0.4746\n",
      "Epoch 11/20\n",
      "1407/1407 [==============================] - 9s 6ms/step - loss: 1.3002 - accuracy: 0.5456 - val_loss: 1.5640 - val_accuracy: 0.4858\n",
      "Epoch 12/20\n",
      "1407/1407 [==============================] - 9s 6ms/step - loss: 1.2770 - accuracy: 0.5528 - val_loss: 1.5093 - val_accuracy: 0.4644\n",
      "Epoch 13/20\n",
      "1407/1407 [==============================] - 9s 6ms/step - loss: 1.2502 - accuracy: 0.5652 - val_loss: 1.4966 - val_accuracy: 0.4900\n",
      "Epoch 14/20\n",
      "1407/1407 [==============================] - 9s 6ms/step - loss: 1.2315 - accuracy: 0.5741 - val_loss: 1.4913 - val_accuracy: 0.4980\n",
      "Epoch 15/20\n",
      "1407/1407 [==============================] - 9s 6ms/step - loss: 1.2029 - accuracy: 0.5847 - val_loss: 1.5040 - val_accuracy: 0.4962\n",
      "Epoch 16/20\n",
      "1407/1407 [==============================] - 9s 6ms/step - loss: 1.1782 - accuracy: 0.5922 - val_loss: 1.5320 - val_accuracy: 0.4922\n",
      "Epoch 17/20\n",
      "1407/1407 [==============================] - 9s 6ms/step - loss: 1.1667 - accuracy: 0.5947 - val_loss: 1.5049 - val_accuracy: 0.4924\n",
      "Epoch 18/20\n",
      "1407/1407 [==============================] - 9s 6ms/step - loss: 1.1353 - accuracy: 0.6066 - val_loss: 1.4970 - val_accuracy: 0.5082\n",
      "Epoch 19/20\n",
      "1407/1407 [==============================] - 9s 6ms/step - loss: 1.1198 - accuracy: 0.6111 - val_loss: 1.4975 - val_accuracy: 0.5058\n",
      "Epoch 20/20\n",
      "1407/1407 [==============================] - 9s 6ms/step - loss: 1.1503 - accuracy: 0.6090 - val_loss: 1.5465 - val_accuracy: 0.4896\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(x_train_scaled, y_train, epochs = 20, validation_data = (x_valid_scaled, y_valid), callbacks = [tensorboard_cb])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- After changing the architecture we could find that the model performs at its best at lr = 0.0004."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 363,
   "metadata": {},
   "outputs": [],
   "source": [
    "keras.backend.clear_session()\n",
    "tf.random.set_seed(42)\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 364,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.Sequential()\n",
    "model.add(keras.layers.Flatten(input_shape = [32, 32, 3]))\n",
    "for _ in range(20):\n",
    "    model.add(keras.layers.Dense(100, activation = keras.activations.selu, kernel_initializer = keras.initializers.lecun_normal()))\n",
    "model.add(keras.layers.Dense(10, activation = keras.activations.softmax))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 365,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "flatten (Flatten)            (None, 3072)              0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 100)               307300    \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "dense_12 (Dense)             (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "dense_13 (Dense)             (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "dense_14 (Dense)             (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "dense_15 (Dense)             (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "dense_16 (Dense)             (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "dense_17 (Dense)             (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "dense_18 (Dense)             (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "dense_19 (Dense)             (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "dense_20 (Dense)             (None, 10)                1010      \n",
      "=================================================================\n",
      "Total params: 500,210\n",
      "Trainable params: 500,210\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 366,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimal_lr = 0.0004"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 367,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss = keras.losses.sparse_categorical_crossentropy, optimizer = keras.optimizers.Nadam(optimal_lr), metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 368,
   "metadata": {},
   "outputs": [],
   "source": [
    "early_stopping_cb = keras.callbacks.EarlyStopping(patience = 20)\n",
    "model_checkpoint_cb = keras.callbacks.ModelCheckpoint('CIFAR10_SELU_model', save_best_only = True, save_weights_only = True)\n",
    "run_logdir = os.path.join(os.curdir, 'CIFAR10_logs', 'SELU', 'optimal_model', 'lr = {}'.format(optimal_lr))\n",
    "tensorboard_cb = keras.callbacks.TensorBoard(run_logdir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 369,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "   2/1407 [..............................] - ETA: 2:20 - loss: 2.9763 - accuracy: 0.1250WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0069s vs `on_train_batch_end` time: 0.1924s). Check your callbacks.\n",
      "1407/1407 [==============================] - 10s 7ms/step - loss: 1.8783 - accuracy: 0.3294 - val_loss: 1.7889 - val_accuracy: 0.3518\n",
      "Epoch 2/100\n",
      "1407/1407 [==============================] - 10s 7ms/step - loss: 1.6694 - accuracy: 0.4085 - val_loss: 1.7444 - val_accuracy: 0.3730\n",
      "Epoch 3/100\n",
      "1407/1407 [==============================] - 9s 7ms/step - loss: 1.5651 - accuracy: 0.4500 - val_loss: 1.5895 - val_accuracy: 0.4310\n",
      "Epoch 4/100\n",
      "1407/1407 [==============================] - 9s 7ms/step - loss: 1.4906 - accuracy: 0.4757 - val_loss: 1.5508 - val_accuracy: 0.4604\n",
      "Epoch 5/100\n",
      "1407/1407 [==============================] - 9s 6ms/step - loss: 1.4261 - accuracy: 0.4994 - val_loss: 1.5326 - val_accuracy: 0.4566\n",
      "Epoch 6/100\n",
      "1407/1407 [==============================] - 9s 6ms/step - loss: 1.3760 - accuracy: 0.5150 - val_loss: 1.4838 - val_accuracy: 0.4800\n",
      "Epoch 7/100\n",
      "1407/1407 [==============================] - 8s 6ms/step - loss: 1.3262 - accuracy: 0.5373 - val_loss: 1.4950 - val_accuracy: 0.4838\n",
      "Epoch 8/100\n",
      "1407/1407 [==============================] - 9s 7ms/step - loss: 1.2808 - accuracy: 0.5492 - val_loss: 1.4441 - val_accuracy: 0.5000\n",
      "Epoch 9/100\n",
      "1407/1407 [==============================] - 9s 6ms/step - loss: 1.2378 - accuracy: 0.5706 - val_loss: 1.4800 - val_accuracy: 0.4890\n",
      "Epoch 10/100\n",
      "1407/1407 [==============================] - 9s 6ms/step - loss: 1.2034 - accuracy: 0.5829 - val_loss: 1.4828 - val_accuracy: 0.4990\n",
      "Epoch 11/100\n",
      "1407/1407 [==============================] - 9s 6ms/step - loss: 1.1686 - accuracy: 0.5956 - val_loss: 1.4911 - val_accuracy: 0.5016\n",
      "Epoch 12/100\n",
      "1407/1407 [==============================] - 9s 6ms/step - loss: 1.1321 - accuracy: 0.6078 - val_loss: 1.4539 - val_accuracy: 0.5060\n",
      "Epoch 13/100\n",
      "1407/1407 [==============================] - 9s 7ms/step - loss: 1.1041 - accuracy: 0.6196 - val_loss: 1.4871 - val_accuracy: 0.5054\n",
      "Epoch 14/100\n",
      "1407/1407 [==============================] - 9s 6ms/step - loss: 1.0754 - accuracy: 0.6319 - val_loss: 1.4716 - val_accuracy: 0.5104\n",
      "Epoch 15/100\n",
      "1407/1407 [==============================] - 9s 6ms/step - loss: 1.0390 - accuracy: 0.6407 - val_loss: 1.4800 - val_accuracy: 0.5136\n",
      "Epoch 16/100\n",
      "1407/1407 [==============================] - 9s 6ms/step - loss: 1.0146 - accuracy: 0.6528 - val_loss: 1.5277 - val_accuracy: 0.5210\n",
      "Epoch 17/100\n",
      "1407/1407 [==============================] - 9s 6ms/step - loss: 0.9892 - accuracy: 0.6593 - val_loss: 1.5216 - val_accuracy: 0.5124\n",
      "Epoch 18/100\n",
      "1407/1407 [==============================] - 9s 7ms/step - loss: 0.9640 - accuracy: 0.6736 - val_loss: 1.5493 - val_accuracy: 0.5202\n",
      "Epoch 19/100\n",
      "1407/1407 [==============================] - 9s 7ms/step - loss: 0.9382 - accuracy: 0.6769 - val_loss: 1.5601 - val_accuracy: 0.5192\n",
      "Epoch 20/100\n",
      "1407/1407 [==============================] - 9s 7ms/step - loss: 0.9178 - accuracy: 0.6874 - val_loss: 1.5716 - val_accuracy: 0.5178\n",
      "Epoch 21/100\n",
      "1407/1407 [==============================] - 10s 7ms/step - loss: 0.8949 - accuracy: 0.6958 - val_loss: 1.6265 - val_accuracy: 0.5190\n",
      "Epoch 22/100\n",
      "1407/1407 [==============================] - 10s 7ms/step - loss: 0.8722 - accuracy: 0.7011 - val_loss: 1.6008 - val_accuracy: 0.5132\n",
      "Epoch 23/100\n",
      "1407/1407 [==============================] - 9s 7ms/step - loss: 0.8502 - accuracy: 0.7090 - val_loss: 1.6837 - val_accuracy: 0.4986\n",
      "Epoch 24/100\n",
      "1407/1407 [==============================] - 10s 7ms/step - loss: 0.8390 - accuracy: 0.7138 - val_loss: 1.5859 - val_accuracy: 0.5146\n",
      "Epoch 25/100\n",
      "1407/1407 [==============================] - 10s 7ms/step - loss: 0.8158 - accuracy: 0.7239 - val_loss: 1.6536 - val_accuracy: 0.5072\n",
      "Epoch 26/100\n",
      "1407/1407 [==============================] - 9s 7ms/step - loss: 0.8006 - accuracy: 0.7312 - val_loss: 1.6247 - val_accuracy: 0.5058\n",
      "Epoch 27/100\n",
      "1407/1407 [==============================] - 9s 6ms/step - loss: 0.7830 - accuracy: 0.7374 - val_loss: 1.6342 - val_accuracy: 0.5168\n",
      "Epoch 28/100\n",
      "1407/1407 [==============================] - 9s 6ms/step - loss: 0.7634 - accuracy: 0.7443 - val_loss: 1.6528 - val_accuracy: 0.5068\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(x_train_scaled, y_train, epochs = 100, validation_data = (x_valid_scaled, y_valid), callbacks = [early_stopping_cb, model_checkpoint_cb, tensorboard_cb])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 371,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "157/157 [==============================] - 0s 2ms/step - loss: 1.6528 - accuracy: 0.5068\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[1.652758240699768, 0.5067999958992004]"
      ]
     },
     "execution_count": 371,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(x_valid_scaled, y_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 372,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 1s 2ms/step - loss: 1.6576 - accuracy: 0.5017\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[1.657639980316162, 0.5016999840736389]"
      ]
     },
     "execution_count": 372,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(x_test_scaled, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 373,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.training.tracking.util.CheckpointLoadStatus at 0x7fd0744f53a0>"
      ]
     },
     "execution_count": 373,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_weights('CIFAR10_SELU_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 374,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "157/157 [==============================] - 0s 2ms/step - loss: 1.4441 - accuracy: 0.5000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[1.4440743923187256, 0.5]"
      ]
     },
     "execution_count": 374,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(x_valid_scaled, y_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 376,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 1s 2ms/step - loss: 1.4459 - accuracy: 0.4979\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[1.445914626121521, 0.49790000915527344]"
      ]
     },
     "execution_count": 376,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(x_test_scaled, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- It is the fastest model by far as the best model in terms of val_error was achieved in 8 epochs with 10s for each epoch.\n",
    "- But the drawback is the model performance which even though is better than the raw model is less than that of the model with BN layers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using AlphaDropouts for regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "keras.backend.clear_session()\n",
    "tf.random.set_seed(42)\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.Sequential()\n",
    "model.add(keras.layers.Flatten(input_shape = [32, 32, 3]))\n",
    "for _ in range(20):\n",
    "    model.add(keras.layers.Dense(100, activation = keras.activations.selu, kernel_initializer = keras.initializers.lecun_normal()))\n",
    "model.add(keras.layers.AlphaDropout(rate = 0.1))\n",
    "model.add(keras.layers.Dense(10, activation = keras.activations.softmax))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "flatten (Flatten)            (None, 3072)              0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 100)               307300    \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "dense_12 (Dense)             (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "dense_13 (Dense)             (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "dense_14 (Dense)             (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "dense_15 (Dense)             (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "dense_16 (Dense)             (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "dense_17 (Dense)             (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "dense_18 (Dense)             (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "dense_19 (Dense)             (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "alpha_dropout (AlphaDropout) (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_20 (Dense)             (None, 10)                1010      \n",
      "=================================================================\n",
      "Total params: 500,210\n",
      "Trainable params: 500,210\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss = keras.losses.sparse_categorical_crossentropy, optimizer = keras.optimizers.Nadam(), metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_logdir = os.path.join(os.curdir, 'CIFAR10_logs', 'Dropout', 'Div_3_dropout')\n",
    "tensorboard_cb = keras.callbacks.TensorBoard(run_logdir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "   1/1407 [..............................] - ETA: 0s - loss: 3.0241 - accuracy: 0.0938WARNING:tensorflow:From /home/gokul/.local/lib/python3.8/site-packages/tensorflow/python/ops/summary_ops_v2.py:1277: stop (from tensorflow.python.eager.profiler) is deprecated and will be removed after 2020-07-01.\n",
      "Instructions for updating:\n",
      "use `tf.profiler.experimental.stop` instead.\n",
      "   2/1407 [..............................] - ETA: 1:37 - loss: 3.1068 - accuracy: 0.0625WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0225s vs `on_train_batch_end` time: 0.1146s). Check your callbacks.\n",
      " 383/1407 [=======>......................] - ETA: 5s - loss: 2.1534 - accuracy: 0.2321"
     ]
    }
   ],
   "source": [
    "history = model.fit(x_train_scaled, y_train, epochs = 20, validation_data = (x_valid_scaled, y_valid), callbacks = [tensorboard_cb])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- After training the model we could see that the model performs the best when dropout is added after the last hidden layer.\n",
    "- Now we need to decide the dropout rate as well as the learning_rate for which we will be using Randomized Search CV."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "keras.backend.clear_session()\n",
    "tf.random.set_seed(42)\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(dropout_rate = 0.1, learning_rate = 1e-3):\n",
    "    model = keras.models.Sequential()\n",
    "    model.add(keras.layers.Flatten(input_shape = [32, 32, 3]))\n",
    "    for _ in range(20):\n",
    "        model.add(keras.layers.Dense(100, activation = keras.activations.selu, kernel_initializer = keras.initializers.lecun_normal()))\n",
    "    model.add(keras.layers.AlphaDropout(rate = dropout_rate))\n",
    "    model.add(keras.layers.Dense(10, activation = keras.activations.softmax))\n",
    "    model.compile(loss = keras.losses.sparse_categorical_crossentropy, optimizer = keras.optimizers.Nadam(lr = learning_rate), metrics = ['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = keras.wrappers.scikit_learn.KerasClassifier(build_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {'dropout_rate' : [0.03, 0.1], 'learning_rate' : [0.0001, 0.0003]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "rand_search = RandomizedSearchCV(estimator = classifier, param_distributions = params, n_iter = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "1125/1125 [==============================] - 6s 6ms/step - loss: 2.0797 - accuracy: 0.2344 - val_loss: 1.8949 - val_accuracy: 0.2986\n",
      "Epoch 2/100\n",
      "1125/1125 [==============================] - 6s 6ms/step - loss: 1.8699 - accuracy: 0.3185 - val_loss: 1.7980 - val_accuracy: 0.3504\n",
      "Epoch 3/100\n",
      "1125/1125 [==============================] - 7s 6ms/step - loss: 1.7942 - accuracy: 0.3502 - val_loss: 1.7778 - val_accuracy: 0.3464\n",
      "Epoch 4/100\n",
      "1125/1125 [==============================] - 6s 6ms/step - loss: 1.7502 - accuracy: 0.3672 - val_loss: 1.7320 - val_accuracy: 0.3730\n",
      "Epoch 5/100\n",
      "1125/1125 [==============================] - 7s 6ms/step - loss: 1.7119 - accuracy: 0.3814 - val_loss: 1.7495 - val_accuracy: 0.3668\n",
      "Epoch 6/100\n",
      "1125/1125 [==============================] - 7s 6ms/step - loss: 1.6812 - accuracy: 0.3937 - val_loss: 1.6764 - val_accuracy: 0.3908\n",
      "Epoch 7/100\n",
      "1125/1125 [==============================] - 6s 6ms/step - loss: 1.6541 - accuracy: 0.4060 - val_loss: 1.6416 - val_accuracy: 0.4068\n",
      "Epoch 8/100\n",
      "1125/1125 [==============================] - 7s 6ms/step - loss: 1.6242 - accuracy: 0.4194 - val_loss: 1.6437 - val_accuracy: 0.4092\n",
      "Epoch 9/100\n",
      "1125/1125 [==============================] - 7s 6ms/step - loss: 1.5989 - accuracy: 0.4277 - val_loss: 1.6002 - val_accuracy: 0.4210\n",
      "Epoch 10/100\n",
      "1125/1125 [==============================] - 7s 6ms/step - loss: 1.5826 - accuracy: 0.4329 - val_loss: 1.6282 - val_accuracy: 0.4062\n",
      "Epoch 11/100\n",
      "1125/1125 [==============================] - 7s 6ms/step - loss: 1.5610 - accuracy: 0.4412 - val_loss: 1.6696 - val_accuracy: 0.4164\n",
      "Epoch 12/100\n",
      "1125/1125 [==============================] - 7s 6ms/step - loss: 1.5472 - accuracy: 0.4447 - val_loss: 1.5763 - val_accuracy: 0.4416\n",
      "Epoch 13/100\n",
      "1125/1125 [==============================] - 7s 6ms/step - loss: 1.5253 - accuracy: 0.4558 - val_loss: 1.5989 - val_accuracy: 0.4220\n",
      "Epoch 14/100\n",
      "1125/1125 [==============================] - 7s 6ms/step - loss: 1.5147 - accuracy: 0.4622 - val_loss: 1.6234 - val_accuracy: 0.4256\n",
      "Epoch 15/100\n",
      "1125/1125 [==============================] - 7s 6ms/step - loss: 1.4952 - accuracy: 0.4657 - val_loss: 1.5967 - val_accuracy: 0.4278\n",
      "Epoch 16/100\n",
      "1125/1125 [==============================] - 6s 6ms/step - loss: 1.4848 - accuracy: 0.4694 - val_loss: 1.5665 - val_accuracy: 0.4418\n",
      "Epoch 17/100\n",
      "1125/1125 [==============================] - 7s 6ms/step - loss: 1.4747 - accuracy: 0.4732 - val_loss: 1.5974 - val_accuracy: 0.4240\n",
      "Epoch 18/100\n",
      "1125/1125 [==============================] - 7s 6ms/step - loss: 1.4637 - accuracy: 0.4786 - val_loss: 1.5726 - val_accuracy: 0.4424\n",
      "Epoch 19/100\n",
      "1125/1125 [==============================] - 7s 6ms/step - loss: 1.4473 - accuracy: 0.4823 - val_loss: 1.5735 - val_accuracy: 0.4512\n",
      "Epoch 20/100\n",
      "1125/1125 [==============================] - 7s 6ms/step - loss: 1.4329 - accuracy: 0.4908 - val_loss: 1.5656 - val_accuracy: 0.4550\n",
      "Epoch 21/100\n",
      "1125/1125 [==============================] - 7s 6ms/step - loss: 1.4225 - accuracy: 0.4931 - val_loss: 1.6451 - val_accuracy: 0.4200\n",
      "Epoch 22/100\n",
      "1125/1125 [==============================] - 7s 6ms/step - loss: 1.4177 - accuracy: 0.4944 - val_loss: 1.5544 - val_accuracy: 0.4530\n",
      "Epoch 23/100\n",
      "1125/1125 [==============================] - 7s 6ms/step - loss: 1.4069 - accuracy: 0.4965 - val_loss: 1.6110 - val_accuracy: 0.4342\n",
      "Epoch 24/100\n",
      "1125/1125 [==============================] - 7s 6ms/step - loss: 1.3925 - accuracy: 0.5048 - val_loss: 1.5658 - val_accuracy: 0.4472\n",
      "Epoch 25/100\n",
      "1125/1125 [==============================] - 7s 6ms/step - loss: 1.3845 - accuracy: 0.5077 - val_loss: 1.6030 - val_accuracy: 0.4404\n",
      "Epoch 26/100\n",
      "1125/1125 [==============================] - 7s 6ms/step - loss: 1.3774 - accuracy: 0.5134 - val_loss: 1.5659 - val_accuracy: 0.4576\n",
      "Epoch 27/100\n",
      "1125/1125 [==============================] - 7s 6ms/step - loss: 1.3618 - accuracy: 0.5154 - val_loss: 1.5555 - val_accuracy: 0.4538\n",
      "Epoch 28/100\n",
      "1125/1125 [==============================] - 7s 6ms/step - loss: 1.3511 - accuracy: 0.5200 - val_loss: 1.5486 - val_accuracy: 0.4642\n",
      "Epoch 29/100\n",
      "1125/1125 [==============================] - 7s 6ms/step - loss: 1.3450 - accuracy: 0.5207 - val_loss: 1.5645 - val_accuracy: 0.4568\n",
      "Epoch 30/100\n",
      "1125/1125 [==============================] - 7s 6ms/step - loss: 1.3342 - accuracy: 0.5241 - val_loss: 1.5518 - val_accuracy: 0.4678\n",
      "Epoch 31/100\n",
      "1125/1125 [==============================] - 7s 6ms/step - loss: 1.3240 - accuracy: 0.5274 - val_loss: 1.5777 - val_accuracy: 0.4502\n",
      "Epoch 32/100\n",
      "1125/1125 [==============================] - 7s 6ms/step - loss: 1.3164 - accuracy: 0.5322 - val_loss: 1.5529 - val_accuracy: 0.4562\n",
      "Epoch 33/100\n",
      "1125/1125 [==============================] - 7s 6ms/step - loss: 1.3096 - accuracy: 0.5329 - val_loss: 1.6097 - val_accuracy: 0.4522\n",
      "Epoch 34/100\n",
      "1125/1125 [==============================] - 7s 6ms/step - loss: 1.3030 - accuracy: 0.5354 - val_loss: 1.5683 - val_accuracy: 0.4570\n",
      "Epoch 35/100\n",
      "1125/1125 [==============================] - 7s 6ms/step - loss: 1.2928 - accuracy: 0.5387 - val_loss: 1.5551 - val_accuracy: 0.4706\n",
      "Epoch 36/100\n",
      "1125/1125 [==============================] - 7s 6ms/step - loss: 1.2777 - accuracy: 0.5442 - val_loss: 1.6317 - val_accuracy: 0.4468\n",
      "Epoch 37/100\n",
      "1125/1125 [==============================] - 7s 6ms/step - loss: 1.2700 - accuracy: 0.5452 - val_loss: 1.6250 - val_accuracy: 0.4606\n",
      "Epoch 38/100\n",
      "1125/1125 [==============================] - 7s 6ms/step - loss: 1.2817 - accuracy: 0.5469 - val_loss: 1.6392 - val_accuracy: 0.4584\n",
      "Epoch 39/100\n",
      "1125/1125 [==============================] - 7s 6ms/step - loss: 1.2516 - accuracy: 0.5541 - val_loss: 1.5829 - val_accuracy: 0.4598\n",
      "Epoch 40/100\n",
      "1125/1125 [==============================] - 7s 6ms/step - loss: 1.2472 - accuracy: 0.5514 - val_loss: 1.5975 - val_accuracy: 0.4550\n",
      "Epoch 41/100\n",
      "1125/1125 [==============================] - 7s 6ms/step - loss: 1.4629 - accuracy: 0.4767 - val_loss: 1.7118 - val_accuracy: 0.3910\n",
      "Epoch 42/100\n",
      "1125/1125 [==============================] - 7s 6ms/step - loss: 1.5717 - accuracy: 0.4346 - val_loss: 1.6252 - val_accuracy: 0.4218\n",
      "Epoch 43/100\n",
      "1125/1125 [==============================] - 7s 6ms/step - loss: 1.3816 - accuracy: 0.5091 - val_loss: 1.6192 - val_accuracy: 0.4604\n",
      "Epoch 44/100\n",
      "1125/1125 [==============================] - 7s 6ms/step - loss: 1.2620 - accuracy: 0.5510 - val_loss: 1.5851 - val_accuracy: 0.4606\n",
      "Epoch 45/100\n",
      "1125/1125 [==============================] - 7s 6ms/step - loss: 1.2434 - accuracy: 0.5561 - val_loss: 1.6252 - val_accuracy: 0.4518\n",
      "Epoch 46/100\n",
      "1125/1125 [==============================] - 7s 6ms/step - loss: 1.2253 - accuracy: 0.5646 - val_loss: 1.6490 - val_accuracy: 0.4482\n",
      "Epoch 47/100\n",
      "1125/1125 [==============================] - 7s 6ms/step - loss: 1.2179 - accuracy: 0.5657 - val_loss: 1.5946 - val_accuracy: 0.4594\n",
      "Epoch 48/100\n",
      "1125/1125 [==============================] - 7s 6ms/step - loss: 1.2039 - accuracy: 0.5748 - val_loss: 1.6346 - val_accuracy: 0.4510\n",
      "282/282 [==============================] - 2s 6ms/step - loss: 1.5425 - accuracy: 0.4679\n",
      "Epoch 1/100\n",
      "1125/1125 [==============================] - 7s 6ms/step - loss: 2.0689 - accuracy: 0.2379 - val_loss: 1.8828 - val_accuracy: 0.3016\n",
      "Epoch 2/100\n",
      "1125/1125 [==============================] - 7s 6ms/step - loss: 1.8756 - accuracy: 0.3051 - val_loss: 1.8590 - val_accuracy: 0.3182\n",
      "Epoch 3/100\n",
      "1125/1125 [==============================] - 7s 6ms/step - loss: 1.8082 - accuracy: 0.3376 - val_loss: 1.7762 - val_accuracy: 0.3504\n",
      "Epoch 4/100\n",
      "1125/1125 [==============================] - 7s 6ms/step - loss: 1.7605 - accuracy: 0.3592 - val_loss: 1.7264 - val_accuracy: 0.3666\n",
      "Epoch 5/100\n",
      "1125/1125 [==============================] - 6s 6ms/step - loss: 1.7117 - accuracy: 0.3812 - val_loss: 1.7237 - val_accuracy: 0.3790\n",
      "Epoch 6/100\n",
      "1125/1125 [==============================] - 7s 6ms/step - loss: 1.6749 - accuracy: 0.3931 - val_loss: 1.7549 - val_accuracy: 0.3730\n",
      "Epoch 7/100\n",
      "1125/1125 [==============================] - 7s 6ms/step - loss: 1.6447 - accuracy: 0.4091 - val_loss: 1.6248 - val_accuracy: 0.4250\n",
      "Epoch 8/100\n",
      "1125/1125 [==============================] - 7s 6ms/step - loss: 1.6100 - accuracy: 0.4202 - val_loss: 1.6212 - val_accuracy: 0.4212\n",
      "Epoch 9/100\n",
      "1125/1125 [==============================] - 7s 6ms/step - loss: 1.5924 - accuracy: 0.4266 - val_loss: 1.7096 - val_accuracy: 0.3860\n",
      "Epoch 10/100\n",
      "1125/1125 [==============================] - 7s 6ms/step - loss: 1.5718 - accuracy: 0.4340 - val_loss: 1.6055 - val_accuracy: 0.4300\n",
      "Epoch 11/100\n",
      "1125/1125 [==============================] - 7s 6ms/step - loss: 1.5558 - accuracy: 0.4418 - val_loss: 1.6288 - val_accuracy: 0.4254\n",
      "Epoch 12/100\n",
      "1125/1125 [==============================] - 7s 6ms/step - loss: 1.5536 - accuracy: 0.4415 - val_loss: 1.6342 - val_accuracy: 0.4160\n",
      "Epoch 13/100\n",
      "1125/1125 [==============================] - 7s 6ms/step - loss: 1.5238 - accuracy: 0.4543 - val_loss: 1.6011 - val_accuracy: 0.4312\n",
      "Epoch 14/100\n",
      "1125/1125 [==============================] - 7s 6ms/step - loss: 1.5091 - accuracy: 0.4565 - val_loss: 1.6138 - val_accuracy: 0.4210\n",
      "Epoch 15/100\n",
      "1125/1125 [==============================] - 7s 6ms/step - loss: 1.4949 - accuracy: 0.4653 - val_loss: 1.5836 - val_accuracy: 0.4352\n",
      "Epoch 16/100\n",
      "1125/1125 [==============================] - 7s 6ms/step - loss: 1.4812 - accuracy: 0.4683 - val_loss: 1.5736 - val_accuracy: 0.4506\n",
      "Epoch 17/100\n",
      "1125/1125 [==============================] - 7s 6ms/step - loss: 1.4691 - accuracy: 0.4710 - val_loss: 1.5818 - val_accuracy: 0.4340\n",
      "Epoch 18/100\n",
      "1125/1125 [==============================] - 7s 6ms/step - loss: 1.4575 - accuracy: 0.4799 - val_loss: 1.5723 - val_accuracy: 0.4576\n",
      "Epoch 19/100\n",
      "1125/1125 [==============================] - 7s 6ms/step - loss: 1.4469 - accuracy: 0.4826 - val_loss: 1.5760 - val_accuracy: 0.4532\n",
      "Epoch 20/100\n",
      "1125/1125 [==============================] - 7s 6ms/step - loss: 1.4327 - accuracy: 0.4871 - val_loss: 1.5690 - val_accuracy: 0.4500\n",
      "Epoch 21/100\n",
      "1125/1125 [==============================] - 7s 6ms/step - loss: 1.4235 - accuracy: 0.4887 - val_loss: 1.5820 - val_accuracy: 0.4424\n",
      "Epoch 22/100\n",
      "1125/1125 [==============================] - 7s 6ms/step - loss: 1.4108 - accuracy: 0.4937 - val_loss: 1.5750 - val_accuracy: 0.4516\n",
      "Epoch 23/100\n",
      "1125/1125 [==============================] - 7s 6ms/step - loss: 1.4013 - accuracy: 0.4991 - val_loss: 1.5575 - val_accuracy: 0.4570\n",
      "Epoch 24/100\n",
      "1125/1125 [==============================] - 7s 6ms/step - loss: 1.3923 - accuracy: 0.5022 - val_loss: 1.5541 - val_accuracy: 0.4540\n",
      "Epoch 25/100\n",
      "1125/1125 [==============================] - 7s 6ms/step - loss: 1.3771 - accuracy: 0.5049 - val_loss: 1.6160 - val_accuracy: 0.4442\n",
      "Epoch 26/100\n",
      "1125/1125 [==============================] - 7s 6ms/step - loss: 1.3687 - accuracy: 0.5096 - val_loss: 1.6303 - val_accuracy: 0.4418\n",
      "Epoch 27/100\n",
      "1125/1125 [==============================] - 7s 6ms/step - loss: 1.3568 - accuracy: 0.5133 - val_loss: 1.6081 - val_accuracy: 0.4460\n",
      "Epoch 28/100\n",
      "1125/1125 [==============================] - 7s 6ms/step - loss: 1.3480 - accuracy: 0.5185 - val_loss: 1.6047 - val_accuracy: 0.4608\n",
      "Epoch 29/100\n",
      "1125/1125 [==============================] - 7s 6ms/step - loss: 1.3393 - accuracy: 0.5198 - val_loss: 1.6026 - val_accuracy: 0.4556\n",
      "Epoch 30/100\n",
      "1125/1125 [==============================] - 7s 6ms/step - loss: 1.3303 - accuracy: 0.5240 - val_loss: 1.6581 - val_accuracy: 0.4412\n",
      "Epoch 31/100\n",
      "1125/1125 [==============================] - 7s 6ms/step - loss: 1.3192 - accuracy: 0.5272 - val_loss: 1.6114 - val_accuracy: 0.4524\n",
      "Epoch 32/100\n",
      "1125/1125 [==============================] - 7s 6ms/step - loss: 1.3082 - accuracy: 0.5299 - val_loss: 1.6037 - val_accuracy: 0.4444\n",
      "Epoch 33/100\n",
      "1125/1125 [==============================] - 7s 6ms/step - loss: 1.2997 - accuracy: 0.5353 - val_loss: 1.6099 - val_accuracy: 0.4554\n",
      "Epoch 34/100\n",
      "1125/1125 [==============================] - 7s 6ms/step - loss: 1.2899 - accuracy: 0.5400 - val_loss: 1.5810 - val_accuracy: 0.4522\n",
      "Epoch 35/100\n",
      "1125/1125 [==============================] - 7s 6ms/step - loss: 1.2808 - accuracy: 0.5439 - val_loss: 1.5883 - val_accuracy: 0.4554\n",
      "Epoch 36/100\n",
      "1125/1125 [==============================] - 7s 6ms/step - loss: 1.2789 - accuracy: 0.5446 - val_loss: 1.5788 - val_accuracy: 0.4604\n",
      "Epoch 37/100\n",
      "1125/1125 [==============================] - 7s 6ms/step - loss: 1.3389 - accuracy: 0.5235 - val_loss: 1.6177 - val_accuracy: 0.4514\n",
      "Epoch 38/100\n",
      "1125/1125 [==============================] - 7s 6ms/step - loss: 1.2571 - accuracy: 0.5523 - val_loss: 1.6069 - val_accuracy: 0.4486\n",
      "Epoch 39/100\n",
      "1125/1125 [==============================] - 7s 6ms/step - loss: 1.2476 - accuracy: 0.5530 - val_loss: 1.5946 - val_accuracy: 0.4606\n",
      "Epoch 40/100\n",
      "1125/1125 [==============================] - 7s 6ms/step - loss: 1.2446 - accuracy: 0.5547 - val_loss: 1.5611 - val_accuracy: 0.4664\n",
      "Epoch 41/100\n",
      "1125/1125 [==============================] - 7s 6ms/step - loss: 1.2353 - accuracy: 0.5592 - val_loss: 1.6603 - val_accuracy: 0.4384\n",
      "Epoch 42/100\n",
      "1125/1125 [==============================] - 7s 6ms/step - loss: 1.2284 - accuracy: 0.5624 - val_loss: 1.6450 - val_accuracy: 0.4462\n",
      "Epoch 43/100\n",
      "1125/1125 [==============================] - 7s 6ms/step - loss: 1.2172 - accuracy: 0.5635 - val_loss: 1.6661 - val_accuracy: 0.4578\n",
      "Epoch 44/100\n",
      "1125/1125 [==============================] - 7s 6ms/step - loss: 1.2069 - accuracy: 0.5693 - val_loss: 1.5949 - val_accuracy: 0.4574\n",
      "282/282 [==============================] - 1s 2ms/step - loss: 1.5544 - accuracy: 0.4528\n",
      "Epoch 1/100\n",
      "1125/1125 [==============================] - 7s 6ms/step - loss: 2.1054 - accuracy: 0.2250 - val_loss: 1.9194 - val_accuracy: 0.2908\n",
      "Epoch 2/100\n",
      "1125/1125 [==============================] - 7s 6ms/step - loss: 1.8891 - accuracy: 0.3022 - val_loss: 1.9271 - val_accuracy: 0.3230\n",
      "Epoch 3/100\n",
      "1125/1125 [==============================] - 7s 6ms/step - loss: 1.8182 - accuracy: 0.3331 - val_loss: 1.7943 - val_accuracy: 0.3364\n",
      "Epoch 4/100\n",
      "1125/1125 [==============================] - 7s 6ms/step - loss: 1.7670 - accuracy: 0.3562 - val_loss: 1.7302 - val_accuracy: 0.3634\n",
      "Epoch 5/100\n",
      "1125/1125 [==============================] - 7s 6ms/step - loss: 1.7223 - accuracy: 0.3759 - val_loss: 1.7249 - val_accuracy: 0.3630\n",
      "Epoch 6/100\n",
      "1125/1125 [==============================] - 7s 6ms/step - loss: 1.6840 - accuracy: 0.3907 - val_loss: 1.7556 - val_accuracy: 0.3824\n",
      "Epoch 7/100\n",
      "1125/1125 [==============================] - 7s 6ms/step - loss: 1.6534 - accuracy: 0.4007 - val_loss: 1.6650 - val_accuracy: 0.3942\n",
      "Epoch 8/100\n",
      "1125/1125 [==============================] - 7s 6ms/step - loss: 1.6279 - accuracy: 0.4156 - val_loss: 1.6370 - val_accuracy: 0.4118\n",
      "Epoch 9/100\n",
      "1125/1125 [==============================] - 7s 6ms/step - loss: 1.5992 - accuracy: 0.4280 - val_loss: 1.6274 - val_accuracy: 0.4170\n",
      "Epoch 10/100\n",
      "1125/1125 [==============================] - 7s 6ms/step - loss: 1.5782 - accuracy: 0.4324 - val_loss: 1.6038 - val_accuracy: 0.4270\n",
      "Epoch 11/100\n",
      "1125/1125 [==============================] - 7s 6ms/step - loss: 1.5632 - accuracy: 0.4395 - val_loss: 1.6002 - val_accuracy: 0.4314\n",
      "Epoch 12/100\n",
      "1125/1125 [==============================] - 7s 6ms/step - loss: 1.5471 - accuracy: 0.4393 - val_loss: 1.5902 - val_accuracy: 0.4290\n",
      "Epoch 13/100\n",
      "1125/1125 [==============================] - 7s 6ms/step - loss: 1.5262 - accuracy: 0.4518 - val_loss: 1.5664 - val_accuracy: 0.4264\n",
      "Epoch 14/100\n",
      "1125/1125 [==============================] - 7s 6ms/step - loss: 1.5165 - accuracy: 0.4561 - val_loss: 1.5718 - val_accuracy: 0.4360\n",
      "Epoch 15/100\n",
      "1125/1125 [==============================] - 7s 6ms/step - loss: 1.5021 - accuracy: 0.4638 - val_loss: 1.5656 - val_accuracy: 0.4316\n",
      "Epoch 16/100\n",
      "1125/1125 [==============================] - 7s 6ms/step - loss: 1.4979 - accuracy: 0.4646 - val_loss: 1.5643 - val_accuracy: 0.4476\n",
      "Epoch 17/100\n",
      "1125/1125 [==============================] - 7s 6ms/step - loss: 1.4745 - accuracy: 0.4736 - val_loss: 1.5473 - val_accuracy: 0.4416\n",
      "Epoch 18/100\n",
      "1125/1125 [==============================] - 7s 6ms/step - loss: 1.4616 - accuracy: 0.4755 - val_loss: 1.5664 - val_accuracy: 0.4462\n",
      "Epoch 19/100\n",
      "1125/1125 [==============================] - 7s 6ms/step - loss: 1.5565 - accuracy: 0.4406 - val_loss: 1.5598 - val_accuracy: 0.4526\n",
      "Epoch 20/100\n",
      "1125/1125 [==============================] - 7s 6ms/step - loss: 1.4650 - accuracy: 0.4748 - val_loss: 1.5562 - val_accuracy: 0.4358\n",
      "Epoch 21/100\n",
      "1125/1125 [==============================] - 7s 6ms/step - loss: 1.4512 - accuracy: 0.4816 - val_loss: 1.5648 - val_accuracy: 0.4468\n",
      "Epoch 22/100\n",
      "1125/1125 [==============================] - 7s 6ms/step - loss: 1.4383 - accuracy: 0.4876 - val_loss: 1.5810 - val_accuracy: 0.4434\n",
      "Epoch 23/100\n",
      "1125/1125 [==============================] - 7s 6ms/step - loss: 1.4402 - accuracy: 0.4860 - val_loss: 1.5742 - val_accuracy: 0.4494\n",
      "Epoch 24/100\n",
      "1125/1125 [==============================] - 7s 6ms/step - loss: 1.4078 - accuracy: 0.4977 - val_loss: 1.5812 - val_accuracy: 0.4358\n",
      "Epoch 25/100\n",
      "1125/1125 [==============================] - 7s 6ms/step - loss: 1.4045 - accuracy: 0.4972 - val_loss: 1.5572 - val_accuracy: 0.4536\n",
      "Epoch 26/100\n",
      "1125/1125 [==============================] - 7s 6ms/step - loss: 1.3973 - accuracy: 0.4991 - val_loss: 1.5471 - val_accuracy: 0.4534\n",
      "Epoch 27/100\n",
      "1125/1125 [==============================] - 7s 6ms/step - loss: 1.3846 - accuracy: 0.5061 - val_loss: 1.5549 - val_accuracy: 0.4574\n",
      "Epoch 28/100\n",
      "1125/1125 [==============================] - 7s 6ms/step - loss: 1.3727 - accuracy: 0.5113 - val_loss: 1.5701 - val_accuracy: 0.4530\n",
      "Epoch 29/100\n",
      "1125/1125 [==============================] - 7s 6ms/step - loss: 1.3654 - accuracy: 0.5133 - val_loss: 1.5498 - val_accuracy: 0.4482\n",
      "Epoch 30/100\n",
      "1125/1125 [==============================] - 7s 6ms/step - loss: 1.3597 - accuracy: 0.5149 - val_loss: 1.5735 - val_accuracy: 0.4462\n",
      "Epoch 31/100\n",
      "1125/1125 [==============================] - 7s 6ms/step - loss: 1.3401 - accuracy: 0.5218 - val_loss: 1.5757 - val_accuracy: 0.4600\n",
      "Epoch 32/100\n",
      "1125/1125 [==============================] - 7s 6ms/step - loss: 1.3309 - accuracy: 0.5257 - val_loss: 1.5569 - val_accuracy: 0.4654\n",
      "Epoch 33/100\n",
      "1125/1125 [==============================] - 7s 6ms/step - loss: 1.3211 - accuracy: 0.5297 - val_loss: 1.6104 - val_accuracy: 0.4342\n",
      "Epoch 34/100\n",
      "1125/1125 [==============================] - 7s 6ms/step - loss: 1.3195 - accuracy: 0.5263 - val_loss: 1.5638 - val_accuracy: 0.4614\n",
      "Epoch 35/100\n",
      "1125/1125 [==============================] - 7s 6ms/step - loss: 1.3145 - accuracy: 0.5298 - val_loss: 1.5839 - val_accuracy: 0.4558\n",
      "Epoch 36/100\n",
      "1125/1125 [==============================] - 7s 6ms/step - loss: 1.3068 - accuracy: 0.5332 - val_loss: 1.6175 - val_accuracy: 0.4456\n",
      "Epoch 37/100\n",
      "1125/1125 [==============================] - 7s 6ms/step - loss: 1.2893 - accuracy: 0.5382 - val_loss: 1.5995 - val_accuracy: 0.4546\n",
      "Epoch 38/100\n",
      "1125/1125 [==============================] - 7s 6ms/step - loss: 1.2856 - accuracy: 0.5415 - val_loss: 1.5651 - val_accuracy: 0.4580\n",
      "Epoch 39/100\n",
      "1125/1125 [==============================] - 7s 6ms/step - loss: 1.2773 - accuracy: 0.5429 - val_loss: 1.5818 - val_accuracy: 0.4586\n",
      "Epoch 40/100\n",
      "1125/1125 [==============================] - 7s 6ms/step - loss: 1.2673 - accuracy: 0.5475 - val_loss: 1.6217 - val_accuracy: 0.4628\n",
      "Epoch 41/100\n",
      "1125/1125 [==============================] - 7s 6ms/step - loss: 1.2647 - accuracy: 0.5452 - val_loss: 1.6278 - val_accuracy: 0.4602\n",
      "Epoch 42/100\n",
      "1125/1125 [==============================] - 7s 6ms/step - loss: 1.2552 - accuracy: 0.5488 - val_loss: 1.6323 - val_accuracy: 0.4566\n",
      "Epoch 43/100\n",
      "1125/1125 [==============================] - 7s 6ms/step - loss: 1.2433 - accuracy: 0.5546 - val_loss: 1.6184 - val_accuracy: 0.4602\n",
      "Epoch 44/100\n",
      "1125/1125 [==============================] - 7s 6ms/step - loss: 1.2429 - accuracy: 0.5552 - val_loss: 1.6096 - val_accuracy: 0.4576\n",
      "Epoch 45/100\n",
      "1125/1125 [==============================] - 7s 6ms/step - loss: 1.2249 - accuracy: 0.5641 - val_loss: 1.6338 - val_accuracy: 0.4602\n",
      "Epoch 46/100\n",
      "1125/1125 [==============================] - 7s 6ms/step - loss: 1.2186 - accuracy: 0.5633 - val_loss: 1.5993 - val_accuracy: 0.4678\n",
      "282/282 [==============================] - 1s 2ms/step - loss: 1.5526 - accuracy: 0.4502\n",
      "Epoch 1/100\n",
      "1125/1125 [==============================] - 7s 6ms/step - loss: 2.1000 - accuracy: 0.2264 - val_loss: 1.8731 - val_accuracy: 0.3182\n",
      "Epoch 2/100\n",
      "1125/1125 [==============================] - 7s 6ms/step - loss: 1.8690 - accuracy: 0.3169 - val_loss: 1.8130 - val_accuracy: 0.3502\n",
      "Epoch 3/100\n",
      "1125/1125 [==============================] - 7s 6ms/step - loss: 1.7972 - accuracy: 0.3469 - val_loss: 1.7787 - val_accuracy: 0.3578\n",
      "Epoch 4/100\n",
      "1125/1125 [==============================] - 7s 6ms/step - loss: 1.7414 - accuracy: 0.3691 - val_loss: 1.7000 - val_accuracy: 0.3816\n",
      "Epoch 5/100\n",
      "1125/1125 [==============================] - 7s 6ms/step - loss: 1.6953 - accuracy: 0.3861 - val_loss: 1.6657 - val_accuracy: 0.3960\n",
      "Epoch 6/100\n",
      "1125/1125 [==============================] - 7s 6ms/step - loss: 1.6610 - accuracy: 0.4025 - val_loss: 1.7173 - val_accuracy: 0.3936\n",
      "Epoch 7/100\n",
      "1125/1125 [==============================] - 7s 6ms/step - loss: 1.6295 - accuracy: 0.4164 - val_loss: 1.6463 - val_accuracy: 0.4224\n",
      "Epoch 8/100\n",
      "1125/1125 [==============================] - 7s 6ms/step - loss: 1.6039 - accuracy: 0.4279 - val_loss: 1.6103 - val_accuracy: 0.4302\n",
      "Epoch 9/100\n",
      "1125/1125 [==============================] - 7s 6ms/step - loss: 1.5788 - accuracy: 0.4351 - val_loss: 1.6319 - val_accuracy: 0.4164\n",
      "Epoch 10/100\n",
      "1125/1125 [==============================] - 7s 6ms/step - loss: 1.5634 - accuracy: 0.4396 - val_loss: 1.6178 - val_accuracy: 0.4294\n",
      "Epoch 11/100\n",
      "1125/1125 [==============================] - 7s 6ms/step - loss: 1.5442 - accuracy: 0.4461 - val_loss: 1.6306 - val_accuracy: 0.4226\n",
      "Epoch 12/100\n",
      "1125/1125 [==============================] - 7s 6ms/step - loss: 1.5311 - accuracy: 0.4518 - val_loss: 1.5942 - val_accuracy: 0.4376\n",
      "Epoch 13/100\n",
      "1125/1125 [==============================] - 7s 6ms/step - loss: 1.5165 - accuracy: 0.4572 - val_loss: 1.6019 - val_accuracy: 0.4294\n",
      "Epoch 14/100\n",
      "1125/1125 [==============================] - 7s 7ms/step - loss: 1.4955 - accuracy: 0.4629 - val_loss: 1.6270 - val_accuracy: 0.4170\n",
      "Epoch 15/100\n",
      "1125/1125 [==============================] - 7s 6ms/step - loss: 1.4855 - accuracy: 0.4695 - val_loss: 1.5749 - val_accuracy: 0.4502\n",
      "Epoch 16/100\n",
      "1125/1125 [==============================] - 7s 6ms/step - loss: 1.4666 - accuracy: 0.4732 - val_loss: 1.6082 - val_accuracy: 0.4290\n",
      "Epoch 17/100\n",
      "1125/1125 [==============================] - 7s 6ms/step - loss: 1.4576 - accuracy: 0.4773 - val_loss: 1.5547 - val_accuracy: 0.4512\n",
      "Epoch 18/100\n",
      "1125/1125 [==============================] - 7s 6ms/step - loss: 1.4473 - accuracy: 0.4799 - val_loss: 1.5448 - val_accuracy: 0.4586\n",
      "Epoch 19/100\n",
      "1125/1125 [==============================] - 7s 6ms/step - loss: 1.4338 - accuracy: 0.4875 - val_loss: 1.5573 - val_accuracy: 0.4544\n",
      "Epoch 20/100\n",
      "1125/1125 [==============================] - 7s 6ms/step - loss: 1.4222 - accuracy: 0.4918 - val_loss: 1.5455 - val_accuracy: 0.4532\n",
      "Epoch 21/100\n",
      "1125/1125 [==============================] - 7s 6ms/step - loss: 1.4123 - accuracy: 0.4966 - val_loss: 1.6139 - val_accuracy: 0.4240\n",
      "Epoch 22/100\n",
      "1125/1125 [==============================] - 7s 6ms/step - loss: 1.3989 - accuracy: 0.4961 - val_loss: 1.5721 - val_accuracy: 0.4598\n",
      "Epoch 23/100\n",
      "1125/1125 [==============================] - 7s 6ms/step - loss: 1.3845 - accuracy: 0.5042 - val_loss: 1.5934 - val_accuracy: 0.4488\n",
      "Epoch 24/100\n",
      "1125/1125 [==============================] - 7s 6ms/step - loss: 1.3800 - accuracy: 0.5047 - val_loss: 1.5444 - val_accuracy: 0.4546\n",
      "Epoch 25/100\n",
      "1125/1125 [==============================] - 7s 6ms/step - loss: 1.3629 - accuracy: 0.5099 - val_loss: 1.5328 - val_accuracy: 0.4672\n",
      "Epoch 26/100\n",
      "1125/1125 [==============================] - 7s 6ms/step - loss: 1.3541 - accuracy: 0.5143 - val_loss: 1.5822 - val_accuracy: 0.4588\n",
      "Epoch 27/100\n",
      "1125/1125 [==============================] - 7s 6ms/step - loss: 1.3398 - accuracy: 0.5204 - val_loss: 1.5823 - val_accuracy: 0.4548\n",
      "Epoch 28/100\n",
      "1125/1125 [==============================] - 7s 6ms/step - loss: 1.3353 - accuracy: 0.5203 - val_loss: 1.6129 - val_accuracy: 0.4494\n",
      "Epoch 29/100\n",
      "1125/1125 [==============================] - 7s 6ms/step - loss: 1.3199 - accuracy: 0.5255 - val_loss: 1.5972 - val_accuracy: 0.4496\n",
      "Epoch 30/100\n",
      "1125/1125 [==============================] - 7s 6ms/step - loss: 1.3157 - accuracy: 0.5321 - val_loss: 1.5580 - val_accuracy: 0.4610\n",
      "Epoch 31/100\n",
      "1125/1125 [==============================] - 7s 6ms/step - loss: 1.3035 - accuracy: 0.5329 - val_loss: 1.5973 - val_accuracy: 0.4548\n",
      "Epoch 32/100\n",
      "1125/1125 [==============================] - 7s 6ms/step - loss: 1.2930 - accuracy: 0.5333 - val_loss: 1.5651 - val_accuracy: 0.4610\n",
      "Epoch 33/100\n",
      "1125/1125 [==============================] - 7s 6ms/step - loss: 1.2820 - accuracy: 0.5414 - val_loss: 1.6102 - val_accuracy: 0.4616\n",
      "Epoch 34/100\n",
      "1125/1125 [==============================] - 7s 6ms/step - loss: 1.2756 - accuracy: 0.5435 - val_loss: 1.5868 - val_accuracy: 0.4516\n",
      "Epoch 35/100\n",
      "1125/1125 [==============================] - 7s 6ms/step - loss: 1.2687 - accuracy: 0.5472 - val_loss: 1.5769 - val_accuracy: 0.4696\n",
      "Epoch 36/100\n",
      "1125/1125 [==============================] - 7s 6ms/step - loss: 1.2589 - accuracy: 0.5489 - val_loss: 1.6031 - val_accuracy: 0.4644\n",
      "Epoch 37/100\n",
      "1125/1125 [==============================] - 7s 6ms/step - loss: 1.2484 - accuracy: 0.5510 - val_loss: 1.6276 - val_accuracy: 0.4502\n",
      "Epoch 38/100\n",
      "1125/1125 [==============================] - 7s 6ms/step - loss: 1.2410 - accuracy: 0.5574 - val_loss: 1.5538 - val_accuracy: 0.4684\n",
      "Epoch 39/100\n",
      "1125/1125 [==============================] - 7s 6ms/step - loss: 1.2294 - accuracy: 0.5598 - val_loss: 1.6219 - val_accuracy: 0.4632\n",
      "Epoch 40/100\n",
      "1125/1125 [==============================] - 7s 6ms/step - loss: 1.2179 - accuracy: 0.5642 - val_loss: 1.6021 - val_accuracy: 0.4712\n",
      "Epoch 41/100\n",
      "1125/1125 [==============================] - 7s 6ms/step - loss: 1.2140 - accuracy: 0.5647 - val_loss: 1.6187 - val_accuracy: 0.4748\n",
      "Epoch 42/100\n",
      "1125/1125 [==============================] - 7s 6ms/step - loss: 1.2024 - accuracy: 0.5671 - val_loss: 1.6352 - val_accuracy: 0.4698\n",
      "Epoch 43/100\n",
      "1125/1125 [==============================] - 7s 6ms/step - loss: 1.1936 - accuracy: 0.5690 - val_loss: 1.6276 - val_accuracy: 0.4570\n",
      "Epoch 44/100\n",
      "1125/1125 [==============================] - 7s 6ms/step - loss: 1.1864 - accuracy: 0.5736 - val_loss: 1.6295 - val_accuracy: 0.4606\n",
      "Epoch 45/100\n",
      "1125/1125 [==============================] - 7s 6ms/step - loss: 1.1775 - accuracy: 0.5777 - val_loss: 1.6385 - val_accuracy: 0.4634\n",
      "282/282 [==============================] - 1s 2ms/step - loss: 1.5469 - accuracy: 0.4638\n",
      "Epoch 1/100\n",
      "1125/1125 [==============================] - 7s 6ms/step - loss: 2.0862 - accuracy: 0.2294 - val_loss: 1.8634 - val_accuracy: 0.3204\n",
      "Epoch 2/100\n",
      "1125/1125 [==============================] - 7s 6ms/step - loss: 1.8723 - accuracy: 0.3134 - val_loss: 1.8729 - val_accuracy: 0.3296\n",
      "Epoch 3/100\n",
      "1125/1125 [==============================] - 7s 6ms/step - loss: 1.7996 - accuracy: 0.3457 - val_loss: 1.8401 - val_accuracy: 0.3504\n",
      "Epoch 4/100\n",
      "1125/1125 [==============================] - 7s 6ms/step - loss: 1.7431 - accuracy: 0.3639 - val_loss: 1.7343 - val_accuracy: 0.3800\n",
      "Epoch 5/100\n",
      "1125/1125 [==============================] - 7s 6ms/step - loss: 1.7031 - accuracy: 0.3825 - val_loss: 1.6816 - val_accuracy: 0.3956\n",
      "Epoch 6/100\n",
      "1125/1125 [==============================] - 7s 6ms/step - loss: 1.6655 - accuracy: 0.3984 - val_loss: 1.7431 - val_accuracy: 0.3778\n",
      "Epoch 7/100\n",
      "1125/1125 [==============================] - 7s 6ms/step - loss: 1.6369 - accuracy: 0.4113 - val_loss: 1.6420 - val_accuracy: 0.4186\n",
      "Epoch 8/100\n",
      "1125/1125 [==============================] - 7s 6ms/step - loss: 1.6084 - accuracy: 0.4231 - val_loss: 1.6355 - val_accuracy: 0.4012\n",
      "Epoch 9/100\n",
      "1125/1125 [==============================] - 8s 7ms/step - loss: 1.6094 - accuracy: 0.4206 - val_loss: 1.6378 - val_accuracy: 0.4124\n",
      "Epoch 10/100\n",
      "1125/1125 [==============================] - 8s 7ms/step - loss: 1.5681 - accuracy: 0.4345 - val_loss: 1.6364 - val_accuracy: 0.4174\n",
      "Epoch 11/100\n",
      "1125/1125 [==============================] - 8s 7ms/step - loss: 1.5492 - accuracy: 0.4459 - val_loss: 1.6559 - val_accuracy: 0.4136\n",
      "Epoch 12/100\n",
      "1125/1125 [==============================] - 7s 6ms/step - loss: 1.5447 - accuracy: 0.4431 - val_loss: 1.6125 - val_accuracy: 0.4076\n",
      "Epoch 13/100\n",
      "1125/1125 [==============================] - 7s 6ms/step - loss: 1.5245 - accuracy: 0.4518 - val_loss: 1.5994 - val_accuracy: 0.4362\n",
      "Epoch 14/100\n",
      "1125/1125 [==============================] - 7s 6ms/step - loss: 1.5097 - accuracy: 0.4579 - val_loss: 1.6138 - val_accuracy: 0.4366\n",
      "Epoch 15/100\n",
      "1125/1125 [==============================] - 7s 6ms/step - loss: 1.4906 - accuracy: 0.4666 - val_loss: 1.5877 - val_accuracy: 0.4392\n",
      "Epoch 16/100\n",
      "1125/1125 [==============================] - 7s 6ms/step - loss: 1.4756 - accuracy: 0.4700 - val_loss: 1.5884 - val_accuracy: 0.4336\n",
      "Epoch 17/100\n",
      "1125/1125 [==============================] - 7s 6ms/step - loss: 1.4647 - accuracy: 0.4766 - val_loss: 1.5886 - val_accuracy: 0.4356\n",
      "Epoch 18/100\n",
      "1125/1125 [==============================] - 7s 6ms/step - loss: 1.4512 - accuracy: 0.4803 - val_loss: 1.5587 - val_accuracy: 0.4528\n",
      "Epoch 19/100\n",
      "1125/1125 [==============================] - 7s 6ms/step - loss: 1.4417 - accuracy: 0.4875 - val_loss: 1.5590 - val_accuracy: 0.4442\n",
      "Epoch 20/100\n",
      "1125/1125 [==============================] - 7s 6ms/step - loss: 1.4295 - accuracy: 0.4918 - val_loss: 1.6055 - val_accuracy: 0.4462\n",
      "Epoch 21/100\n",
      "1125/1125 [==============================] - 7s 6ms/step - loss: 1.4144 - accuracy: 0.4962 - val_loss: 1.6716 - val_accuracy: 0.4344\n",
      "Epoch 22/100\n",
      "1125/1125 [==============================] - 7s 6ms/step - loss: 1.3999 - accuracy: 0.4992 - val_loss: 1.6257 - val_accuracy: 0.4332\n",
      "Epoch 23/100\n",
      "1125/1125 [==============================] - 7s 6ms/step - loss: 1.3917 - accuracy: 0.5057 - val_loss: 1.5527 - val_accuracy: 0.4634\n",
      "Epoch 24/100\n",
      "1125/1125 [==============================] - 7s 6ms/step - loss: 1.3771 - accuracy: 0.5102 - val_loss: 1.5556 - val_accuracy: 0.4524\n",
      "Epoch 25/100\n",
      "1125/1125 [==============================] - 7s 6ms/step - loss: 1.4221 - accuracy: 0.4899 - val_loss: 1.6635 - val_accuracy: 0.3976\n",
      "Epoch 26/100\n",
      "1125/1125 [==============================] - 7s 6ms/step - loss: 1.4816 - accuracy: 0.4730 - val_loss: 1.5806 - val_accuracy: 0.4396\n",
      "Epoch 27/100\n",
      "1125/1125 [==============================] - 7s 6ms/step - loss: 1.3838 - accuracy: 0.5073 - val_loss: 1.5867 - val_accuracy: 0.4580\n",
      "Epoch 28/100\n",
      "1125/1125 [==============================] - 7s 6ms/step - loss: 1.3560 - accuracy: 0.5178 - val_loss: 1.5817 - val_accuracy: 0.4556\n",
      "Epoch 29/100\n",
      "1125/1125 [==============================] - 7s 6ms/step - loss: 1.3376 - accuracy: 0.5272 - val_loss: 1.5519 - val_accuracy: 0.4654\n",
      "Epoch 30/100\n",
      "1125/1125 [==============================] - 7s 6ms/step - loss: 1.3276 - accuracy: 0.5252 - val_loss: 1.5431 - val_accuracy: 0.4688\n",
      "Epoch 31/100\n",
      "1125/1125 [==============================] - 7s 6ms/step - loss: 1.3177 - accuracy: 0.5311 - val_loss: 1.6241 - val_accuracy: 0.4388\n",
      "Epoch 32/100\n",
      "1125/1125 [==============================] - 7s 6ms/step - loss: 1.3067 - accuracy: 0.5368 - val_loss: 1.5171 - val_accuracy: 0.4856\n",
      "Epoch 33/100\n",
      "1125/1125 [==============================] - 7s 6ms/step - loss: 1.2963 - accuracy: 0.5393 - val_loss: 1.6010 - val_accuracy: 0.4530\n",
      "Epoch 34/100\n",
      "1125/1125 [==============================] - 7s 6ms/step - loss: 1.2900 - accuracy: 0.5421 - val_loss: 1.5313 - val_accuracy: 0.4628\n",
      "Epoch 35/100\n",
      "1125/1125 [==============================] - 7s 6ms/step - loss: 1.5974 - accuracy: 0.4307 - val_loss: 1.7424 - val_accuracy: 0.3716\n",
      "Epoch 36/100\n",
      "1125/1125 [==============================] - 7s 6ms/step - loss: 1.6101 - accuracy: 0.4201 - val_loss: 1.6155 - val_accuracy: 0.4298\n",
      "Epoch 37/100\n",
      "1125/1125 [==============================] - 7s 6ms/step - loss: 1.5228 - accuracy: 0.4567 - val_loss: 1.5856 - val_accuracy: 0.4400\n",
      "Epoch 38/100\n",
      "1125/1125 [==============================] - 7s 6ms/step - loss: 1.4751 - accuracy: 0.4719 - val_loss: 1.5817 - val_accuracy: 0.4432\n",
      "Epoch 39/100\n",
      "1125/1125 [==============================] - 7s 6ms/step - loss: 1.4335 - accuracy: 0.4874 - val_loss: 1.5898 - val_accuracy: 0.4474\n",
      "Epoch 40/100\n",
      "1125/1125 [==============================] - 7s 6ms/step - loss: 1.3963 - accuracy: 0.5056 - val_loss: 1.5286 - val_accuracy: 0.4632\n",
      "Epoch 41/100\n",
      "1125/1125 [==============================] - 7s 6ms/step - loss: 1.3560 - accuracy: 0.5168 - val_loss: 1.5781 - val_accuracy: 0.4556\n",
      "Epoch 42/100\n",
      "1125/1125 [==============================] - 7s 6ms/step - loss: 1.3256 - accuracy: 0.5281 - val_loss: 1.5655 - val_accuracy: 0.4584\n",
      "Epoch 43/100\n",
      "1125/1125 [==============================] - 7s 6ms/step - loss: 1.3031 - accuracy: 0.5340 - val_loss: 1.5514 - val_accuracy: 0.4630\n",
      "Epoch 44/100\n",
      "1125/1125 [==============================] - 7s 6ms/step - loss: 1.2850 - accuracy: 0.5413 - val_loss: 1.5532 - val_accuracy: 0.4766\n",
      "Epoch 45/100\n",
      "1125/1125 [==============================] - 7s 6ms/step - loss: 1.2663 - accuracy: 0.5493 - val_loss: 1.5863 - val_accuracy: 0.4606\n",
      "Epoch 46/100\n",
      "1125/1125 [==============================] - 7s 6ms/step - loss: 1.2519 - accuracy: 0.5564 - val_loss: 1.5424 - val_accuracy: 0.4766\n",
      "Epoch 47/100\n",
      "1125/1125 [==============================] - 7s 6ms/step - loss: 1.2480 - accuracy: 0.5584 - val_loss: 1.5715 - val_accuracy: 0.4682\n",
      "Epoch 48/100\n",
      "1125/1125 [==============================] - 7s 6ms/step - loss: 1.2319 - accuracy: 0.5610 - val_loss: 1.5529 - val_accuracy: 0.4716\n",
      "Epoch 49/100\n",
      "1125/1125 [==============================] - 7s 6ms/step - loss: 1.2145 - accuracy: 0.5678 - val_loss: 1.6265 - val_accuracy: 0.4596\n",
      "Epoch 50/100\n",
      "1125/1125 [==============================] - 7s 6ms/step - loss: 1.2079 - accuracy: 0.5716 - val_loss: 1.6099 - val_accuracy: 0.4678\n",
      "Epoch 51/100\n",
      "1125/1125 [==============================] - 7s 6ms/step - loss: 1.1994 - accuracy: 0.5754 - val_loss: 1.5919 - val_accuracy: 0.4736\n",
      "Epoch 52/100\n",
      "1125/1125 [==============================] - 7s 6ms/step - loss: 1.1858 - accuracy: 0.5812 - val_loss: 1.6027 - val_accuracy: 0.4718\n",
      "282/282 [==============================] - 1s 2ms/step - loss: 1.5296 - accuracy: 0.4731\n",
      "Epoch 1/100\n",
      "1407/1407 [==============================] - 9s 7ms/step - loss: 2.0692 - accuracy: 0.2361 - val_loss: 1.9108 - val_accuracy: 0.2894\n",
      "Epoch 2/100\n",
      "1407/1407 [==============================] - 9s 6ms/step - loss: 1.8561 - accuracy: 0.3177 - val_loss: 1.9661 - val_accuracy: 0.3100\n",
      "Epoch 3/100\n",
      "1407/1407 [==============================] - 9s 6ms/step - loss: 1.7837 - accuracy: 0.3539 - val_loss: 1.7882 - val_accuracy: 0.3424\n",
      "Epoch 4/100\n",
      "1407/1407 [==============================] - 9s 6ms/step - loss: 1.7302 - accuracy: 0.3738 - val_loss: 1.8133 - val_accuracy: 0.3552\n",
      "Epoch 5/100\n",
      "1407/1407 [==============================] - 9s 6ms/step - loss: 1.6891 - accuracy: 0.3940 - val_loss: 1.7475 - val_accuracy: 0.3620\n",
      "Epoch 6/100\n",
      "1407/1407 [==============================] - 9s 6ms/step - loss: 1.6516 - accuracy: 0.4081 - val_loss: 1.6577 - val_accuracy: 0.4094\n",
      "Epoch 7/100\n",
      "1407/1407 [==============================] - 9s 6ms/step - loss: 1.6215 - accuracy: 0.4200 - val_loss: 1.6849 - val_accuracy: 0.3938\n",
      "Epoch 8/100\n",
      "1407/1407 [==============================] - 9s 6ms/step - loss: 1.5945 - accuracy: 0.4298 - val_loss: 1.6135 - val_accuracy: 0.4190\n",
      "Epoch 9/100\n",
      "1407/1407 [==============================] - 9s 6ms/step - loss: 1.5762 - accuracy: 0.4349 - val_loss: 1.6252 - val_accuracy: 0.4156\n",
      "Epoch 10/100\n",
      "1407/1407 [==============================] - 9s 7ms/step - loss: 1.5543 - accuracy: 0.4434 - val_loss: 1.6071 - val_accuracy: 0.4200\n",
      "Epoch 11/100\n",
      "1407/1407 [==============================] - 9s 6ms/step - loss: 1.5401 - accuracy: 0.4495 - val_loss: 1.6650 - val_accuracy: 0.4176\n",
      "Epoch 12/100\n",
      "1407/1407 [==============================] - 9s 6ms/step - loss: 1.5268 - accuracy: 0.4553 - val_loss: 1.6205 - val_accuracy: 0.4236\n",
      "Epoch 13/100\n",
      "1407/1407 [==============================] - 9s 6ms/step - loss: 1.5138 - accuracy: 0.4600 - val_loss: 1.5894 - val_accuracy: 0.4302\n",
      "Epoch 14/100\n",
      "1407/1407 [==============================] - 9s 6ms/step - loss: 1.4965 - accuracy: 0.4638 - val_loss: 1.5518 - val_accuracy: 0.4584\n",
      "Epoch 15/100\n",
      "1407/1407 [==============================] - 9s 6ms/step - loss: 1.4884 - accuracy: 0.4689 - val_loss: 1.6421 - val_accuracy: 0.4172\n",
      "Epoch 16/100\n",
      "1407/1407 [==============================] - 9s 6ms/step - loss: 1.4726 - accuracy: 0.4737 - val_loss: 1.5587 - val_accuracy: 0.4522\n",
      "Epoch 17/100\n",
      "1407/1407 [==============================] - 9s 7ms/step - loss: 1.4612 - accuracy: 0.4798 - val_loss: 1.5883 - val_accuracy: 0.4402\n",
      "Epoch 18/100\n",
      "1407/1407 [==============================] - 9s 6ms/step - loss: 1.4534 - accuracy: 0.4832 - val_loss: 1.5631 - val_accuracy: 0.4452\n",
      "Epoch 19/100\n",
      "1407/1407 [==============================] - 9s 6ms/step - loss: 1.4377 - accuracy: 0.4846 - val_loss: 1.5787 - val_accuracy: 0.4460\n",
      "Epoch 20/100\n",
      "1407/1407 [==============================] - 9s 6ms/step - loss: 1.4288 - accuracy: 0.4881 - val_loss: 1.5492 - val_accuracy: 0.4558\n",
      "Epoch 21/100\n",
      "1407/1407 [==============================] - 9s 6ms/step - loss: 1.4185 - accuracy: 0.4927 - val_loss: 1.5322 - val_accuracy: 0.4528\n",
      "Epoch 22/100\n",
      "1407/1407 [==============================] - 9s 6ms/step - loss: 1.4058 - accuracy: 0.4961 - val_loss: 1.5578 - val_accuracy: 0.4522\n",
      "Epoch 23/100\n",
      "1407/1407 [==============================] - 9s 7ms/step - loss: 1.3998 - accuracy: 0.4996 - val_loss: 1.5702 - val_accuracy: 0.4412\n",
      "Epoch 24/100\n",
      "1407/1407 [==============================] - 10s 7ms/step - loss: 1.3881 - accuracy: 0.5005 - val_loss: 1.5759 - val_accuracy: 0.4536\n",
      "Epoch 25/100\n",
      "1407/1407 [==============================] - 10s 7ms/step - loss: 1.3796 - accuracy: 0.5040 - val_loss: 1.5329 - val_accuracy: 0.4622\n",
      "Epoch 26/100\n",
      "1407/1407 [==============================] - 9s 6ms/step - loss: 1.3699 - accuracy: 0.5097 - val_loss: 1.5415 - val_accuracy: 0.4444\n",
      "Epoch 27/100\n",
      "1407/1407 [==============================] - 10s 7ms/step - loss: 1.3574 - accuracy: 0.5155 - val_loss: 1.5001 - val_accuracy: 0.4744\n",
      "Epoch 28/100\n",
      "1407/1407 [==============================] - 9s 7ms/step - loss: 1.5449 - accuracy: 0.4463 - val_loss: 1.7733 - val_accuracy: 0.3534\n",
      "Epoch 29/100\n",
      "1407/1407 [==============================] - 9s 7ms/step - loss: 1.6814 - accuracy: 0.3879 - val_loss: 1.6656 - val_accuracy: 0.3984\n",
      "Epoch 30/100\n",
      "1407/1407 [==============================] - 9s 7ms/step - loss: 1.6160 - accuracy: 0.4153 - val_loss: 1.6812 - val_accuracy: 0.3868\n",
      "Epoch 31/100\n",
      "1407/1407 [==============================] - 10s 7ms/step - loss: 1.5886 - accuracy: 0.4249 - val_loss: 1.6398 - val_accuracy: 0.4050\n",
      "Epoch 32/100\n",
      "1407/1407 [==============================] - 9s 6ms/step - loss: 1.5729 - accuracy: 0.4336 - val_loss: 1.6019 - val_accuracy: 0.4198\n",
      "Epoch 33/100\n",
      "1407/1407 [==============================] - 9s 6ms/step - loss: 1.5399 - accuracy: 0.4465 - val_loss: 1.6299 - val_accuracy: 0.4322\n",
      "Epoch 34/100\n",
      "1407/1407 [==============================] - 9s 6ms/step - loss: 1.4937 - accuracy: 0.4666 - val_loss: 1.5977 - val_accuracy: 0.4400\n",
      "Epoch 35/100\n",
      "1407/1407 [==============================] - 9s 6ms/step - loss: 1.4580 - accuracy: 0.4766 - val_loss: 1.5749 - val_accuracy: 0.4468\n",
      "Epoch 36/100\n",
      "1407/1407 [==============================] - 9s 6ms/step - loss: 1.4315 - accuracy: 0.4872 - val_loss: 1.5681 - val_accuracy: 0.4458\n",
      "Epoch 37/100\n",
      "1407/1407 [==============================] - 9s 6ms/step - loss: 1.4004 - accuracy: 0.4981 - val_loss: 1.5211 - val_accuracy: 0.4706\n",
      "Epoch 38/100\n",
      "1407/1407 [==============================] - 9s 6ms/step - loss: 1.3791 - accuracy: 0.5053 - val_loss: 1.5358 - val_accuracy: 0.4562\n",
      "Epoch 39/100\n",
      "1407/1407 [==============================] - 9s 6ms/step - loss: 1.3662 - accuracy: 0.5117 - val_loss: 1.5396 - val_accuracy: 0.4668\n",
      "Epoch 40/100\n",
      "1407/1407 [==============================] - 9s 6ms/step - loss: 1.3485 - accuracy: 0.5187 - val_loss: 1.5665 - val_accuracy: 0.4656\n",
      "Epoch 41/100\n",
      "1407/1407 [==============================] - 9s 6ms/step - loss: 1.3352 - accuracy: 0.5214 - val_loss: 1.5186 - val_accuracy: 0.4738\n",
      "Epoch 42/100\n",
      "1407/1407 [==============================] - 9s 6ms/step - loss: 1.3249 - accuracy: 0.5291 - val_loss: 1.5086 - val_accuracy: 0.4662\n",
      "Epoch 43/100\n",
      "1407/1407 [==============================] - 9s 6ms/step - loss: 1.3137 - accuracy: 0.5331 - val_loss: 1.5557 - val_accuracy: 0.4670\n",
      "Epoch 44/100\n",
      "1407/1407 [==============================] - 9s 6ms/step - loss: 1.3009 - accuracy: 0.5366 - val_loss: 1.5481 - val_accuracy: 0.4670\n",
      "Epoch 45/100\n",
      "1407/1407 [==============================] - 9s 6ms/step - loss: 1.2978 - accuracy: 0.5393 - val_loss: 1.5101 - val_accuracy: 0.4742\n",
      "Epoch 46/100\n",
      "1407/1407 [==============================] - 9s 6ms/step - loss: 1.2857 - accuracy: 0.5415 - val_loss: 1.5410 - val_accuracy: 0.4680\n",
      "Epoch 47/100\n",
      "1407/1407 [==============================] - 9s 6ms/step - loss: 1.2768 - accuracy: 0.5437 - val_loss: 1.5550 - val_accuracy: 0.4580\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomizedSearchCV(estimator=<tensorflow.python.keras.wrappers.scikit_learn.KerasClassifier object at 0x7fdf1402d5b0>,\n",
       "                   n_iter=1,\n",
       "                   param_distributions={'dropout_rate': [0.03, 0.1],\n",
       "                                        'learning_rate': [0.0001, 0.0003]})"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rand_search.fit(x_train, y_train, epochs = 100, validation_data = (x_valid, y_valid), callbacks = keras.callbacks.EarlyStopping(patience = 20, restore_best_weights = True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'learning_rate': 0.0003, 'dropout_rate': 0.03}"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rand_search.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.46155555844306945"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rand_search.best_score_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- As we can see the model performs better on a dropout rate of 0.03 and a lr of 0.0003. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "keras.backend.clear_session()\n",
    "tf.random.set_seed(42)\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 3e-4\n",
    "dropout_rate = 0.03"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.Sequential()\n",
    "model.add(keras.layers.Flatten(input_shape = [32, 32, 3]))\n",
    "for _ in range(20):\n",
    "    model.add(keras.layers.Dense(100, activation = keras.activations.selu, kernel_initializer = keras.initializers.lecun_normal()))\n",
    "model.add(keras.layers.AlphaDropout(rate = dropout_rate))\n",
    "model.add(keras.layers.Dense(10, activation = keras.activations.softmax))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "flatten (Flatten)            (None, 3072)              0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 100)               307300    \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "dense_12 (Dense)             (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "dense_13 (Dense)             (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "dense_14 (Dense)             (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "dense_15 (Dense)             (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "dense_16 (Dense)             (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "dense_17 (Dense)             (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "dense_18 (Dense)             (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "dense_19 (Dense)             (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "alpha_dropout (AlphaDropout) (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_20 (Dense)             (None, 10)                1010      \n",
      "=================================================================\n",
      "Total params: 500,210\n",
      "Trainable params: 500,210\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss = keras.losses.sparse_categorical_crossentropy, optimizer = keras.optimizers.Nadam(lr = learning_rate), metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "early_stopping_cb = keras.callbacks.EarlyStopping(patience = 20, restore_best_weights = True)\n",
    "model_checkpoint_cb = keras.callbacks.ModelCheckpoint('CIFAR10_Dropout_model', save_best_only = True, save_weights_only = True)\n",
    "run_logdir = os.path.join(os.curdir, 'CIFAR10_logs', 'Dropout', 'optimal_model', 'lr = {}'.format(learning_rate))\n",
    "tensorboard_cb = keras.callbacks.TensorBoard(run_logdir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "   1/1407 [..............................] - ETA: 0s - loss: 2.9325 - accuracy: 0.1562WARNING:tensorflow:From /home/gokul/.local/lib/python3.8/site-packages/tensorflow/python/ops/summary_ops_v2.py:1277: stop (from tensorflow.python.eager.profiler) is deprecated and will be removed after 2020-07-01.\n",
      "Instructions for updating:\n",
      "use `tf.profiler.experimental.stop` instead.\n",
      "   2/1407 [..............................] - ETA: 1:57 - loss: 2.9073 - accuracy: 0.1875WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0252s vs `on_train_batch_end` time: 0.1419s). Check your callbacks.\n",
      "1407/1407 [==============================] - 9s 6ms/step - loss: 1.8803 - accuracy: 0.3300 - val_loss: 1.7047 - val_accuracy: 0.4004\n",
      "Epoch 2/100\n",
      "1407/1407 [==============================] - 8s 6ms/step - loss: 1.6275 - accuracy: 0.4248 - val_loss: 1.5999 - val_accuracy: 0.4342\n",
      "Epoch 3/100\n",
      "1407/1407 [==============================] - 8s 6ms/step - loss: 1.5197 - accuracy: 0.4628 - val_loss: 1.5673 - val_accuracy: 0.4450\n",
      "Epoch 4/100\n",
      "1407/1407 [==============================] - 8s 6ms/step - loss: 1.4527 - accuracy: 0.4902 - val_loss: 1.5385 - val_accuracy: 0.4620\n",
      "Epoch 5/100\n",
      "1407/1407 [==============================] - 8s 6ms/step - loss: 1.3878 - accuracy: 0.5100 - val_loss: 1.4937 - val_accuracy: 0.4764\n",
      "Epoch 6/100\n",
      "1407/1407 [==============================] - 8s 6ms/step - loss: 1.3323 - accuracy: 0.5324 - val_loss: 1.4472 - val_accuracy: 0.4972\n",
      "Epoch 7/100\n",
      "1407/1407 [==============================] - 8s 6ms/step - loss: 1.2866 - accuracy: 0.5495 - val_loss: 1.4855 - val_accuracy: 0.4906\n",
      "Epoch 8/100\n",
      "1407/1407 [==============================] - 8s 6ms/step - loss: 1.2433 - accuracy: 0.5637 - val_loss: 1.4497 - val_accuracy: 0.5042\n",
      "Epoch 9/100\n",
      "1407/1407 [==============================] - 8s 6ms/step - loss: 1.1989 - accuracy: 0.5805 - val_loss: 1.4759 - val_accuracy: 0.4996\n",
      "Epoch 10/100\n",
      "1407/1407 [==============================] - 8s 6ms/step - loss: 1.1630 - accuracy: 0.5941 - val_loss: 1.5034 - val_accuracy: 0.4998\n",
      "Epoch 11/100\n",
      "1407/1407 [==============================] - 8s 6ms/step - loss: 1.1256 - accuracy: 0.6079 - val_loss: 1.4670 - val_accuracy: 0.5106\n",
      "Epoch 12/100\n",
      "1407/1407 [==============================] - 8s 6ms/step - loss: 1.0908 - accuracy: 0.6184 - val_loss: 1.4762 - val_accuracy: 0.5088\n",
      "Epoch 13/100\n",
      "1407/1407 [==============================] - 8s 6ms/step - loss: 1.0581 - accuracy: 0.6304 - val_loss: 1.4950 - val_accuracy: 0.5092\n",
      "Epoch 14/100\n",
      "1407/1407 [==============================] - 8s 6ms/step - loss: 1.0288 - accuracy: 0.6421 - val_loss: 1.4859 - val_accuracy: 0.5198\n",
      "Epoch 15/100\n",
      "1407/1407 [==============================] - 8s 6ms/step - loss: 0.9993 - accuracy: 0.6504 - val_loss: 1.5271 - val_accuracy: 0.5098\n",
      "Epoch 16/100\n",
      "1407/1407 [==============================] - 8s 6ms/step - loss: 0.9741 - accuracy: 0.6581 - val_loss: 1.5503 - val_accuracy: 0.5152\n",
      "Epoch 17/100\n",
      "1407/1407 [==============================] - 8s 6ms/step - loss: 0.9419 - accuracy: 0.6719 - val_loss: 1.5433 - val_accuracy: 0.5092\n",
      "Epoch 18/100\n",
      "1407/1407 [==============================] - 9s 7ms/step - loss: 0.9110 - accuracy: 0.6816 - val_loss: 1.5869 - val_accuracy: 0.5054\n",
      "Epoch 19/100\n",
      "1407/1407 [==============================] - 9s 6ms/step - loss: 0.8893 - accuracy: 0.6936 - val_loss: 1.6390 - val_accuracy: 0.5146\n",
      "Epoch 20/100\n",
      "1407/1407 [==============================] - 8s 6ms/step - loss: 0.8661 - accuracy: 0.6984 - val_loss: 1.6319 - val_accuracy: 0.5036\n",
      "Epoch 21/100\n",
      "1407/1407 [==============================] - 8s 6ms/step - loss: 0.8454 - accuracy: 0.7050 - val_loss: 1.6356 - val_accuracy: 0.5094\n",
      "Epoch 22/100\n",
      "1407/1407 [==============================] - 8s 6ms/step - loss: 0.8249 - accuracy: 0.7144 - val_loss: 1.6629 - val_accuracy: 0.5092\n",
      "Epoch 23/100\n",
      "1407/1407 [==============================] - 8s 6ms/step - loss: 0.7968 - accuracy: 0.7254 - val_loss: 1.6979 - val_accuracy: 0.5136\n",
      "Epoch 24/100\n",
      "1407/1407 [==============================] - 8s 6ms/step - loss: 0.7768 - accuracy: 0.7332 - val_loss: 1.6852 - val_accuracy: 0.5094\n",
      "Epoch 25/100\n",
      "1407/1407 [==============================] - 8s 6ms/step - loss: 0.7560 - accuracy: 0.7374 - val_loss: 1.7379 - val_accuracy: 0.5090\n",
      "Epoch 26/100\n",
      "1407/1407 [==============================] - 8s 6ms/step - loss: 0.7400 - accuracy: 0.7437 - val_loss: 1.7818 - val_accuracy: 0.5064\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(x_train_scaled, y_train, epochs = 100, validation_data = (x_valid_scaled, y_valid), callbacks = [early_stopping_cb, model_checkpoint_cb, tensorboard_cb])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "157/157 [==============================] - 0s 2ms/step - loss: 1.4472 - accuracy: 0.4972\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[1.4472453594207764, 0.49720001220703125]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(x_valid_scaled, y_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.training.tracking.util.CheckpointLoadStatus at 0x7f4cd473dc10>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_weights('CIFAR10_Dropout_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "157/157 [==============================] - 0s 2ms/step - loss: 1.4472 - accuracy: 0.4972\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[1.4472453594207764, 0.49720001220703125]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(x_valid_scaled, y_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 1s 2ms/step - loss: 1.4405 - accuracy: 0.4987\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[1.4404797554016113, 0.49869999289512634]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(x_test_scaled, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The model achieved an accuracy of 49.8% which is less than that achieved by using Batch Normalization.\n",
    "- But this model converged the fastest, in 6 epochs with each epoch having a time period of 10s."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using MCAlphaDropout after training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the MC AlphaDroput class\n",
    "class MC_Alpha_Dropout(keras.layers.AlphaDropout):\n",
    "    def call(self, inputs):\n",
    "        return super().call(inputs, training = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Creating a new model by replacing the AlphaDropout layers with the above custom layer class.\n",
    "mc_model = keras.models.Sequential()\n",
    "[mc_model.add(MC_Alpha_Dropout(layer.rate)) if isinstance(layer, keras.layers.AlphaDropout) else mc_model.add(layer) for layer in model.layers]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "flatten (Flatten)            (None, 3072)              0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 100)               307300    \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "dense_12 (Dense)             (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "dense_13 (Dense)             (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "dense_14 (Dense)             (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "dense_15 (Dense)             (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "dense_16 (Dense)             (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "dense_17 (Dense)             (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "dense_18 (Dense)             (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "dense_19 (Dense)             (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "mc__alpha__dropout (MC_Alpha (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_20 (Dense)             (None, 10)                1010      \n",
      "=================================================================\n",
      "Total params: 500,210\n",
      "Trainable params: 500,210\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "mc_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining a function for predicting the output probabilities of each class for n-samples. \n",
    "# Keep in mind that these samples wiil be different from each other due to the randomness of the dropout layer.\n",
    "def mc_dropout_predict_probas(mc_model, x, n_sample = 10):\n",
    "    y_prob = [mc_model.predict(x) for sample in range(n_sample)]\n",
    "    return np.mean(y_prob, axis = 0)\n",
    "# Using the above probability value to predict the class to which an image belongs.\n",
    "def mc_dropout_predict_class(mc_model, x, n_sample = 10):\n",
    "    y_prob = mc_dropout_predict_probas(mc_model, x, n_sample)\n",
    "    return np.argmax(y_prob, axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predicting the outputs for the validation dataset.\n",
    "y_pred = mc_dropout_predict_class(mc_model, x_valid_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5000, 1)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_valid.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4954\n"
     ]
    }
   ],
   "source": [
    "# Calculating the accuracy of the predictions\n",
    "accuracy = np.mean(y_pred == y_valid[:, 0])\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- We can see that there is no change in the performance of the model after applying MCAlphaDropout"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1cycle lr scheduling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- We will be scheduling the learning rate of the model using 1cycle strategy and check the model's performance.\n",
    "- For that we need the maximum learning rate at which the model diverges."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = keras.backend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Class for increasing the lr by the input factor and to get the lr and loss values at the end of each epoch\n",
    "class exponential_lr(keras.callbacks.Callback):\n",
    "    def __init__(self, factor):\n",
    "        self.factor = factor\n",
    "        self.rates = []\n",
    "        self.loss = []\n",
    "    def on_batch_end(self, batch, logs):\n",
    "        self.rates.append(k.get_value(self.model.optimizer.lr))\n",
    "        self.loss.append(logs['loss'])\n",
    "        k.set_value(self.model.optimizer.lr, self.model.optimizer.lr * self.factor)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The function for training the model and finding the learning rate at which the model diverges\n",
    "# It is done by replacing the model's lr with the set min_rate and then increasing it by the factor at the end of each iteration.\n",
    "# In the end the model is restored to its initial lr and weights for further use.\n",
    "def find_lr(model, x, y, epochs = 1, batch_size = 32, min_rate = 1e-4, max_rate = 10):\n",
    "    init_weights = model.get_weights()\n",
    "    init_lr = k.get_value(model.optimizer.lr)\n",
    "    iterations = len(x) // batch_size * epochs\n",
    "    factor = np.exp(np.log(max_rate / min_rate) / iterations) # The factor by which the lr should be mutiplied\n",
    "    k.set_value(model.optimizer.lr, min_rate)\n",
    "    exp_lr = exponential_lr(factor)\n",
    "    history = model.fit(x, y, epochs = epochs, batch_size = batch_size, callbacks = [exp_lr])\n",
    "    k.set_value(model.optimizer.lr, init_lr)\n",
    "    model.set_weights(init_weights)\n",
    "    return exp_lr.rates, exp_lr.loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "keras.backend.clear_session()\n",
    "tf.random.set_seed(42)\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the model\n",
    "model = keras.models.Sequential()\n",
    "model.add(keras.layers.Flatten(input_shape = [32, 32, 3]))\n",
    "for _ in range(20):\n",
    "    model.add(keras.layers.Dense(100, activation = keras.activations.selu, kernel_initializer = keras.initializers.lecun_normal))\n",
    "model.add(keras.layers.AlphaDropout(rate = 0.03))\n",
    "model.add(keras.layers.Dense(10, activation = keras.activations.softmax))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "flatten (Flatten)            (None, 3072)              0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 100)               307300    \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "dense_12 (Dense)             (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "dense_13 (Dense)             (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "dense_14 (Dense)             (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "dense_15 (Dense)             (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "dense_16 (Dense)             (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "dense_17 (Dense)             (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "dense_18 (Dense)             (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "dense_19 (Dense)             (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "alpha_dropout (AlphaDropout) (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_20 (Dense)             (None, 10)                1010      \n",
      "=================================================================\n",
      "Total params: 500,210\n",
      "Trainable params: 500,210\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compiling the model\n",
    "model.compile(loss = keras.losses.sparse_categorical_crossentropy, optimizer = keras.optimizers.SGD(lr = 3e-4), metrics = 'accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1407/1407 [==============================] - 5s 4ms/step - loss: nan - accuracy: 0.1716\n"
     ]
    }
   ],
   "source": [
    "# Using the above to find the max_rate at which the model starts to diverge\n",
    "batch_size = 32\n",
    "rates, losses = find_lr(model, x_train_scaled, y_train, epochs = 1, batch_size = batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.14, 0.175, 2.2576181888580322, 10.0)"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x720 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize = (10, 10))\n",
    "sns.set(style = 'whitegrid')\n",
    "sns.lineplot(rates, losses)\n",
    "plt.axis([0.14, 0.175, min(losses), 10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- We can see that at around lr = 0.4 the model starts to diverge.\n",
    "- Now we need to define a custom 1cycle callback function which increases and decreases the lr as per the strategy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "class one_cycle_scheduler(keras.callbacks.Callback):\n",
    "    def __init__(self, iterations, max_rate, start_rate = None, last_iterations = None, last_rate = None):\n",
    "        self.iterations = iterations\n",
    "        self.max_rate = max_rate\n",
    "        self.start_rate = start_rate or max_rate / 10 # The max_rate should be approx 10 times the initial rate\n",
    "        self.last_iterations = last_iterations or self.iterations // 10 + 1 # The last set of iterations where lr is decreased rapidly. \n",
    "        self.half_iteration = (self.iterations - self.last_iterations) // 2 # The iteration at which max_rate occurs.\n",
    "        self.last_rate = last_rate or self.start_rate / 1000\n",
    "        self.iteration = 0\n",
    "    def _interpolate(self, iter1, iter2, rate1, rate2): # Func to interpolate the learning rate\n",
    "        return ((rate2 - rate1) * (self.iteration - iter1) / (iter2 - iter1) + rate1)\n",
    "    def on_batch_begin(self, batch, logs):\n",
    "        if self.iteration < self.half_iteration: # Condition if the training is in the first half\n",
    "            rate = self._interpolate(0, self.half_iteration, self.start_rate, self.max_rate)\n",
    "        elif self.iteration < 2 * self.half_iteration: # Condition is the training is in the second half\n",
    "            rate = self._interpolate(self.half_iteration, 2 * self.half_iteration, self.max_rate, self.start_rate)\n",
    "        else: # When training is in the end iterations in which lr is decreased rapidly\n",
    "            rate = self._interpolate(2 * self.half_iteration, self.iterations, self.start_rate, self.last_rate)\n",
    "            rate = max(rate, self.last_rate)\n",
    "        self.iteration += 1\n",
    "        k.set_value(self.model.optimizer.lr, rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Now that we have the callback func. we can go on to build and train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [],
   "source": [
    "keras.backend.clear_session()\n",
    "tf.random.set_seed(42)\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Building the model\n",
    "model = keras.models.Sequential()\n",
    "model.add(keras.layers.Flatten(input_shape = [32, 32, 3]))\n",
    "for _ in range(20):\n",
    "    model.add(keras.layers.Dense(100, activation = keras.activations.selu, kernel_initializer = keras.initializers.lecun_normal()))\n",
    "model.add(keras.layers.AlphaDropout(rate = 0.03))\n",
    "model.add(keras.layers.Dense(10, activation = keras.activations.softmax))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "flatten (Flatten)            (None, 3072)              0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 100)               307300    \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "dense_12 (Dense)             (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "dense_13 (Dense)             (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "dense_14 (Dense)             (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "dense_15 (Dense)             (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "dense_16 (Dense)             (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "dense_17 (Dense)             (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "dense_18 (Dense)             (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "dense_19 (Dense)             (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "alpha_dropout (AlphaDropout) (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_20 (Dense)             (None, 10)                1010      \n",
      "=================================================================\n",
      "Total params: 500,210\n",
      "Trainable params: 500,210\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 3e-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compiling the model\n",
    "model.compile(loss = keras.losses.sparse_categorical_crossentropy, optimizer = keras.optimizers.SGD(learning_rate), metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "epochs = 15\n",
    "iterations = len(x_train_scaled) // batch_size * epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initializing the callbacks\n",
    "early_stopping_cb = keras.callbacks.EarlyStopping(patience = 20, restore_best_weights = True)\n",
    "model_checkpoint_cb = keras.callbacks.ModelCheckpoint('CIFAR10_1cycle_model', save_best_only = True, save_weights_only = True)\n",
    "run_logdir = os.path.join(os.curdir, 'CIFAR10_logs', '1cycle', 'start_rate = {}'.format(learning_rate))\n",
    "tensorboard_cb = keras.callbacks.TensorBoard(run_logdir)\n",
    "one_cycle_cb = one_cycle_scheduler(iterations = iterations, max_rate = 0.125)\n",
    "callbacks = [early_stopping_cb, model_checkpoint_cb, tensorboard_cb, one_cycle_cb]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "   2/1407 [..............................] - ETA: 1:10 - loss: 2.9454 - accuracy: 0.1250WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0054s vs `on_train_batch_end` time: 0.0936s). Check your callbacks.\n",
      "1407/1407 [==============================] - 5s 4ms/step - loss: 1.9018 - accuracy: 0.3212 - val_loss: 1.8578 - val_accuracy: 0.3370\n",
      "Epoch 2/15\n",
      "1407/1407 [==============================] - 6s 4ms/step - loss: 1.7097 - accuracy: 0.3962 - val_loss: 1.9729 - val_accuracy: 0.3124\n",
      "Epoch 3/15\n",
      "1407/1407 [==============================] - 6s 4ms/step - loss: 1.6433 - accuracy: 0.4218 - val_loss: 2.0499 - val_accuracy: 0.3400\n",
      "Epoch 4/15\n",
      "1407/1407 [==============================] - 5s 4ms/step - loss: 1.6183 - accuracy: 0.4349 - val_loss: 1.8082 - val_accuracy: 0.3798\n",
      "Epoch 5/15\n",
      "1407/1407 [==============================] - 5s 4ms/step - loss: 1.6173 - accuracy: 0.4374 - val_loss: 1.7758 - val_accuracy: 0.3612\n",
      "Epoch 6/15\n",
      "1407/1407 [==============================] - 5s 4ms/step - loss: 1.6155 - accuracy: 0.4398 - val_loss: 1.7877 - val_accuracy: 0.3678\n",
      "Epoch 7/15\n",
      "1407/1407 [==============================] - 5s 4ms/step - loss: 1.6173 - accuracy: 0.4417 - val_loss: 1.9032 - val_accuracy: 0.3482\n",
      "Epoch 8/15\n",
      "1407/1407 [==============================] - 6s 4ms/step - loss: 1.5539 - accuracy: 0.4651 - val_loss: 1.5842 - val_accuracy: 0.4684\n",
      "Epoch 9/15\n",
      "1407/1407 [==============================] - 6s 4ms/step - loss: 1.4653 - accuracy: 0.4967 - val_loss: 1.5756 - val_accuracy: 0.4592\n",
      "Epoch 10/15\n",
      "1407/1407 [==============================] - 6s 4ms/step - loss: 1.3777 - accuracy: 0.5280 - val_loss: 1.4877 - val_accuracy: 0.4980\n",
      "Epoch 11/15\n",
      "1407/1407 [==============================] - 5s 4ms/step - loss: 1.2932 - accuracy: 0.5559 - val_loss: 1.5320 - val_accuracy: 0.4954\n",
      "Epoch 12/15\n",
      "1407/1407 [==============================] - 6s 4ms/step - loss: 1.2139 - accuracy: 0.5820 - val_loss: 1.4420 - val_accuracy: 0.5064\n",
      "Epoch 13/15\n",
      "1407/1407 [==============================] - 6s 4ms/step - loss: 1.1255 - accuracy: 0.6122 - val_loss: 1.4333 - val_accuracy: 0.5284\n",
      "Epoch 14/15\n",
      "1407/1407 [==============================] - 5s 4ms/step - loss: 1.0410 - accuracy: 0.6415 - val_loss: 1.4299 - val_accuracy: 0.5298\n",
      "Epoch 15/15\n",
      "1407/1407 [==============================] - 6s 4ms/step - loss: 0.9813 - accuracy: 0.6610 - val_loss: 1.4532 - val_accuracy: 0.5304\n"
     ]
    }
   ],
   "source": [
    "# Training the model\n",
    "history = model.fit(x_train_scaled, y_train, epochs = epochs, validation_data = (x_valid_scaled, y_valid), callbacks = callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "157/157 [==============================] - 0s 2ms/step - loss: 1.4532 - accuracy: 0.5304\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[1.4532454013824463, 0.5303999781608582]"
      ]
     },
     "execution_count": 212,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(x_valid_scaled, y_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.training.tracking.util.CheckpointLoadStatus at 0x7f6cb0197be0>"
      ]
     },
     "execution_count": 213,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_weights('CIFAR10_1cycle_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "157/157 [==============================] - 0s 2ms/step - loss: 1.4299 - accuracy: 0.5298\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[1.4299284219741821, 0.5297999978065491]"
      ]
     },
     "execution_count": 214,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(x_valid_scaled, y_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 1s 2ms/step - loss: 1.4283 - accuracy: 0.5307\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[1.4283372163772583, 0.5307000279426575]"
      ]
     },
     "execution_count": 215,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(x_test_scaled, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- We can see that that the performance of the model has increased to 53% but still the best performance was achieved when using Batch Normalization.\n",
    "- Now lets try normalize the model usign BN and implement 1cycle scheduling to try increase the performance of the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The first step would be to find the max_rate value.\n",
    "- While implementing BN earlier we found that the model performs the best on lr = 0.001."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "metadata": {},
   "outputs": [],
   "source": [
    "keras.backend.clear_session()\n",
    "tf.random.set_seed(42)\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Building the model\n",
    "model = keras.models.Sequential()\n",
    "model.add(keras.layers.Flatten(input_shape = [32, 32, 3]))\n",
    "model.add(keras.layers.BatchNormalization())\n",
    "for _ in range(20):\n",
    "    model.add(keras.layers.Dense(100, kernel_initializer = keras.initializers.he_normal()))\n",
    "    model.add(keras.layers.BatchNormalization())\n",
    "    model.add(keras.layers.Activation('elu'))\n",
    "model.add(keras.layers.Dense(10, activation = keras.activations.softmax))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "flatten (Flatten)            (None, 3072)              0         \n",
      "_________________________________________________________________\n",
      "batch_normalization (BatchNo (None, 3072)              12288     \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 100)               307300    \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 100)               400       \n",
      "_________________________________________________________________\n",
      "activation (Activation)      (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 100)               400       \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 100)               400       \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "batch_normalization_4 (Batch (None, 100)               400       \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "batch_normalization_5 (Batch (None, 100)               400       \n",
      "_________________________________________________________________\n",
      "activation_4 (Activation)    (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "batch_normalization_6 (Batch (None, 100)               400       \n",
      "_________________________________________________________________\n",
      "activation_5 (Activation)    (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "batch_normalization_7 (Batch (None, 100)               400       \n",
      "_________________________________________________________________\n",
      "activation_6 (Activation)    (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "batch_normalization_8 (Batch (None, 100)               400       \n",
      "_________________________________________________________________\n",
      "activation_7 (Activation)    (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "batch_normalization_9 (Batch (None, 100)               400       \n",
      "_________________________________________________________________\n",
      "activation_8 (Activation)    (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "batch_normalization_10 (Batc (None, 100)               400       \n",
      "_________________________________________________________________\n",
      "activation_9 (Activation)    (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "batch_normalization_11 (Batc (None, 100)               400       \n",
      "_________________________________________________________________\n",
      "activation_10 (Activation)   (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "batch_normalization_12 (Batc (None, 100)               400       \n",
      "_________________________________________________________________\n",
      "activation_11 (Activation)   (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_12 (Dense)             (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "batch_normalization_13 (Batc (None, 100)               400       \n",
      "_________________________________________________________________\n",
      "activation_12 (Activation)   (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_13 (Dense)             (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "batch_normalization_14 (Batc (None, 100)               400       \n",
      "_________________________________________________________________\n",
      "activation_13 (Activation)   (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_14 (Dense)             (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "batch_normalization_15 (Batc (None, 100)               400       \n",
      "_________________________________________________________________\n",
      "activation_14 (Activation)   (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_15 (Dense)             (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "batch_normalization_16 (Batc (None, 100)               400       \n",
      "_________________________________________________________________\n",
      "activation_15 (Activation)   (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_16 (Dense)             (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "batch_normalization_17 (Batc (None, 100)               400       \n",
      "_________________________________________________________________\n",
      "activation_16 (Activation)   (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_17 (Dense)             (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "batch_normalization_18 (Batc (None, 100)               400       \n",
      "_________________________________________________________________\n",
      "activation_17 (Activation)   (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_18 (Dense)             (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "batch_normalization_19 (Batc (None, 100)               400       \n",
      "_________________________________________________________________\n",
      "activation_18 (Activation)   (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_19 (Dense)             (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "batch_normalization_20 (Batc (None, 100)               400       \n",
      "_________________________________________________________________\n",
      "activation_19 (Activation)   (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_20 (Dense)             (None, 10)                1010      \n",
      "=================================================================\n",
      "Total params: 520,498\n",
      "Trainable params: 510,354\n",
      "Non-trainable params: 10,144\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compiling the model\n",
    "model.compile(loss = keras.losses.sparse_categorical_crossentropy, optimizer = keras.optimizers.Nadam(0.01), metrics = 'accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1407/1407 [==============================] - 15s 10ms/step - loss: 138.4531 - accuracy: 0.1920\n"
     ]
    }
   ],
   "source": [
    "# Getting the rates and corresponding loss after each iteration when increasing the learning rate\n",
    "batch_size = 32\n",
    "rates, losses = find_lr(model, x_train, y_train, epochs = 1, batch_size = batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.2, 0.3, 0.0, 10.0)"
      ]
     },
     "execution_count": 325,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x720 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize = (10, 10))\n",
    "sns.lineplot(x = rates, y = losses)\n",
    "plt.axis([0.2, 0.3, 0, 10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- We can see that the max_rate after which the loss rises is around 5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "metadata": {},
   "outputs": [],
   "source": [
    "keras.backend.clear_session()\n",
    "tf.random.set_seed(42)\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Building the model\n",
    "model = keras.models.Sequential()\n",
    "model.add(keras.layers.Flatten(input_shape = [32, 32, 3]))\n",
    "model.add(keras.layers.BatchNormalization())\n",
    "for _ in range(20):\n",
    "    model.add(keras.layers.Dense(100, kernel_initializer = keras.initializers.he_normal()))\n",
    "    model.add(keras.layers.BatchNormalization())\n",
    "    model.add(keras.layers.Activation('elu'))\n",
    "model.add(keras.layers.Dense(10, activation = keras.activations.softmax))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compiling the model\n",
    "model.compile(loss = keras.losses.sparse_categorical_crossentropy, optimizer = keras.optimizers.SGD(lr = 0.001), metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 15\n",
    "batch_size = 32\n",
    "iterations = len(x_train) // batch_size * epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initializing the callback\n",
    "one_cycle_cb = one_cycle_scheduler(iterations = iterations, max_rate = 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "1407/1407 [==============================] - 11s 8ms/step - loss: 1.8607 - accuracy: 0.3311 - val_loss: 1.7437 - val_accuracy: 0.3688\n",
      "Epoch 2/15\n",
      "1407/1407 [==============================] - 11s 8ms/step - loss: 1.7049 - accuracy: 0.3914 - val_loss: 1.8058 - val_accuracy: 0.3532\n",
      "Epoch 3/15\n",
      "1407/1407 [==============================] - 11s 8ms/step - loss: 1.6491 - accuracy: 0.4125 - val_loss: 1.7989 - val_accuracy: 0.3576\n",
      "Epoch 4/15\n",
      "1407/1407 [==============================] - 11s 8ms/step - loss: 1.6087 - accuracy: 0.4254 - val_loss: 1.7456 - val_accuracy: 0.3930\n",
      "Epoch 5/15\n",
      "1407/1407 [==============================] - 11s 8ms/step - loss: 1.5699 - accuracy: 0.4431 - val_loss: 1.6241 - val_accuracy: 0.4190\n",
      "Epoch 6/15\n",
      "1407/1407 [==============================] - 11s 8ms/step - loss: 1.5384 - accuracy: 0.4522 - val_loss: 1.5470 - val_accuracy: 0.4558\n",
      "Epoch 7/15\n",
      "1407/1407 [==============================] - 11s 8ms/step - loss: 1.5022 - accuracy: 0.4670 - val_loss: 1.6864 - val_accuracy: 0.4144\n",
      "Epoch 8/15\n",
      "1407/1407 [==============================] - 11s 8ms/step - loss: 1.4545 - accuracy: 0.4856 - val_loss: 1.5610 - val_accuracy: 0.4584\n",
      "Epoch 9/15\n",
      "1407/1407 [==============================] - 11s 8ms/step - loss: 1.4003 - accuracy: 0.5041 - val_loss: 1.3905 - val_accuracy: 0.4998\n",
      "Epoch 10/15\n",
      "1407/1407 [==============================] - 11s 8ms/step - loss: 1.3593 - accuracy: 0.5163 - val_loss: 1.3697 - val_accuracy: 0.5088\n",
      "Epoch 11/15\n",
      "1407/1407 [==============================] - 12s 8ms/step - loss: 1.3147 - accuracy: 0.5340 - val_loss: 1.3320 - val_accuracy: 0.5262\n",
      "Epoch 12/15\n",
      "1407/1407 [==============================] - 12s 8ms/step - loss: 1.2674 - accuracy: 0.5533 - val_loss: 1.3289 - val_accuracy: 0.5250\n",
      "Epoch 13/15\n",
      "1407/1407 [==============================] - 12s 8ms/step - loss: 1.2266 - accuracy: 0.5636 - val_loss: 1.3673 - val_accuracy: 0.5194\n",
      "Epoch 14/15\n",
      "1407/1407 [==============================] - 12s 8ms/step - loss: 1.1842 - accuracy: 0.5788 - val_loss: 1.2713 - val_accuracy: 0.5520\n",
      "Epoch 15/15\n",
      "1407/1407 [==============================] - 12s 8ms/step - loss: 1.1566 - accuracy: 0.5892 - val_loss: 1.2615 - val_accuracy: 0.5526\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(x_train, y_train, epochs = epochs, validation_data = (x_valid, y_valid), callbacks = [one_cycle_cb])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
